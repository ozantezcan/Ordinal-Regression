{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchsample\n",
    "from torchsample import transforms as ts_transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from PIL import Image\n",
    "from tensorboardX import SummaryWriter\n",
    "from datetime import datetime\n",
    "import importlib\n",
    "\n",
    "\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "num_classes=5\n",
    "\n",
    "\n",
    "#from torchsample.transforms import RangeNorm\n",
    "\n",
    "import functions.fine_tune as ft\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bank32nh.data\n",
      "bank8FM.data\n",
      "bostonhousing\n",
      "cal_housing.data\n",
      "cpu_act.data\n",
      "cpu_small.data\n",
      "house_16H.data\n",
      "house_8L.data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"./dataset/regression\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8192, 33)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8192, 33)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"./dataset/regression/bank32nh.data\", sep=' ', header=None)\n",
    "train_df=train_df.drop(train_df.columns[-1],axis=1)\n",
    "print(train_df.shape)\n",
    "\n",
    "columns=[\"feat\"+str(k) for k in range(train_df.shape[1])]\n",
    "columns[-1]=\"label\"\n",
    "train_df.columns=columns\n",
    "\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat0</th>\n",
       "      <th>feat1</th>\n",
       "      <th>feat2</th>\n",
       "      <th>feat3</th>\n",
       "      <th>feat4</th>\n",
       "      <th>feat5</th>\n",
       "      <th>feat6</th>\n",
       "      <th>feat7</th>\n",
       "      <th>feat8</th>\n",
       "      <th>feat9</th>\n",
       "      <th>feat10</th>\n",
       "      <th>feat11</th>\n",
       "      <th>feat12</th>\n",
       "      <th>feat13</th>\n",
       "      <th>feat14</th>\n",
       "      <th>feat15</th>\n",
       "      <th>feat16</th>\n",
       "      <th>feat17</th>\n",
       "      <th>feat18</th>\n",
       "      <th>feat19</th>\n",
       "      <th>feat20</th>\n",
       "      <th>feat21</th>\n",
       "      <th>feat22</th>\n",
       "      <th>feat23</th>\n",
       "      <th>feat24</th>\n",
       "      <th>feat25</th>\n",
       "      <th>feat26</th>\n",
       "      <th>feat27</th>\n",
       "      <th>feat28</th>\n",
       "      <th>feat29</th>\n",
       "      <th>feat30</th>\n",
       "      <th>feat31</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.413010</td>\n",
       "      <td>0.607442</td>\n",
       "      <td>0.332608</td>\n",
       "      <td>0.406812</td>\n",
       "      <td>-0.151224</td>\n",
       "      <td>1.525222</td>\n",
       "      <td>-0.144368</td>\n",
       "      <td>0.852368</td>\n",
       "      <td>0.412397</td>\n",
       "      <td>1.728169</td>\n",
       "      <td>-0.449231</td>\n",
       "      <td>4.078482</td>\n",
       "      <td>0.232042</td>\n",
       "      <td>-0.323190</td>\n",
       "      <td>0.792235</td>\n",
       "      <td>0.421474</td>\n",
       "      <td>-0.307503</td>\n",
       "      <td>3.086689</td>\n",
       "      <td>0.363949</td>\n",
       "      <td>0.441308</td>\n",
       "      <td>-0.276851</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.974706</td>\n",
       "      <td>-0.776759</td>\n",
       "      <td>-0.783770</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.603486</td>\n",
       "      <td>-0.997118</td>\n",
       "      <td>-0.502138</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.169388</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.049118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.602384</td>\n",
       "      <td>0.350618</td>\n",
       "      <td>0.429196</td>\n",
       "      <td>0.414476</td>\n",
       "      <td>-0.124489</td>\n",
       "      <td>4.597991</td>\n",
       "      <td>0.579458</td>\n",
       "      <td>0.651134</td>\n",
       "      <td>0.104394</td>\n",
       "      <td>0.636356</td>\n",
       "      <td>-0.283787</td>\n",
       "      <td>3.546643</td>\n",
       "      <td>0.115860</td>\n",
       "      <td>0.409074</td>\n",
       "      <td>2.152997</td>\n",
       "      <td>0.758680</td>\n",
       "      <td>0.341127</td>\n",
       "      <td>1.478951</td>\n",
       "      <td>0.662488</td>\n",
       "      <td>0.462398</td>\n",
       "      <td>0.339673</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.798979</td>\n",
       "      <td>-0.002820</td>\n",
       "      <td>-0.080542</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.125542</td>\n",
       "      <td>-0.983397</td>\n",
       "      <td>-0.107632</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.186039</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.242579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.322881</td>\n",
       "      <td>-0.538491</td>\n",
       "      <td>1.602260</td>\n",
       "      <td>0.039605</td>\n",
       "      <td>0.196023</td>\n",
       "      <td>1.909005</td>\n",
       "      <td>-0.675672</td>\n",
       "      <td>0.963618</td>\n",
       "      <td>0.147458</td>\n",
       "      <td>1.414008</td>\n",
       "      <td>0.495453</td>\n",
       "      <td>0.056459</td>\n",
       "      <td>-0.163151</td>\n",
       "      <td>0.350221</td>\n",
       "      <td>1.124090</td>\n",
       "      <td>1.398160</td>\n",
       "      <td>-0.456921</td>\n",
       "      <td>1.600723</td>\n",
       "      <td>0.650252</td>\n",
       "      <td>-0.247380</td>\n",
       "      <td>0.318002</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.577355</td>\n",
       "      <td>-0.952645</td>\n",
       "      <td>-0.571600</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.280392</td>\n",
       "      <td>0.771129</td>\n",
       "      <td>-0.665756</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.024203</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.233570</td>\n",
       "      <td>-0.936451</td>\n",
       "      <td>1.710192</td>\n",
       "      <td>2.179527</td>\n",
       "      <td>0.438461</td>\n",
       "      <td>4.742055</td>\n",
       "      <td>-0.163625</td>\n",
       "      <td>-0.923273</td>\n",
       "      <td>0.597622</td>\n",
       "      <td>0.118409</td>\n",
       "      <td>0.229981</td>\n",
       "      <td>3.209085</td>\n",
       "      <td>-0.165046</td>\n",
       "      <td>0.012872</td>\n",
       "      <td>0.398148</td>\n",
       "      <td>1.335824</td>\n",
       "      <td>0.119910</td>\n",
       "      <td>13.070052</td>\n",
       "      <td>0.308221</td>\n",
       "      <td>-0.743841</td>\n",
       "      <td>0.258362</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.760084</td>\n",
       "      <td>-0.198235</td>\n",
       "      <td>-0.205276</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.509727</td>\n",
       "      <td>-0.579544</td>\n",
       "      <td>0.480094</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.568492</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.469045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.403126</td>\n",
       "      <td>0.313367</td>\n",
       "      <td>0.822382</td>\n",
       "      <td>1.393975</td>\n",
       "      <td>0.253435</td>\n",
       "      <td>9.398630</td>\n",
       "      <td>0.312528</td>\n",
       "      <td>0.288321</td>\n",
       "      <td>0.431867</td>\n",
       "      <td>0.110369</td>\n",
       "      <td>0.294665</td>\n",
       "      <td>1.274100</td>\n",
       "      <td>0.328350</td>\n",
       "      <td>-0.288962</td>\n",
       "      <td>0.067075</td>\n",
       "      <td>0.632938</td>\n",
       "      <td>0.148618</td>\n",
       "      <td>3.633846</td>\n",
       "      <td>0.233204</td>\n",
       "      <td>-0.685285</td>\n",
       "      <td>-0.758206</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.170067</td>\n",
       "      <td>0.573352</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.622033</td>\n",
       "      <td>-0.134747</td>\n",
       "      <td>0.669948</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.295913</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feat0     feat1     feat2     feat3     feat4     feat5     feat6  \\\n",
       "0  0.413010  0.607442  0.332608  0.406812 -0.151224  1.525222 -0.144368   \n",
       "1 -0.602384  0.350618  0.429196  0.414476 -0.124489  4.597991  0.579458   \n",
       "2 -0.322881 -0.538491  1.602260  0.039605  0.196023  1.909005 -0.675672   \n",
       "3 -0.233570 -0.936451  1.710192  2.179527  0.438461  4.742055 -0.163625   \n",
       "4  0.403126  0.313367  0.822382  1.393975  0.253435  9.398630  0.312528   \n",
       "\n",
       "      feat7     feat8     feat9    feat10    feat11    feat12    feat13  \\\n",
       "0  0.852368  0.412397  1.728169 -0.449231  4.078482  0.232042 -0.323190   \n",
       "1  0.651134  0.104394  0.636356 -0.283787  3.546643  0.115860  0.409074   \n",
       "2  0.963618  0.147458  1.414008  0.495453  0.056459 -0.163151  0.350221   \n",
       "3 -0.923273  0.597622  0.118409  0.229981  3.209085 -0.165046  0.012872   \n",
       "4  0.288321  0.431867  0.110369  0.294665  1.274100  0.328350 -0.288962   \n",
       "\n",
       "     feat14    feat15    feat16     feat17    feat18    feat19    feat20  \\\n",
       "0  0.792235  0.421474 -0.307503   3.086689  0.363949  0.441308 -0.276851   \n",
       "1  2.152997  0.758680  0.341127   1.478951  0.662488  0.462398  0.339673   \n",
       "2  1.124090  1.398160 -0.456921   1.600723  0.650252 -0.247380  0.318002   \n",
       "3  0.398148  1.335824  0.119910  13.070052  0.308221 -0.743841  0.258362   \n",
       "4  0.067075  0.632938  0.148618   3.633846  0.233204 -0.685285 -0.758206   \n",
       "\n",
       "   feat21    feat22    feat23    feat24  feat25    feat26    feat27    feat28  \\\n",
       "0     2.0  1.974706 -0.776759 -0.783770     8.0  0.603486 -0.997118 -0.502138   \n",
       "1     6.0  0.798979 -0.002820 -0.080542     2.0  1.125542 -0.983397 -0.107632   \n",
       "2     3.0  0.577355 -0.952645 -0.571600     5.0  1.280392  0.771129 -0.665756   \n",
       "3     4.0  0.760084 -0.198235 -0.205276     2.0  0.509727 -0.579544  0.480094   \n",
       "4     6.0  1.170067  0.573352  0.315217     2.0  0.622033 -0.134747  0.669948   \n",
       "\n",
       "   feat29    feat30  feat31     label  \n",
       "0     5.0  1.169388     9.0  0.049118  \n",
       "1     5.0  1.186039     7.0  0.242579  \n",
       "2     5.0  1.024203     6.0  0.000000  \n",
       "3     6.0  1.568492     7.0  0.469045  \n",
       "4     3.0  1.295913     9.0  0.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1638.4\n",
      "[0.0, 0.0001, 0.012395000000000002, 0.051264999999999998, 0.15377600000000002, 1.8201649999999998]\n",
      "[False False  True ..., False False False]\n",
      "[False False False ...,  True False False]\n",
      "[ True False False ..., False  True False]\n",
      "[False False False ..., False False  True]\n",
      "[False  True False ..., False False False]\n",
      "[ 2.  4.  0. ...,  1.  2.  3.]\n",
      "      feat0     feat1     feat2     feat3     feat4     feat5     feat6  \\\n",
      "0  0.413010  0.607442  0.332608  0.406812 -0.151224  1.525222 -0.144368   \n",
      "1 -0.602384  0.350618  0.429196  0.414476 -0.124489  4.597991  0.579458   \n",
      "2 -0.322881 -0.538491  1.602260  0.039605  0.196023  1.909005 -0.675672   \n",
      "3 -0.233570 -0.936451  1.710192  2.179527  0.438461  4.742055 -0.163625   \n",
      "4  0.403126  0.313367  0.822382  1.393975  0.253435  9.398630  0.312528   \n",
      "\n",
      "      feat7     feat8     feat9    feat10    feat11    feat12    feat13  \\\n",
      "0  0.852368  0.412397  1.728169 -0.449231  4.078482  0.232042 -0.323190   \n",
      "1  0.651134  0.104394  0.636356 -0.283787  3.546643  0.115860  0.409074   \n",
      "2  0.963618  0.147458  1.414008  0.495453  0.056459 -0.163151  0.350221   \n",
      "3 -0.923273  0.597622  0.118409  0.229981  3.209085 -0.165046  0.012872   \n",
      "4  0.288321  0.431867  0.110369  0.294665  1.274100  0.328350 -0.288962   \n",
      "\n",
      "     feat14    feat15    feat16     feat17    feat18    feat19    feat20  \\\n",
      "0  0.792235  0.421474 -0.307503   3.086689  0.363949  0.441308 -0.276851   \n",
      "1  2.152997  0.758680  0.341127   1.478951  0.662488  0.462398  0.339673   \n",
      "2  1.124090  1.398160 -0.456921   1.600723  0.650252 -0.247380  0.318002   \n",
      "3  0.398148  1.335824  0.119910  13.070052  0.308221 -0.743841  0.258362   \n",
      "4  0.067075  0.632938  0.148618   3.633846  0.233204 -0.685285 -0.758206   \n",
      "\n",
      "   feat21    feat22    feat23    feat24  feat25    feat26    feat27    feat28  \\\n",
      "0     2.0  1.974706 -0.776759 -0.783770     8.0  0.603486 -0.997118 -0.502138   \n",
      "1     6.0  0.798979 -0.002820 -0.080542     2.0  1.125542 -0.983397 -0.107632   \n",
      "2     3.0  0.577355 -0.952645 -0.571600     5.0  1.280392  0.771129 -0.665756   \n",
      "3     4.0  0.760084 -0.198235 -0.205276     2.0  0.509727 -0.579544  0.480094   \n",
      "4     6.0  1.170067  0.573352  0.315217     2.0  0.622033 -0.134747  0.669948   \n",
      "\n",
      "   feat29    feat30  feat31     label  label_ord  \n",
      "0     5.0  1.169388     9.0  0.049118        2.0  \n",
      "1     5.0  1.186039     7.0  0.242579        4.0  \n",
      "2     5.0  1.024203     6.0  0.000000        0.0  \n",
      "3     6.0  1.568492     7.0  0.469045        4.0  \n",
      "4     3.0  1.295913     9.0  0.000000        0.0  \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFJCAYAAACsBZWNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nh6QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1660c9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train_df['label_ord']=train_df['label']\n",
    "label=train_df.label.values\n",
    "sorted_idx=np.argsort(train_df.label.values)\n",
    "num_samples_per_class=train_df.shape[0]/num_classes\n",
    "print(num_samples_per_class)\n",
    "\n",
    "bins=[(k*1e-4+label[sorted_idx[np.round(k*num_samples_per_class-1).astype(np.int)]]) for k in range(1,num_classes+1)]\n",
    "bins.insert(0,0.0)\n",
    "bins[-1]=bins[-1]+1\n",
    "print(bins)\n",
    "\n",
    "label_ord=label.copy()\n",
    "for k in range(num_classes):\n",
    "    print(np.all([label>=bins[k], label<bins[k+1]],0))\n",
    "    label_ord[np.all([label>=bins[k], label<bins[k+1]],0)]=k\n",
    "    \n",
    "print(label_ord)\n",
    "\n",
    "\n",
    "train_df['label_ord']=label_ord\n",
    "print(train_df.head())\n",
    "\n",
    "plt.scatter(label,label_ord)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1a16bdcda0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFyCAYAAADccVJQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XlgFOXh//HPJksgsCGABOrFTTgMCAGpFKKoBUWlHliJ\nVuArVOVQigJCsUGUCCgFa6VFbYuIB2ARrKCgoiIaFSUSNQpBUEEjR4CEZBPItfP7wx8jERKSbHZm\nZ/f9+uuZmc08n/ion+zs7qzLMAxDAADAMSLsDgAAAGqG8gYAwGEobwAAHIbyBgDAYShvAAAchvIG\nAMBh3HYHqK6cnII6PV/Tpg2Vm1tUp+dE7bEewYO1CB6sRfCway3i4mJOuT9sn3m73ZF2R8AJWI/g\nwVoED9YieATbWoRteQMA4FSUNwAADkN5AwDgMJQ3AAAOQ3kDAOAwlDcAAA5DeQMA4DCUNwAADkN5\nAwDgMJQ3AAAOQ3kDAOAwlDcAAA5DeQMA4Kd33nlL06ZNktfrtWQ+yhsAAD8999wzWrz4X3r11TWW\nzOeY7/MGACBY3XvvdPXufYFuuOFGS+ajvAEA8FOnTp3VqVNny+bjsjkAAA5DeQMA4DABK2+fz6cZ\nM2Zo2LBhGj58uHbv3l3h+CuvvKLrrrtOQ4cO1QsvvBCoGAAAhJyAvea9YcMGlZSUaMWKFcrIyNDc\nuXO1aNEi8/gjjzyitWvXqmHDhrrqqqt01VVXKTY2NlBxAAAIGQEr7/T0dCUlJUmSevTooczMzArH\nO3XqpIKCArndbhmGIZfLVeX5mjZtKLc7sk4zxsXF1On54B/WI3iwFsGDtQgewbQWAStvr9crj8dj\nbkdGRqqsrExu909TduzYUUOHDlV0dLQGDhyoxo0bV3m+3NyiOs0XFxejnJyCOj0nao/1CB6sRfBg\nLYKHXWtR2R8MAXvN2+PxqLCw0Nz2+XxmcW/fvl0bN27UW2+9pbfffluHDx/WunXrAhUFAICQErDy\nTkxM1KZNmyRJGRkZio+PN4/FxMSoQYMGql+/viIjI9WsWTPl5+cHKgoAACElYJfNBw4cqLS0NCUn\nJ8swDM2ePVtr1qxRUVGRhg0bpmHDhunmm29WvXr11KpVK1133XWBigIAQEhxGYZh2B2iOur6tQZe\nSwourEfwYC2CB2sRPMLmNW8AABAYlDcAAA5DeQMA4DCUNwAADkN5AwDgMJQ3AAAOQ3kDAOAwlDcA\nAA5DeQMA4DCUNwAADkN5AwDgMJQ3AAAOQ3kDAOAwlDcAAA5DeQMA4DCUNwAADkN5AwDgp+LiYu3d\n+6Nl81HeAAD4acKEsUpK6qMtWzZbMp/bklkAAAhh8fGdtG/fXp155tmWzMczbwAA/DRp0lT973/r\ndPbZ51gyH+UNAICfFi16XKNHD5fX67VkPsobAAA/bdq0URs2vKnvvvvWkvl4zRsAAD899tgiffPN\nTiUkdLNkPsobAAA/tWjRQi1atLBsPi6bAwDgMJQ3AAAOQ3kDAOAwlDcAAA5DeQMA4DCUNwAADkN5\nAwDgMJQ3AAAOQ3kDAOAwlDcAAA5DeQMA4DCUNwAAflq79n+aP/9hGYZhyXyUNwAAflq0aKEWLHhE\n3333jSXz8a1iAAD4adKke/X11zvUpk07S+ajvAEA8NOllw7UpZcOtGw+LpsDAOAwlDcAAH56+ul/\n6cEHZ8jn81kyH+UNAICf7rtvqhYu/Jtycg5YMh/lDQCAH4qLi1VWViZJ8nhiLJmTN6wBAOCH8vJy\nNWnSVDExMWrUqJElc1LeAAD4ISIiQo0bN9avfnWmdXNaNhMAACHo4MEc7du3T8eOHbNsTsobAAA/\nFBcXy+WSfL5yy+bksjkAAH741a/O1CWX/Fbnn9/DsjkpbwAA/NCoUSMtXbrM0jm5bA4AgMNQ3gAA\n+GHfvr3q0qWtxo27zbI5KW8AAPxw661/0KFDh7R69UrL5qS8AQDwwzXXDJUkdevWzbI5KW8AAPxw\n7NhRSVLPnhdYNiflDQCAH3r37qP4+M4aMOBSy+akvAEA8MNHH32gHTu2KyMj3bI5+Zw3AAB+GDly\nlHbsyNK4cX+ybE7KGwAAPwwfnqxPP92i4uJjeuYZa27WwmVzAAD80K/fRWrcuLEGDbrSsjkpbwAA\n/HD0aJFKSkrVqFFDy+akvAEA8EPHjvHq2vU8tW3bzrI5KW8AAPzw9ddZys7+Qfn5BZbNSXkDAOCH\npk2b6YwzzlBsbIxlcwbs3eY+n08zZ85UVlaWoqKilJqaqtatW5vHP//8c82dO1eGYSguLk7z5s1T\n/fr1AxUHAICAGDFilBISuqt7956WzRmwZ94bNmxQSUmJVqxYoUmTJmnu3LnmMcMwlJKSojlz5mjZ\nsmVKSkpSdnZ2oKIAABAwV189UCNH3qQNG96wbM6AlXd6erqSkpIkST169FBmZqZ57Ntvv1WTJk20\nZMkS3XLLLcrLy1O7dta90A8AQF3Zvfs7SdK2bV9ZNmfALpt7vV55PB5zOzIyUmVlZXK73crNzdXW\nrVs1Y8YMtWrVSmPGjFFCQoL69u1b6fmaNm0otzuyTjPGxVn3+gROj/UIHqxF8GAtgkdlaxEfH68f\nf/xR9913r2Uv/wasvD0ejwoLC81tn88nt/un6Zo0aaLWrVurffv2kqSkpCRlZmZWWd65uUV1mi8u\nLkY5Oda9MxBVYz2CB2sRPFiL4FHVWvzwQ7ZKS0uUnX1QMTGN63zeUwnYZfPExERt2rRJkpSRkaH4\n+Hjz2LnnnqvCwkLt3r1bkrRlyxZ17NgxUFEAAAiIgoICFRUVqrS0VA0bNrJs3oA98x44cKDS0tKU\nnJwswzA0e/ZsrVmzRkVFRRo2bJgeeughTZo0SYZhqGfPnhowYECgogAAEBAHDuz//yOXIiPr9qXd\nqgSsvCMiIvTggw9W2Hf8Mrkk9e3bVytXrgzU9AAABNzevT9KkmJjYy2dl5u0AABQS489tkCSVFCQ\nb+m8lDcAALX0298OkiR16XKepfNS3gAA1NLx17y7dOlq6byUNwAAtbR+/WuSpC1bPrF0XsobAIBa\n+utfH9N55yXo8cefsHReyhsAgFpaunSxvv56hz7/PMPSeSlvAABqad26tSopKdHrr79m6byUNwAA\ntVRU9NOtu5s3j7N0XsobAIBaKC8vN8djx95p6dyUNwAAtXD8q0AlqVu38y2dm/IGAKAWduz4WpLk\ndrsVEWFtnVLeAADUwurVL0r66SuvrUZ5AwBQCw0aNJAktWnT1vK5KW8AAGohPX2LJOnIkSOWz015\nAwBQCy1atJQk9euXZPnclDcAALUQEeGSJLVp08b6uS2fEQCAELB1a7ok6fDhXMvnprwBAKiFgoIC\nSVJERKTlc1PeAAD4YdSo2yyfk/IGAKCGPvwwzRx37NjR8vkpbwAAamjixPHmuF69epbPT3kDAFBD\nR48elSS5XC5b5qe8AQCooZSUB3TOOedq4cInbZmf8gYAoIaWLPm3srN/0O7du22Zn/IGAKCGPvnk\nYxmGoWeffdqW+SlvAABqID8/3xyPHn27LRkobwAAamD16pfM8V133W1LBsobAIAaeOqpf5pj3m0O\nAIAD7Ny5w+4IlDcAADVhGIYkqXPnLrZloLwBAKiFLl0SbJub8gYAoJq2bv3UHP/tbwtty0F5AwBQ\nTW++ud4cR0dH25aD8gYAoJoyMtIl2fNlJCeivAEAqKYtW7ZIkjwej605KG8AAKopLy9XklRYWGhr\nDsobAIBqKC4uNsd//ONYG5NQ3gAAVMtll/U3x3/+819sTEJ5AwBQLTt2ZJnj+vXr25iE8gYA4LRO\nvGQeF9fCxiQ/obwBADiNZcuWmeO0tC02JvkJ5Q0AwGk89dRT5rhJkyY2JvkJ5Q0AwGl8+OGHdkeo\ngPIGAKAKeXl55rht23Y2JvkZ5Q0AQBXeeWeDOd60abONSX5GeQMAUIVx424zx3Z/ROw4yhsAgCqU\nl5fbHeEklDcAANVw883D7Y5gorwBAKhETs4Bc/zAA7NtTFIR5Q0AQCVeeWWVOY6NjbUxSUWUNwAA\nlfjzn++1O8IpUd4AAJzCoUOHzLHL5bIxyckobwAATuH++/9sjtetW2djkpNR3gAAnMLq1S+Z48sv\nv9zGJCdzV3Xw6aefrvKHb7311joNAwBAsCgtLZUUfJfMpdOU944dO6zKAQBA0Dhy5Ig5HjbsZhuT\nnFqV5T1nzpwK2/n5+WrcuHFAAwEAYLcBA/qa49mz59mY5NSq9Zr3t99+q6uuukpXXXWV9u/fr8GD\nB2vXrl2BzgYAgC2ys38wxx6Px8Ykp1at8p41a5amT5+uM844Qy1bttQtt9yiGTNmBDobAACWO/Fe\n5omJF9iYpHLVKu+8vDz169fP3P7DH/4gr9cbsFAAANhl8uSJ5njt2tdtTFK5an9UrLi42HzHXU5O\njnw+X8BCAQBgl+eff8Ycu91VvjXMNtVKdfPNN2v06NE6dOiQ5s+fr1dffVV//OMfA50NAADbBGtx\nS9Us7xtuuEGtW7fWxo0bVVZWpgcffFD9+/cPdDYAACx14kekx4y508YkVav2nxUdOnSQ1+uV2+1W\n9+7dA5kJAABbXHLJzx8RmzRpqo1Jqlat8t64caOmTp2qjh07yufzac+ePXr00Ud1wQWVvwvP5/Np\n5syZysrKUlRUlFJTU9W6deuTHpeSkqLY2FhNnjy59r8FAAB14Phd1SSpUaNGNiapWrXK+7HHHtNz\nzz2njh07SpK+/PJLpaSkaNWqVZX+zIYNG1RSUqIVK1YoIyNDc+fO1aJFiyo8Zvny5dqxY0eVfwQA\nAGCFnJwD5njIkOttTHJ61Xq3ucvlMotbks477zwZhlHlz6SnpyspKUmS1KNHD2VmZlY4/umnn+qz\nzz7TsGHDapoZAIA6179/H3P8739X/d0edqvymXdeXp4kKSEhQf/5z3+UnJysiIgIrVq1ShdeeGGV\nJ/Z6vRXuShMZGamysjK53W4dOHBA//jHP7Rw4cJqf81a06YN5XZHVuux1RUXF1On54N/WI/gwVoE\nD9bCOrm5h81xixYn3wo8mNaiyvK+8MIL5XK5zGfZ8+b9fH9Xl8ulqVMrfzHf4/GosLDQ3Pb5fObb\n7tevX6/c3FzdfvvtysnJ0bFjx9SuXTtdf33llylyc4uq9xtVU1xcjHJyCur0nKg91iN4sBbBg7Ww\nzolXk3v1uuCkf+52rUVlfzBUWd7bt2+v9YSJiYl65513dOWVVyojI0Px8fHmsREjRmjEiBGSpFWr\nVumbb76psrgBAAik22//P3O8dOky+4JUU7XesFZSUqJ3333XfCZdXl6uPXv26O677670ZwYOHKi0\ntDQlJyfLMAzNnj1ba9asUVFREa9zAwCCyhtv/Hwb1Li4FjYmqZ5qlffdd9+t77//Xjk5Oeratas+\n++wz9enTp8qfiYiI0IMPPlhhX/v27U96HM+4AQB2O3r0p5dmj98GPNhV693m27Zt06pVq3TZZZdp\n+vTpWr58uQoKeB0GAOB8F1/8a3P8u99dZ2OS6qtWebdo0UJut1tt2rTRjh071KFDBx09ejTQ2QAA\nCLht27aZ46eeCu6PiB1XrfJu2LCh1qxZo86dO2vdunXKysoyP0YGAIBTTZw43hzHxjYJrcvmM2bM\n0Pbt29W/f39FRkZq+PDhGj16dKCzAQAQUC+88Kw5/vjjz2xMUjNVvmFtyJAhFbY3bdokSWrZsqVe\neOEF3XTTTYFLBgBAAG3d+qk5btTIo6ZNm9qYpmaqLO+UlBSrcgAAYKkrr/ytOf7mm2wbk9RcleV9\nuo+DAQDgVOXlZebYKa91H1et17wBAAgl2dk/P9MeN+4uG5PUDuUNAAg7PXt2Mcf3359qY5LaobwB\nAGGlqKjiF1057ZK5RHkDAMJM9+6dzfGePQdsTFJ7lDcAIKzk5/98k7EGDRrYmKT2KG8AQNg48e6g\nQ4Y44z7mp0J5AwDCxrXXXmmOFy58wsYk/qG8AQBh46uvMs1xdHS0jUn8Q3kDAMJOkyZN7I7gF8ob\nABAWunZtb45ffPF/NibxH+UNAAh506dP1sGDOeZ2jx49bUzjP8obABDy/v3vp8zxFVcMtjFJ3aC8\nAQAhrUWLxua4efPmWrp0hY1p6gblDQAIWS+9tLLC9ldffWNTkrpFeQMAQtbYsaPM8erVr9mYpG5R\n3gCAkHTRRb8xxxEREerXr7+NaeoW5Q0ACDnl5eXavv3nG7J8/31OFY92HsobABByFix4xBz369dP\n9erVszFN3aO8AQAhxefzad68Oeb26tXrbEwTGJQ3ACCk9Op1njmOjHTbmCRwKG8AQEjJzs42x3v3\nHrYxSeBQ3gCAkPGb3/Qyx126dLUxSWBR3gCAkLB27Rrt3Pm1uf3uux/ZmCawKG8AQEgYNeoP5njQ\nIOffv7wqlDcAwPFOvH+5JD33nPPvX14VyhsA4GizZt1fYTvUbshyKpQ3AMDRHn/8UXO8ePFzql+/\nvo1prEF5AwAc68TL5dHR0br66t/ZmMY6lDcAwJGWLHm6wvbu3fttSmI9yhsA4Ej33vsnc/zaaxts\nTGI9yhsA4DhnnXWGOY6IiFDv3n1sTGM9yhsA4CiFhYUqKys1t/fty7MxjT0obwCAo7Rte6Y5XrBg\noY1J7EN5AwAc45c3Y7nllhE2JbEX5Q0AcIQTv+pTCo+bsVSG8gYABL2ysjJ9//335vZbb70XFjdj\nqQzlDQAIemed1cwcDxgwQN26nW9jGvtR3gCAoPbMM/+psP3ii6/YlCR4UN4AgKC1deunmjLlbnM7\nPf1LG9MED7fdAQAAOJWhQ4fovffeNbejoqJ07rnn2pgoeFDeAICgc8UVl+nTTz+psO+HHw7alCb4\ncNkcABB0Tizutm3b6cCBfBvTBB/KGwAQVE68EUtiYm9t3pxhY5rgRHkDAILGoEEXV9hev/5tm5IE\nN8obABAU8vLylJGx1dxes2a9jWmCG+UNALBdWtr7io9vZW7/5jdJ+vWvf2NjouBGeQMAbHX48GFd\nd92VFfa9/PKrNqVxBsobAGCbPXv2qHPnNhX28c7y06O8AQC2+Ne/nlDv3gkV9lHc1UN5AwAst3nz\nZt13370V9lHc1Ud5AwAs9d1332nIkIEV9lHcNUN5AwAs4/P51KdP9wr7KO6ao7wBAJb51a+aVNim\nuGuH8gYAWOLE255KFLc/KG8AQMD9srgzMrbblCQ0UN4AgID6ZXGnpaXrrLPOsilNaOD7vAEAAVFW\nVqazzmpWYR+XyutGwMrb5/Np5syZysrKUlRUlFJTU9W6dWvz+Nq1a/XMM88oMjJS8fHxmjlzpiIi\nuBAAAKHAMIyTinvPngM2pQk9AWvLDRs2qKSkRCtWrNCkSZM0d+5c89ixY8f0t7/9TUuXLtXy5cvl\n9Xr1zjvvBCoKAMBiLVvGVtj+7LMsNWjQwKY0oSdgz7zT09OVlJQkSerRo4cyMzPNY1FRUVq+fLmi\no6Ml/XRppX79+oGKAgCwyLFjx9SqVYsK+/buzVVkZKRNiUJTwMrb6/XK4/GY25GRkSorK5Pb7VZE\nRISaN28uSXr22WdVVFSkfv36VXm+pk0byu2u28WPi4up0/PBP6xH8GAtgoeT1iIuLk4HDx6ssM8w\nDJvS1L1gWouAlbfH41FhYaG57fP55Ha7K2zPmzdP3377rR5//HG5XK4qz5ebW1Sn+eLiYpSTU1Cn\n50TtsR7Bg7UIHk5aiyFDLj+puA8cyHdM/tOxay0q+4MhYK95JyYmatOmTZKkjIwMxcfHVzg+Y8YM\nFRcX65///Kd5+RwA4Dx79+7V5s0fmtvt28fzrvIAC9gz74EDByotLU3JyckyDEOzZ8/WmjVrVFRU\npISEBK1cuVK9e/fWyJEjJUkjRozQwIEDT3NWAEAwWb/+VY0YcZO53avXr7Vu3Zs2JgoPLsMhL0jU\n9eUKJ12OCgesR/BgLYJHsK/FyJF/0Lp1ayrsC9Vn3GFz2RwAELpuv/3/wqa4gxF3WAMA1EjLlrEV\n3kXevHmcvvpql42Jwg/PvAEA1daiReMKxd2mTRuK2waUNwCgWn75BSP33DNVH3/8uU1pwhvlDQCo\n0vPPP3dScf/nP89q2rT7bEoEXvMGAJxSbm6uOnVqfdL+jIztfKWnzShvAMBJ+vfvox07tp+0f9eu\nbMXEBM9tQsMV5Q0AqKB9+3NVUHCkwr5XXnldF17Y16ZE+CXKGwBg6tDhHBUU/Px5bY/Ho2+++dHG\nRDgV3rAGAJD007vJ8/N/Lu6JEydT3EGK8gaAMHf33Xee9G7ysWPv1PTpM2xKhNPhsjkAhLE77hip\n1atXV9i3bdu3OuOMM2xKhOrgmTcAhKFjx47pjjt++lbHpk2bmvsPHMinuB2AZ94AEGaOl/ZxAwYM\n0JVXXqtrrrnOpkSoKcobAMLIL4tbksaMmaCePXvZkAa1RXkDQBjYvPlDLV78RIV9zZo115w5821K\nBH9Q3gAQwkpLS3XnnX88af+MGQ/p7LPPsSER6gLlDQAh6lSXyCXpySefsTgJ6hrlDQAh6FTFfd99\nM9WqVVsb0qCuUd4AEGJ+Wdzt23fSvfdOtykNAoHyBoAQMXbsrfL5fBX2TZ/+oFq3PvlrPeFslDcA\nOFxa2jtaunTJSfvnzHlUzZo1sz4QAo7yBgCHKiws1D33jDvlMd6UFtoobwBwoKef/rc++ui9k/Y/\n8cQSuVwuGxLBSpQ3ADjIF198poULF5y0PyXlIZ1zDp/bDheUNwA4wI8/ZuuBB079jnEukYcfyhsA\nglxlN1uZPXsB3wAWpihvAAhSEyZM0A8//HDS/vHj71H37ufbkAjBgvIGgCCzevV/tX792pP2ezwx\nmj9/oQ2JEGwobwAIEo8+Olfbt2875TFe18aJKG8AsFllr2lLfPQLp0Z5A4BNxo+/XWVlxac8lpr6\nV3Xt2k45OQUWp4ITUN4AYCGfz6exY2+t9DiXx1EdlDcAWOChh2Zoz57dlR7n8jhqgvIGgAApKSnR\nXXfdVunxiIhILVq02MJECBWUNwDUodNdFpekX/+6n0aNut2iRAhFlDcA1IGq3jF+3D//uViRkZEW\npEGoo7wBoJbKy8s1btyoKh9z/fU36vLLr7IoEcIF5Q0ANTRlygTl5x+p9Di3L0WgUd4AUA2nuyzu\nckXoiSeetigNwh3lDQCVOHjwoO67b1KVj+nWrbvuvLPqxwB1jfIGgBPs379XM2ZMq/Ixgwdfo2uv\nvd6iRMDJKG8AkDRlyp+Un59X6fEuXbpq4sSpFiYCKkd5AwhLR48e1cSJY077uHvumaZOnbpYkAio\nPsobQNiYPn2yDh3KOe3jRo8erz59+liQCKgdyhtAyHrppRV6443XqvXYAQMu0003jQhwIqBuUN4A\nQsbevT9q5sw/V/vxfB4bTkV5A3Cs/Px8LVnyhL788stqPf6GG5I1cODgAKcCAo/yBuAYpaWlevnl\n5RX2VVXc3bv31PjxEwMdC7Ac5Q0gqP33v89Wefzss89Wdna2uf3QQ/PVvHnzQMcCbEV5Awgqr732\nigoLK79v+C9Nn/6g3G7+V4bwwr/xAGxTUJCn9evX1OhnBg36nWJjYwOUCHAGyhuAJYqLi/XKKy/W\n+OcSEnqqS5eEACQCnIvyBhAQn376sXbtyqrxz3Xr1lOdO1PWQFUobwB+yc7+Xh98sLFWPxsRUU+D\nB/9ODRs2rMtIQMijvAFUy/79+7Vp0xt+neOSSy5X8+Yt6igREL4obwCmkpISrVmzRt9//73f54qP\nP0/nn59YB6kA/BLlDYShvLxcvfnm2jo5V+fO56lbN0oasBLlDYSo3NyDeuut12UYvjo6o0vXXjtM\n9erVq6PzAagtyhtwqKKiIr311nodO1ZYZ+eMjKyn5OQbVVISUWfnBFD3KG8gSO3Zs1sff5wmwyiv\n83N7PDEaNGiIIiMjTzoWGxujnJyCOp8TQN2hvAGL5ebuNcdHjx5VWlpaQOY599zWuvDCiwJybgD2\norwBP51YxjXlT3E3aNBQl1xyuTweT63PAcCZKG+Evdzc/ZLq6k1dNdOzZ09t3br1pP0REfXUq1cf\ntWnTzoZUAIJdwMrb5/Np5syZysrKUlRUlFJTU9W6dWvz+Ntvv61//OMfcrvdGjp0qG688cZARUEI\nKSgoUFmZ1+4YfnG5otWkSRNJUtOmZ6pDB24FCqBmAlbeGzZsUElJiVasWKGMjAzNnTtXixYtkiSV\nlpZqzpw5WrlypaKjo3XTTTfp0ksv5Tt4HSgv75AMo8Tv8/hz6dl+9dS0Kf/uArBOwMo7PT1dSUlJ\nkqQePXooMzPTPLZr1y61atXK/Fq/Xr166ZNPPtHgwYMDFaeC2NgGkqTmzXmt0B+FhYXKzfW/uO1H\n+QJwloCVt9frrfBGmsjISJWVlcntdsvr9SomJsY81qhRI3m9VV8Kbdq0odzukz/W4g+Xy1Wn5ws3\nVt+sIyIiQvXr11fz5s0VHR1t6dzhJi4u5vQPgiVYi+ARTGsRsPL2eDwqLPz55hE+n09ut/uUxwoL\nCyuU+ank5hbVab64OD7LWheaNj2zTs5Tk/Xwesvk9bJ2gcJ/G8GDtQgedq1FZX8wBOw2SomJidq0\naZMkKSMjQ/Hx8eax9u3ba/fu3crLy1NJSYm2bNminj17BioKAAAhJWDPvAcOHKi0tDQlJyfLMAzN\nnj1ba9asUVFRkYYNG6Zp06Zp9OjRMgxDQ4cOVcuWLQMVBQCAkOIyDMOwO0R11PXlCi5HBRfWI3iw\nFsGDtQgeYXPZHAAABAblDQCAw1DeAAA4DOUNAIDDUN4AADgM5Q0AgMNQ3gAAOAzlDQCAw1DeAAA4\nDOUNAIDDUN4AADiMY+5tDgAAfsIzbwAAHIbyBgDAYShvAAAchvIGAMBhKG8AAByG8gYAwGHcdgew\nms/n08yZM5WVlaWoqCilpqaqdevWdscKSaWlpZo+fbqys7NVUlKisWPHqkOHDpo2bZpcLpc6duyo\n+++/XxEREXrxxRe1fPlyud1ujR07VpdccomOHTumKVOm6NChQ2rUqJEefvhhNWvWzO5fy9EOHTqk\n66+/XosYUA2EAAAGQ0lEQVQXL5bb7WYtbPLkk0/q7bffVmlpqW666Sb16dOHtbBBaWmppk2bpuzs\nbEVERGjWrFnO+e/CCDOvv/66MXXqVMMwDGPr1q3GmDFjbE4UulauXGmkpqYahmEYubm5xsUXX2zc\ncccdxkcffWQYhmGkpKQYb7zxhnHgwAHj6quvNoqLi438/HxzvHjxYuPvf/+7YRiGsXbtWmPWrFm2\n/S6hoKSkxBg3bpwxaNAgY+fOnayFTT766CPjjjvuMMrLyw2v12v8/e9/Zy1s8uabbxoTJkwwDMMw\n3n//fePOO+90zFqE3WXz9PR0JSUlSZJ69OihzMxMmxOFriuuuEJ/+tOfJEmGYSgyMlJffvml+vTp\nI0m66KKL9MEHH+jzzz9Xz549FRUVpZiYGLVq1Urbt2+vsFYXXXSRPvzwQ9t+l1Dw8MMPKzk5WS1a\ntJAk1sIm77//vuLj4zV+/HiNGTNGAwYMYC1s0rZtW5WXl8vn88nr9crtdjtmLcKuvL1erzwej7kd\nGRmpsrIyGxOFrkaNGsnj8cjr9WrChAmaOHGiDMOQy+UyjxcUFMjr9SomJqbCz3m93gr7jz8WtbNq\n1So1a9bM/B+NJNbCJrm5ucrMzNRjjz2mBx54QJMnT2YtbNKwYUNlZ2dr8ODBSklJ0fDhwx2zFmH3\nmrfH41FhYaG57fP55HaH3T8Gy+zdu1fjx4/XzTffrCFDhmjevHnmscLCQjVu3PikNSksLFRMTEyF\n/ccfi9p56aWX5HK59OGHH2rbtm2aOnWqDh8+bB5nLazTpEkTtWvXTlFRUWrXrp3q16+vffv2mcdZ\nC+ssWbJE/fv316RJk7R3716NHDlSpaWl5vFgXouwe+admJioTZs2SZIyMjIUHx9vc6LQdfDgQY0a\nNUpTpkzRDTfcIEnq2rWrNm/eLEnatGmTevfure7duys9PV3FxcUqKCjQrl27FB8fr8TERL377rvm\nY3v16mXb7+J0zz//vJ577jk9++yz6tKlix5++GFddNFFrIUNevXqpffee0+GYWj//v06evSo+vbt\ny1rYoHHjxuYz59jYWJWVlTnm/1Fh98Ukx99tvmPHDhmGodmzZ6t9+/Z2xwpJqampWrdundq1a2fu\nu++++5SamqrS0lK1a9dOqampioyM1IsvvqgVK1bIMAzdcccduvzyy3X06FFNnTpVOTk5qlevnubP\nn6+4uDgbf6PQMHz4cM2cOVMRERFKSUlhLWzwyCOPaPPmzTIMQ3fffbfOOecc1sIGhYWFmj59unJy\nclRaWqoRI0YoISHBEWsRduUNAIDThd1lcwAAnI7yBgDAYShvAAAchvIGAMBhKG8AAByG8gbCyBdf\nfKEJEyZU+/GHDx9Wp06dApgIQG3wUTEAlTp8+LD69u2rrKwsu6MAOAH3BQXCyObNmzVr1iwlJCTI\n4/EoKytL+/btU7t27bRgwQI1atRIb7zxhh599FFFR0crISGhws//97//1bJly+Tz+dSkSROlpKSo\nbdu2uvXWW3Xeeefp3nvv1QcffKBp06Zp1apVat68uU2/KRDaKG8gTGVmZmrp0qVyuVy68cYbtX79\nel188cWaPn26li9frg4dOujJJ580H//xxx/r5Zdf1vPPP6/o6Gi9//77uuuuu/Taa69p3rx5uu66\n65SYmKhZs2Zp/vz5FDcQQJQ3EKaSkpIUFRUlSYqPj9eRI0eUnp6u+Ph4dejQQZI0bNgwLViwQJK0\nceNG7d69W8nJyeY5jhw5ory8PLVo0UKzZs3SuHHjdNddd+mCCy6w/hcCwgjlDYSpBg0amGOXy2V+\nFeKJb4M58Rv3fD6frrnmGk2ZMsXcPnDggGJjYyVJO3fuVPPmzfXFF19Y9BsA4Yt3mwMw9e7dWzt3\n7tT27dsl/fQ94Mf169dPr776qg4cOCBJWrZsmUaOHClJ+vzzz7V06VK99NJLys/P1zPPPGN9eCCM\n8MwbgKlZs2b661//qsmTJ6tevXoVLn8nJSXptttu06hRo+RyueTxeLRw4UIVFhbqnnvu0V/+8he1\nbNlSc+fO1e9//3tdcMEF6tq1q42/DRC6+KgYAAAOw2VzAAAchvIGAMBhKG8AAByG8gYAwGEobwAA\nHIbyBgDAYShvAAAchvIGAMBh/h/XNubAQlugZgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a16605a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(range(train_df.shape[0]), label[sorted_idx],s=3,c=np.sort(label_ord[sorted_idx]))\n",
    "plt.xlabel('index', fontsize=12)\n",
    "plt.ylabel('label', fontsize=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1a16605780>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAskAAAHfCAYAAACiUkX2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAICCSAYAgML/Bw8q2TPXlnrmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a16bfb198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''ulimit = np.percentile(train_df.label.values, 98)\n",
    "llimit = np.percentile(train_df.label.values, 2)\n",
    "train_df['label'].ix[train_df['label']>ulimit] = ulimit\n",
    "train_df['label'].ix[train_df['label']<llimit] = llimit'''\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.distplot(train_df.label.values, bins=50, kde=False)\n",
    "plt.xlabel('label', fontsize=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training - Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -3.46686086e-03  -9.20904637e-04  -8.03606985e-01  -7.73078363e-01\n",
      "  -3.56635604e-03  -7.72843486e-01  -1.18393606e-04  -9.97569510e-04\n",
      "  -7.48644283e-01  -8.52985102e-01  -2.84872124e-03  -7.72901335e-01\n",
      "  -6.64816878e-03  -1.01483779e-03  -7.10571434e-01  -7.82965571e-01\n",
      "  -5.51937200e-03  -8.13909000e-01   6.02760301e-03   6.02673678e-03\n",
      "   1.52910570e-03  -3.44319661e-01  -2.20519716e-03  -1.63086888e-02\n",
      "   1.64501047e-03  -3.35123698e-01  -3.73533559e-03   8.63693798e-04\n",
      "   1.63181312e-03  -3.29711914e-01  -1.04553928e-02   1.46484375e-03]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "(8192, 32)\n",
      "[2 4 0 ..., 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "feat=train_df.values[:,:-2]\n",
    "\n",
    "#Normalize the features\n",
    "\n",
    "feat_max = np.amax(feat,axis=0)\n",
    "feat_min = np.amin(feat,axis=0)\n",
    "\n",
    "feat=(feat-feat_min)/(feat_max-feat_min)\n",
    "feat=feat*2-1\n",
    "\n",
    "'''feat_mean = np.mean(feat,axis=0)\n",
    "feat_std = np.std(feat,axis=0)\n",
    "\n",
    "feat=(feat-feat_mean)/feat_std\n",
    "'''\n",
    "label_ord=train_df.values[:,-1].astype(np.int)\n",
    "\n",
    "print(np.mean(feat,axis=0))\n",
    "print(np.min(feat,axis=0))\n",
    "print(feat.shape)\n",
    "print(label_ord)\n",
    "\n",
    "fvec=feat.copy()\n",
    "#label=np.eye(num_classes)[label_ord]\n",
    "#print(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(7192,)\n",
      "(1000, 32)\n",
      "(7192, 32)\n"
     ]
    }
   ],
   "source": [
    "test_size = 1000\n",
    "\n",
    "test_idx=sorted_idx[np.floor(np.linspace(0,len(label_ord)-1,test_size)).astype(np.int)]\n",
    "train_idx = np.setdiff1d(np.arange(0,len(label_ord)),test_idx)\n",
    "\n",
    "\n",
    "\n",
    "label_ord_test=label_ord[test_idx]\n",
    "label_ord_train=label_ord[train_idx]\n",
    "label_test=np.eye(num_classes)[label_ord_test]\n",
    "label_train=np.eye(num_classes)[label_ord_train]\n",
    "fvec_test=fvec[test_idx,:]\n",
    "fvec_train=fvec[train_idx,:]\n",
    "\n",
    "print(label_ord_test.shape)from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.Ridge(alpha = .5)\n",
    "\n",
    "# Train the model using training sets\n",
    "regr.fit(fvec_train, label_ord_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "label_ord_pred = np.round(regr.predict(fvec_test)).astype(np.int)\n",
    "label_ord_pred[label_ord_pred<0]=0\n",
    "label_ord_pred[label_ord_pred>=num_classes]=num_classes-1\n",
    "\n",
    "label_ord_tr_pred = np.round(regr.predict(fvec_train)).astype(np.int)\n",
    "label_ord_tr_pred[label_ord_tr_pred<0]=0\n",
    "label_ord_tr_pred[label_ord_tr_pred>=num_classes]=num_classes-1\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "\n",
    "# The mean squared error\n",
    "print('Training')\n",
    "print(\"Mean absolute error: %.2f\"\n",
    "      % mean_absolute_error(label_ord_tr_pred, label_ord_train))\n",
    "\n",
    "#CCR \n",
    "print(\"Accuracy: %.2f\"\n",
    "      % np.mean(label_ord_tr_pred==label_ord_train))\n",
    "\n",
    "# The mean squared error\n",
    "print('Validation')\n",
    "print(\"Mean absolute error: %.2f\"\n",
    "      % mean_absolute_error(label_ord_pred, label_ord_test))\n",
    "\n",
    "#CCR \n",
    "print(\"Accuracy: %.2f\"\n",
    "      % np.mean(label_ord_pred==label_ord_test))\n",
    "print(label_ord_train.shape)\n",
    "print(fvec_test.shape)\n",
    "print(fvec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Load Data\n",
    "\n",
    "Before running the code, the data should be downloaded and foldered in the way that is usable for imagefolder function of the PyTorch. The following code assumes that the main directory for the dataset is 'data_dir' and it includes subdirectories for all of the separate classes.\n",
    "\n",
    "For details on how to create those folders, pleaserefer to 'dataset/Folder_images.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 7192, 'val': 1000}\n",
      "!!!!! NO CUDA GPUS DETECTED\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Macros\n",
    "'''\n",
    "#uniform_sampler=False\n",
    "batch_size=128\n",
    "\n",
    "dsets={'train': torch.utils.data.TensorDataset(torch.from_numpy(fvec_train).type(torch.FloatTensor),\n",
    "                                               torch.from_numpy(label_ord_train).type(torch.LongTensor)),\n",
    "       'val': torch.utils.data.TensorDataset(torch.from_numpy(fvec_test).type(torch.FloatTensor),\n",
    "                                             torch.from_numpy(label_ord_test).type(torch.LongTensor))}\n",
    "\n",
    "'''Define dataset loaders'''\n",
    "dset_loaders = {'train':torch.utils.data.DataLoader(dsets['train'], batch_size=batch_size,shuffle=True,\n",
    "                                                    num_workers=12),\n",
    "                'val':torch.utils.data.DataLoader(dsets['val'], batch_size=batch_size,shuffle=False,\n",
    "                                                    num_workers=12)}\n",
    "\n",
    "\n",
    "dset_sizes={'train':len(dsets['train']),'val':len(dsets['val'])}\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "print(dset_sizes)\n",
    "\n",
    "if use_gpu:\n",
    "    print('GPU is available')\n",
    "else:\n",
    "    print('!!!!! NO CUDA GPUS DETECTED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 32])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "inputs, classes = next(iter(dset_loaders['val']))\n",
    "print(inputs.shape)\n",
    "print(classes.shape)\n",
    "#print(dsets['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Log Keeping\n",
    "\n",
    "This section includes the functions defined for the log keeping. Since CNNs require lots of trials, I found it easy to record the properties of the each trial with their performances in an excel file. I also added tnesorboard summaries for every trial and individual text files for showing the details of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code declares the required parameters for the network which will be also used inside log keeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network='resnet34' #Initial network archtiecture.'loaded'forusing a saved network\n",
    "networkName='resnet18_real_sgd_multisoft_August29  19:06:27' #Directory for the saved network\n",
    "optimizer='adam' #Optimizer function\n",
    "iter_loc=14 #Number of the first column in the excel file for writing the results.\n",
    "end_to_end=True #Booolean to decide whether to train the network end-to-end or not.\n",
    "lr=0.01 #Initial learning rate\n",
    "momentum=0.9\n",
    "weight_decay=0.0005\n",
    "lr_scheduler=ft.exp_lr_scheduler #Learning rate scheduler\n",
    "lr_decay_epoch=10 #Number of epoch for learning rate decay\n",
    "pretrained=True \n",
    "mse_loss=False #Scalar MSE loss\n",
    "nclasses=5 #Number of output classes\n",
    "split=1000\n",
    "random_seed=1\n",
    "shuffle=True\n",
    "\n",
    "'''Multipliers for loss functions'''\n",
    "single_loss=1.\n",
    "multi_loss=0.\n",
    "\n",
    "comment=' ' #Additional comments if any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the functions for creating a text file and adding networkproperties to an excel file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeLog(logname):\n",
    "    '''\n",
    "    Creates a text file named Network_properties.txt inside runs/'logname'\n",
    "    '''\n",
    "    f=open('runs/'+logname+'/Network_properties.txt','w')\n",
    "    f.write('Batch size: '+str(batch_size)+'\\n')\n",
    "    f.write('Validation size: '+str(split)+'\\n')\n",
    "    f.write('Random seed: '+str(random_seed)+'\\n')\n",
    "    f.write('Shuffle: '+str(shuffle)+'\\n')\n",
    "    f.write('Validation size: '+str(split)+'\\n')\n",
    "    if mse_loss:\n",
    "        crt='MSE'\n",
    "    else:\n",
    "        crt=str(single_loss)+'xsingle + '+str(multi_loss)+'Xmulti'\n",
    "    f.write('Criterion: '+crt+'\\n')\n",
    "    f.write('Learning rate: '+str(lr)+'\\n')\n",
    "    f.write('Momentum: '+str(momentum)+'\\n')\n",
    "    f.write('Leraning Rate Scheduler: '+str(lr_scheduler)+'\\n')\n",
    "    f.write('Leraning Rate Decay Period: '+str(lr_decay_epoch)+'\\n')\n",
    "    f.write('MSE loss function: '+str(mse_loss)+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import time\n",
    "\n",
    "def writeLog_xlsx(logname='logs.xlsx',iter_loc=14):\n",
    "    '''\n",
    "    Adds a line to logs.xlsx with the network properties and outcomes.\n",
    "    :param iter_loc: First column to record the outcomes.\n",
    "    '''\n",
    "    book = openpyxl.load_workbook(logname)\n",
    "    sheet = book.active\n",
    "    if mse_loss:\n",
    "        crt='MSE'\n",
    "    else:\n",
    "        crt=str(single_loss)+'xsingle + '+str(multi_loss)+'Xmulti'\n",
    "    if network=='loaded':\n",
    "        specs=(datetime.now().strftime('%B%d  %H:%M:%S'),networkName,str(split),str(random_seed),str(shuffle),\n",
    "               optimizer, crt,str(lr),str(momentum),str(lr_scheduler),str(lr_decay_epoch),str(pretrained),\n",
    "               str(batch_size))\n",
    "    else:\n",
    "        specs=(datetime.now().strftime('%B%d  %H:%M:%S'),network,str(split),str(random_seed),str(shuffle),\n",
    "               optimizer, crt,str(lr),str(momentum),str(lr_scheduler),str(lr_decay_epoch),str(pretrained),\n",
    "               str(batch_size))\n",
    "    sheet.append(specs)\n",
    "    current_row = sheet.max_row\n",
    "    sheet.cell(row=current_row, column=iter_loc+5).value = comment\n",
    "    book.save(logname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Training and Validation\n",
    "\n",
    "In this part we will define the functions for training a CNN with different properties and loss functions.\n",
    "\n",
    "The following function takes bunch of properties defined in the beginning of Section-2 as input and creates network using those properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n",
      "  (fc0): Linear (32 -> 32)\n",
      "  (relu0): ReLU ()\n",
      "  (drop0): Dropout (p = 0.5)\n",
      "  (fc1): Linear (32 -> 16)\n",
      "  (relu1): ReLU ()\n",
      "  (drop1): Dropout (p = 0.5)\n",
      "  (fc2): Linear (16 -> 16)\n",
      "  (relu2): ReLU ()\n",
      "  (drop2): Dropout (p = 0.5)\n",
      "  (fc3): Linear (16 -> 12)\n",
      "  (relu3): ReLU ()\n",
      "  (drop3): Dropout (p = 0.5)\n",
      "  (fc4): Linear (12 -> 12)\n",
      "  (relu4): ReLU ()\n",
      "  (drop4): Dropout (p = 0.5)\n",
      "  (fc5): Linear (12 -> 8)\n",
      "  (relu5): ReLU ()\n",
      "  (drop5): Dropout (p = 0.5)\n",
      "  (fc6): Linear (8 -> 8)\n",
      "  (relu6): ReLU ()\n",
      "  (drop6): Dropout (p = 0.5)\n",
      "  (fc7): Linear (8 -> 5)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, dropouts, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.numHidden=len(hidden_sizes)\n",
    "        setattr(self, 'fc0', nn.Linear(input_size, hidden_sizes[0]))\n",
    "        setattr(self, 'relu0', nn.ReLU())\n",
    "        setattr(self, 'drop0', nn.Dropout(p=dropouts[0]))\n",
    "        for k in range(len(hidden_sizes)-1):\n",
    "            setattr(self, 'fc'+str(k+1), nn.Linear(hidden_sizes[k], hidden_sizes[k+1]))\n",
    "            setattr(self, 'relu'+str(k+1), nn.ReLU())\n",
    "            setattr(self, 'drop'+str(k+1), nn.Dropout(p=dropouts[k+1]))\n",
    "        setattr(self, 'fc'+str(len(hidden_sizes)), nn.Linear(hidden_sizes[-1], num_classes))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out=self.fc0(x)\n",
    "        out = self.relu0(out)\n",
    "        out = self.drop0(out)\n",
    "        for k in range(self.numHidden-1):\n",
    "            fc = getattr(self,'fc'+str(k+1))\n",
    "            relu = getattr(self,'relu'+str(k+1))\n",
    "            drop = getattr(self,'drop'+str(k+1))\n",
    "            out = fc(out)\n",
    "            out = relu(out)\n",
    "            out = drop(out)\n",
    "        fc = getattr(self,'fc'+str(self.numHidden))\n",
    "        out = fc(out)\n",
    "        return out\n",
    "    \n",
    "model=Net(32, [32, 16, 16, 12, 12, 8, 8],[.5 for k in range(7)], 5)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_loader(comment=comment,\n",
    "                    network=network,\n",
    "                    networkName=networkName,\n",
    "                    optimizer=optimizer,\n",
    "                    iter_loc=iter_loc,\n",
    "                    end_to_end=end_to_end,\n",
    "                    lr=lr,\n",
    "                    momentum=momentum,\n",
    "                    weight_decay=weight_decay,\n",
    "                    lr_scheduler=lr_scheduler,\n",
    "                    lr_decay_epoch=lr_decay_epoch,\n",
    "                    pretrained=pretrained,\n",
    "                    mse_loss=mse_loss,\n",
    "                    nclasses=nclasses,\n",
    "                    hidden_sizes = [32, 16, 12, 8],\n",
    "                    dropouts = [.5 for k in range(4)]):\n",
    "    \n",
    "    '''Load the network from pytorch'''\n",
    "    \n",
    "    model_ft = Net(32, hidden_sizes , dropouts, nclasses)\n",
    "\n",
    "    if use_gpu:\n",
    "        model_ft = model_ft.cuda()\n",
    "\n",
    "    '''Define the optimizer function'''\n",
    "    if(optimizer=='adam'):\n",
    "        optimizer_ft = optim.Adam(model_ft.parameters(),lr=lr,weight_decay=weight_decay)\n",
    "    elif(optimizer=='sgd'):\n",
    "        if(end_to_end):\n",
    "            optimizer_ft = optim.SGD(model_ft.parameters(), lr=lr, momentum=momentum)\n",
    "        else:\n",
    "            optimizer_ft = optim.SGD(model_ft.fc.parameters(), lr=lr, momentum=momentum,weight_decay=weight_decay)\n",
    "    return model_ft, optimizer_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define a simple function to be able to run our training in a single line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "importlib.reload(ft)\n",
    "    \n",
    "def run_network():\n",
    "    '''\n",
    "    Cretaes the log files and starts the training\n",
    "    '''\n",
    "    model_ft, optimizer_ft = network_loader(comment=comment, #'Tested for three rooms'\n",
    "                                            network=network,\n",
    "                                            networkName=networkName,\n",
    "                                            optimizer=optimizer,\n",
    "                                            iter_loc=iter_loc,\n",
    "                                            end_to_end=end_to_end,\n",
    "                                            lr=lr,\n",
    "                                            momentum=momentum,\n",
    "                                            weight_decay=weight_decay,\n",
    "                                            lr_scheduler=lr_scheduler,\n",
    "                                            lr_decay_epoch=lr_decay_epoch,\n",
    "                                            pretrained=pretrained,\n",
    "                                            mse_loss=mse_loss,\n",
    "                                            nclasses=nclasses)\n",
    "    \n",
    "    \n",
    "    '''Name of the trial'''\n",
    "    if mse_loss:\n",
    "        crt='MSE'\n",
    "    else:\n",
    "        crt=str(single_loss)+'xsingle + '+str(multi_loss)+'Xmulti'\n",
    "    logname=network+'_'+'_'+optimizer+'_'+crt+'_'+datetime.now().strftime('%B%d  %H:%M:%S')\n",
    "    writer = SummaryWriter('runs/'+logname) #For tensorboard\n",
    "    writeLog(logname)\n",
    "    writeLog_xlsx()\n",
    "    \n",
    "    '''Start trianing'''\n",
    "    best_model, last_model = ft.train_model(model_ft,optimizer_ft, lr_scheduler,dset_loaders,\n",
    "                            dset_sizes,writer,use_gpu=use_gpu,num_epochs=50,batch_size=batch_size,num_log=250,\n",
    "                            multi_prob=False,lr_decay_epoch=lr_decay_epoch,init_lr=lr,mse_loss=mse_loss,\n",
    "                            iter_loc=iter_loc,cross_loss=single_loss,multi_loss=multi_loss,numOut=nclasses)\n",
    "    \n",
    "    '''Save the models'''\n",
    "    torch.save(best_model,'./saved_models/'+logname+'_best')\n",
    "    torch.save(last_model,'./saved_models/'+logname+'_last')\n",
    "    \n",
    "    '''Free up the memory'''\n",
    "    del model_ft\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function we used in the experiments is the following,\n",
    "\n",
    "$$loss(\\mathbf{y},\\hat{y})=(1-\\lambda )loss_{single}(\\mathbf{y},\\hat{y})+\\lambda loss_{multi}(\\mathbf{y},\\hat{y})$$\n",
    "where $\\mathbf{y}$ is the ground truth and $\\hat{y}$ is the prediction.\n",
    "\n",
    "Now lets test our function for different values of $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "LR is set to 0.01\n",
      "train Loss: 0.0106 Acc: 0.1940 CIR-1: 0.4915 RMSE 2.1311\n",
      "val Loss: 0.0107 Acc: 0.2710 CIR-1: 0.5900 RMSE 1.9834\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 0.0105 Acc: 0.2184 CIR-1: 0.5185 RMSE 2.0355\n",
      "val Loss: 0.0102 Acc: 0.3050 CIR-1: 0.6900 RMSE 1.5614\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 0.0102 Acc: 0.2741 CIR-1: 0.6452 RMSE 1.6966\n",
      "val Loss: 0.0098 Acc: 0.3420 CIR-1: 0.6500 RMSE 1.6685\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.0099 Acc: 0.2960 CIR-1: 0.6844 RMSE 1.5667\n",
      "val Loss: 0.0094 Acc: 0.3580 CIR-1: 0.6970 RMSE 1.5531\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.0097 Acc: 0.3219 CIR-1: 0.6799 RMSE 1.5466\n",
      "val Loss: 0.0090 Acc: 0.3950 CIR-1: 0.7820 RMSE 1.2822\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.0096 Acc: 0.3397 CIR-1: 0.6773 RMSE 1.5419\n",
      "val Loss: 0.0089 Acc: 0.4080 CIR-1: 0.7300 RMSE 1.4107\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.0095 Acc: 0.3401 CIR-1: 0.6962 RMSE 1.5050\n",
      "val Loss: 0.0090 Acc: 0.4030 CIR-1: 0.7410 RMSE 1.4153\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.0095 Acc: 0.3448 CIR-1: 0.6845 RMSE 1.5538\n",
      "val Loss: 0.0090 Acc: 0.3770 CIR-1: 0.7430 RMSE 1.3892\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.0094 Acc: 0.3375 CIR-1: 0.7154 RMSE 1.4530\n",
      "val Loss: 0.0087 Acc: 0.3830 CIR-1: 0.7040 RMSE 1.4826\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.0095 Acc: 0.3475 CIR-1: 0.7008 RMSE 1.4912\n",
      "val Loss: 0.0087 Acc: 0.3990 CIR-1: 0.8090 RMSE 1.2272\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "LR is set to 0.001\n",
      "train Loss: 0.0093 Acc: 0.3617 CIR-1: 0.7207 RMSE 1.4129\n",
      "val Loss: 0.0087 Acc: 0.4030 CIR-1: 0.7820 RMSE 1.2845\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.0093 Acc: 0.3536 CIR-1: 0.7318 RMSE 1.3962\n",
      "val Loss: 0.0086 Acc: 0.4060 CIR-1: 0.7760 RMSE 1.3103\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.0092 Acc: 0.3676 CIR-1: 0.7308 RMSE 1.3957\n",
      "val Loss: 0.0086 Acc: 0.4080 CIR-1: 0.7750 RMSE 1.3042\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.0092 Acc: 0.3610 CIR-1: 0.7283 RMSE 1.3974\n",
      "val Loss: 0.0085 Acc: 0.4060 CIR-1: 0.7880 RMSE 1.2637\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.0091 Acc: 0.3758 CIR-1: 0.7433 RMSE 1.3731\n",
      "val Loss: 0.0085 Acc: 0.4060 CIR-1: 0.7650 RMSE 1.3153\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.0092 Acc: 0.3693 CIR-1: 0.7440 RMSE 1.3713\n",
      "val Loss: 0.0086 Acc: 0.3920 CIR-1: 0.7660 RMSE 1.3289\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.0091 Acc: 0.3696 CIR-1: 0.7396 RMSE 1.3663\n",
      "val Loss: 0.0086 Acc: 0.4030 CIR-1: 0.7810 RMSE 1.2771\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.0092 Acc: 0.3685 CIR-1: 0.7471 RMSE 1.3610\n",
      "val Loss: 0.0086 Acc: 0.3980 CIR-1: 0.7700 RMSE 1.3023\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.0091 Acc: 0.3740 CIR-1: 0.7462 RMSE 1.3712\n",
      "val Loss: 0.0085 Acc: 0.4020 CIR-1: 0.7840 RMSE 1.2732\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.0091 Acc: 0.3740 CIR-1: 0.7380 RMSE 1.3733\n",
      "val Loss: 0.0085 Acc: 0.4000 CIR-1: 0.7760 RMSE 1.2861\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "LR is set to 0.00010000000000000002\n",
      "train Loss: 0.0092 Acc: 0.3678 CIR-1: 0.7430 RMSE 1.3670\n",
      "val Loss: 0.0085 Acc: 0.4030 CIR-1: 0.7830 RMSE 1.2720\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.0091 Acc: 0.3697 CIR-1: 0.7481 RMSE 1.3509\n",
      "val Loss: 0.0085 Acc: 0.4060 CIR-1: 0.7840 RMSE 1.2649\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.0091 Acc: 0.3679 CIR-1: 0.7544 RMSE 1.3388\n",
      "val Loss: 0.0085 Acc: 0.4080 CIR-1: 0.7850 RMSE 1.2649\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.0090 Acc: 0.3839 CIR-1: 0.7622 RMSE 1.3314\n",
      "val Loss: 0.0085 Acc: 0.4080 CIR-1: 0.7880 RMSE 1.2641\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.0090 Acc: 0.3800 CIR-1: 0.7561 RMSE 1.3222\n",
      "val Loss: 0.0085 Acc: 0.4040 CIR-1: 0.7840 RMSE 1.2704\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.0090 Acc: 0.3761 CIR-1: 0.7556 RMSE 1.3420\n",
      "val Loss: 0.0085 Acc: 0.4040 CIR-1: 0.7830 RMSE 1.2716\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.0091 Acc: 0.3704 CIR-1: 0.7518 RMSE 1.3532\n",
      "val Loss: 0.0085 Acc: 0.4020 CIR-1: 0.7810 RMSE 1.2748\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.0091 Acc: 0.3764 CIR-1: 0.7472 RMSE 1.3567\n",
      "val Loss: 0.0085 Acc: 0.4030 CIR-1: 0.7860 RMSE 1.2685\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.0091 Acc: 0.3820 CIR-1: 0.7503 RMSE 1.3381\n",
      "val Loss: 0.0085 Acc: 0.4090 CIR-1: 0.7910 RMSE 1.2602\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.0091 Acc: 0.3685 CIR-1: 0.7472 RMSE 1.3425\n",
      "val Loss: 0.0085 Acc: 0.4040 CIR-1: 0.7910 RMSE 1.2621\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "LR is set to 1.0000000000000003e-05\n",
      "train Loss: 0.0091 Acc: 0.3729 CIR-1: 0.7504 RMSE 1.3508\n",
      "val Loss: 0.0085 Acc: 0.4050 CIR-1: 0.7890 RMSE 1.2641\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.0091 Acc: 0.3779 CIR-1: 0.7533 RMSE 1.3430\n",
      "val Loss: 0.0085 Acc: 0.4040 CIR-1: 0.7900 RMSE 1.2633\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.0091 Acc: 0.3783 CIR-1: 0.7561 RMSE 1.3405\n",
      "val Loss: 0.0085 Acc: 0.4040 CIR-1: 0.7920 RMSE 1.2610\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.0091 Acc: 0.3785 CIR-1: 0.7543 RMSE 1.3404\n",
      "val Loss: 0.0085 Acc: 0.4050 CIR-1: 0.7920 RMSE 1.2606\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.0090 Acc: 0.3807 CIR-1: 0.7561 RMSE 1.3290\n",
      "val Loss: 0.0085 Acc: 0.4050 CIR-1: 0.7920 RMSE 1.2606\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.0091 Acc: 0.3817 CIR-1: 0.7538 RMSE 1.3405\n",
      "val Loss: 0.0085 Acc: 0.4060 CIR-1: 0.7910 RMSE 1.2613\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.0091 Acc: 0.3820 CIR-1: 0.7533 RMSE 1.3430\n",
      "val Loss: 0.0085 Acc: 0.4050 CIR-1: 0.7910 RMSE 1.2617\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.0090 Acc: 0.3761 CIR-1: 0.7614 RMSE 1.3275\n",
      "val Loss: 0.0085 Acc: 0.4050 CIR-1: 0.7890 RMSE 1.2641\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.0091 Acc: 0.3774 CIR-1: 0.7585 RMSE 1.3288\n",
      "val Loss: 0.0085 Acc: 0.4050 CIR-1: 0.7910 RMSE 1.2617\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.0091 Acc: 0.3729 CIR-1: 0.7532 RMSE 1.3420\n",
      "val Loss: 0.0085 Acc: 0.4050 CIR-1: 0.7890 RMSE 1.2641\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "LR is set to 1.0000000000000002e-06\n",
      "train Loss: 0.0090 Acc: 0.3868 CIR-1: 0.7610 RMSE 1.3211\n",
      "val Loss: 0.0085 Acc: 0.4050 CIR-1: 0.7890 RMSE 1.2641\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.0091 Acc: 0.3680 CIR-1: 0.7486 RMSE 1.3465\n",
      "val Loss: 0.0085 Acc: 0.4050 CIR-1: 0.7890 RMSE 1.2641\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.0091 Acc: 0.3686 CIR-1: 0.7550 RMSE 1.3443\n",
      "val Loss: 0.0085 Acc: 0.4050 CIR-1: 0.7890 RMSE 1.2641\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.0090 Acc: 0.3767 CIR-1: 0.7564 RMSE 1.3380\n",
      "val Loss: 0.0085 Acc: 0.4050 CIR-1: 0.7890 RMSE 1.2641\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.0091 Acc: 0.3817 CIR-1: 0.7510 RMSE 1.3461\n",
      "val Loss: 0.0085 Acc: 0.4050 CIR-1: 0.7890 RMSE 1.2641\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.0091 Acc: 0.3718 CIR-1: 0.7513 RMSE 1.3457\n",
      "val Loss: 0.0085 Acc: 0.4050 CIR-1: 0.7890 RMSE 1.2641\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.0091 Acc: 0.3719 CIR-1: 0.7456 RMSE 1.3463\n",
      "val Loss: 0.0085 Acc: 0.4050 CIR-1: 0.7890 RMSE 1.2641\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.0091 Acc: 0.3776 CIR-1: 0.7550 RMSE 1.3407\n",
      "val Loss: 0.0085 Acc: 0.4050 CIR-1: 0.7890 RMSE 1.2641\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.0091 Acc: 0.3786 CIR-1: 0.7514 RMSE 1.3424\n",
      "val Loss: 0.0085 Acc: 0.4050 CIR-1: 0.7890 RMSE 1.2641\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.0090 Acc: 0.3839 CIR-1: 0.7589 RMSE 1.3244\n",
      "val Loss: 0.0085 Acc: 0.4050 CIR-1: 0.7890 RMSE 1.2641\n",
      "\n",
      "Training complete in 0m 45s\n",
      "Best val RMSE: 1.227192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozan-macbook-air/anaconda/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "lmbda =\n",
    "\n",
    "single_loss=1.-lmbda\n",
    "multi_loss = lmbda\n",
    "run_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single loss = 0.6, Multi loss = 0.4\n",
      "Epoch 0/49\n",
      "----------\n",
      "LR is set to 0.01\n",
      "train Loss: 0.0099 Acc: 0.1984 CIR-1: 0.5257 RMSE 1.9611\n",
      "val Loss: 0.0099 Acc: 0.1990 CIR-1: 0.5980 RMSE 1.7384\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 0.0098 Acc: 0.2051 CIR-1: 0.5923 RMSE 1.6972\n",
      "val Loss: 0.0099 Acc: 0.1990 CIR-1: 0.6010 RMSE 1.7300\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 0.0098 Acc: 0.2063 CIR-1: 0.6046 RMSE 1.6693\n",
      "val Loss: 0.0098 Acc: 0.2400 CIR-1: 0.6810 RMSE 1.4567\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.0096 Acc: 0.2453 CIR-1: 0.6311 RMSE 1.7018\n",
      "val Loss: 0.0093 Acc: 0.3550 CIR-1: 0.6550 RMSE 1.6763\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.0094 Acc: 0.2784 CIR-1: 0.6634 RMSE 1.6120\n",
      "val Loss: 0.0090 Acc: 0.3850 CIR-1: 0.7370 RMSE 1.4181\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.0093 Acc: 0.3027 CIR-1: 0.6648 RMSE 1.5917\n",
      "val Loss: 0.0087 Acc: 0.3560 CIR-1: 0.7980 RMSE 1.2422\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.0092 Acc: 0.3106 CIR-1: 0.6881 RMSE 1.5206\n",
      "val Loss: 0.0087 Acc: 0.4060 CIR-1: 0.8100 RMSE 1.1954\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.0091 Acc: 0.3181 CIR-1: 0.6908 RMSE 1.4985\n",
      "val Loss: 0.0085 Acc: 0.3540 CIR-1: 0.8110 RMSE 1.2174\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.0090 Acc: 0.3258 CIR-1: 0.7015 RMSE 1.5053\n",
      "val Loss: 0.0084 Acc: 0.3490 CIR-1: 0.8050 RMSE 1.2256\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.0090 Acc: 0.3254 CIR-1: 0.7023 RMSE 1.4881\n",
      "val Loss: 0.0085 Acc: 0.3960 CIR-1: 0.7330 RMSE 1.3979\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "LR is set to 0.001\n",
      "train Loss: 0.0089 Acc: 0.3454 CIR-1: 0.6920 RMSE 1.4849\n",
      "val Loss: 0.0083 Acc: 0.3960 CIR-1: 0.7670 RMSE 1.3031\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.0088 Acc: 0.3557 CIR-1: 0.7169 RMSE 1.4197\n",
      "val Loss: 0.0083 Acc: 0.4170 CIR-1: 0.7810 RMSE 1.2438\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.0088 Acc: 0.3555 CIR-1: 0.7418 RMSE 1.3799\n",
      "val Loss: 0.0082 Acc: 0.4190 CIR-1: 0.7800 RMSE 1.2486\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.0087 Acc: 0.3522 CIR-1: 0.7329 RMSE 1.3876\n",
      "val Loss: 0.0082 Acc: 0.4070 CIR-1: 0.7680 RMSE 1.2724\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.0088 Acc: 0.3561 CIR-1: 0.7321 RMSE 1.3973\n",
      "val Loss: 0.0082 Acc: 0.4180 CIR-1: 0.7760 RMSE 1.2438\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.0087 Acc: 0.3476 CIR-1: 0.7311 RMSE 1.4033\n",
      "val Loss: 0.0082 Acc: 0.4150 CIR-1: 0.7750 RMSE 1.2629\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.0088 Acc: 0.3458 CIR-1: 0.7336 RMSE 1.4025\n",
      "val Loss: 0.0082 Acc: 0.4100 CIR-1: 0.7910 RMSE 1.2157\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.0087 Acc: 0.3472 CIR-1: 0.7454 RMSE 1.3638\n",
      "val Loss: 0.0082 Acc: 0.4090 CIR-1: 0.7690 RMSE 1.2732\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.0087 Acc: 0.3623 CIR-1: 0.7339 RMSE 1.3891\n",
      "val Loss: 0.0082 Acc: 0.4100 CIR-1: 0.7720 RMSE 1.2677\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.0087 Acc: 0.3512 CIR-1: 0.7301 RMSE 1.4042\n",
      "val Loss: 0.0082 Acc: 0.4100 CIR-1: 0.7750 RMSE 1.2570\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "LR is set to 0.00010000000000000002\n",
      "train Loss: 0.0087 Acc: 0.3585 CIR-1: 0.7503 RMSE 1.3878\n",
      "val Loss: 0.0082 Acc: 0.4100 CIR-1: 0.7770 RMSE 1.2546\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.0087 Acc: 0.3522 CIR-1: 0.7482 RMSE 1.3852\n",
      "val Loss: 0.0081 Acc: 0.4130 CIR-1: 0.7750 RMSE 1.2578\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.0087 Acc: 0.3636 CIR-1: 0.7494 RMSE 1.3835\n",
      "val Loss: 0.0081 Acc: 0.4060 CIR-1: 0.7710 RMSE 1.2653\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.0087 Acc: 0.3573 CIR-1: 0.7478 RMSE 1.3966\n",
      "val Loss: 0.0081 Acc: 0.4130 CIR-1: 0.7730 RMSE 1.2602\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.0087 Acc: 0.3598 CIR-1: 0.7507 RMSE 1.3888\n",
      "val Loss: 0.0081 Acc: 0.4080 CIR-1: 0.7720 RMSE 1.2613\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.0086 Acc: 0.3662 CIR-1: 0.7513 RMSE 1.3794\n",
      "val Loss: 0.0081 Acc: 0.4100 CIR-1: 0.7710 RMSE 1.2617\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.0087 Acc: 0.3523 CIR-1: 0.7429 RMSE 1.3963\n",
      "val Loss: 0.0081 Acc: 0.4030 CIR-1: 0.7720 RMSE 1.2633\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.0086 Acc: 0.3614 CIR-1: 0.7499 RMSE 1.3870\n",
      "val Loss: 0.0081 Acc: 0.4030 CIR-1: 0.7700 RMSE 1.2657\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.0087 Acc: 0.3614 CIR-1: 0.7489 RMSE 1.3839\n",
      "val Loss: 0.0081 Acc: 0.4100 CIR-1: 0.7680 RMSE 1.2673\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.0087 Acc: 0.3572 CIR-1: 0.7476 RMSE 1.4007\n",
      "val Loss: 0.0081 Acc: 0.4030 CIR-1: 0.7670 RMSE 1.2673\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "LR is set to 1.0000000000000003e-05\n",
      "train Loss: 0.0087 Acc: 0.3505 CIR-1: 0.7506 RMSE 1.3910\n",
      "val Loss: 0.0081 Acc: 0.4010 CIR-1: 0.7660 RMSE 1.2693\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.0086 Acc: 0.3623 CIR-1: 0.7518 RMSE 1.3865\n",
      "val Loss: 0.0081 Acc: 0.4020 CIR-1: 0.7660 RMSE 1.2689\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.0087 Acc: 0.3569 CIR-1: 0.7489 RMSE 1.3883\n",
      "val Loss: 0.0081 Acc: 0.4010 CIR-1: 0.7660 RMSE 1.2693\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.0086 Acc: 0.3642 CIR-1: 0.7487 RMSE 1.3748\n",
      "val Loss: 0.0081 Acc: 0.4010 CIR-1: 0.7660 RMSE 1.2693\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.0087 Acc: 0.3579 CIR-1: 0.7500 RMSE 1.3803\n",
      "val Loss: 0.0081 Acc: 0.4030 CIR-1: 0.7660 RMSE 1.2685\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.0087 Acc: 0.3604 CIR-1: 0.7449 RMSE 1.3907\n",
      "val Loss: 0.0081 Acc: 0.4030 CIR-1: 0.7660 RMSE 1.2724\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.0086 Acc: 0.3607 CIR-1: 0.7565 RMSE 1.3596\n",
      "val Loss: 0.0081 Acc: 0.4040 CIR-1: 0.7660 RMSE 1.2700\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.0086 Acc: 0.3600 CIR-1: 0.7486 RMSE 1.3944\n",
      "val Loss: 0.0081 Acc: 0.4040 CIR-1: 0.7660 RMSE 1.2700\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.0087 Acc: 0.3619 CIR-1: 0.7475 RMSE 1.3857\n",
      "val Loss: 0.0081 Acc: 0.4040 CIR-1: 0.7660 RMSE 1.2720\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.0086 Acc: 0.3564 CIR-1: 0.7487 RMSE 1.3806\n",
      "val Loss: 0.0081 Acc: 0.4040 CIR-1: 0.7660 RMSE 1.2720\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "LR is set to 1.0000000000000002e-06\n",
      "train Loss: 0.0087 Acc: 0.3540 CIR-1: 0.7486 RMSE 1.3865\n",
      "val Loss: 0.0081 Acc: 0.4040 CIR-1: 0.7660 RMSE 1.2720\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.0087 Acc: 0.3640 CIR-1: 0.7522 RMSE 1.3815\n",
      "val Loss: 0.0081 Acc: 0.4040 CIR-1: 0.7660 RMSE 1.2720\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.0086 Acc: 0.3623 CIR-1: 0.7496 RMSE 1.3789\n",
      "val Loss: 0.0081 Acc: 0.4050 CIR-1: 0.7660 RMSE 1.2716\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.0087 Acc: 0.3528 CIR-1: 0.7401 RMSE 1.3999\n",
      "val Loss: 0.0081 Acc: 0.4040 CIR-1: 0.7660 RMSE 1.2720\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.0087 Acc: 0.3591 CIR-1: 0.7510 RMSE 1.3772\n",
      "val Loss: 0.0081 Acc: 0.4040 CIR-1: 0.7660 RMSE 1.2720\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.0086 Acc: 0.3648 CIR-1: 0.7510 RMSE 1.3732\n",
      "val Loss: 0.0081 Acc: 0.4040 CIR-1: 0.7660 RMSE 1.2720\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.0086 Acc: 0.3540 CIR-1: 0.7481 RMSE 1.3896\n",
      "val Loss: 0.0081 Acc: 0.4040 CIR-1: 0.7660 RMSE 1.2720\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.0087 Acc: 0.3562 CIR-1: 0.7458 RMSE 1.3961\n",
      "val Loss: 0.0081 Acc: 0.4040 CIR-1: 0.7660 RMSE 1.2720\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.0087 Acc: 0.3562 CIR-1: 0.7450 RMSE 1.3909\n",
      "val Loss: 0.0081 Acc: 0.4050 CIR-1: 0.7660 RMSE 1.2716\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.0087 Acc: 0.3555 CIR-1: 0.7467 RMSE 1.3884\n",
      "val Loss: 0.0081 Acc: 0.4050 CIR-1: 0.7660 RMSE 1.2716\n",
      "\n",
      "Training complete in 0m 46s\n",
      "Best val RMSE: 1.195408\n",
      "Single loss = 0.5, Multi loss = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozan-macbook-air/anaconda/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "LR is set to 0.01\n",
      "train Loss: 0.0091 Acc: 0.2019 CIR-1: 0.5246 RMSE 1.9583\n",
      "val Loss: 0.0092 Acc: 0.2340 CIR-1: 0.6970 RMSE 1.5395\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 0.0091 Acc: 0.2157 CIR-1: 0.6338 RMSE 1.6528\n",
      "val Loss: 0.0090 Acc: 0.2720 CIR-1: 0.7390 RMSE 1.4269\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 0.0088 Acc: 0.2624 CIR-1: 0.6813 RMSE 1.5573\n",
      "val Loss: 0.0086 Acc: 0.3470 CIR-1: 0.6930 RMSE 1.5304\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.0086 Acc: 0.2823 CIR-1: 0.7129 RMSE 1.4790\n",
      "val Loss: 0.0083 Acc: 0.3240 CIR-1: 0.6500 RMSE 1.7079\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.0085 Acc: 0.3017 CIR-1: 0.7075 RMSE 1.5031\n",
      "val Loss: 0.0081 Acc: 0.3510 CIR-1: 0.6990 RMSE 1.5456\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.0084 Acc: 0.3033 CIR-1: 0.6869 RMSE 1.5672\n",
      "val Loss: 0.0077 Acc: 0.3930 CIR-1: 0.8120 RMSE 1.2128\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.0083 Acc: 0.2978 CIR-1: 0.7215 RMSE 1.4734\n",
      "val Loss: 0.0078 Acc: 0.3870 CIR-1: 0.7390 RMSE 1.3979\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.0083 Acc: 0.3083 CIR-1: 0.7157 RMSE 1.4819\n",
      "val Loss: 0.0077 Acc: 0.3840 CIR-1: 0.7620 RMSE 1.3777\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.0083 Acc: 0.3273 CIR-1: 0.7222 RMSE 1.4561\n",
      "val Loss: 0.0075 Acc: 0.4060 CIR-1: 0.7650 RMSE 1.3217\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.0082 Acc: 0.3259 CIR-1: 0.7322 RMSE 1.4155\n",
      "val Loss: 0.0074 Acc: 0.4110 CIR-1: 0.7710 RMSE 1.3142\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "LR is set to 0.001\n",
      "train Loss: 0.0080 Acc: 0.3436 CIR-1: 0.7472 RMSE 1.3721\n",
      "val Loss: 0.0074 Acc: 0.4100 CIR-1: 0.7760 RMSE 1.2841\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.0080 Acc: 0.3547 CIR-1: 0.7563 RMSE 1.3499\n",
      "val Loss: 0.0074 Acc: 0.4180 CIR-1: 0.7810 RMSE 1.2771\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.0079 Acc: 0.3505 CIR-1: 0.7542 RMSE 1.3520\n",
      "val Loss: 0.0074 Acc: 0.4190 CIR-1: 0.7880 RMSE 1.2490\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.0079 Acc: 0.3529 CIR-1: 0.7582 RMSE 1.3419\n",
      "val Loss: 0.0074 Acc: 0.4160 CIR-1: 0.7920 RMSE 1.2554\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.0079 Acc: 0.3621 CIR-1: 0.7653 RMSE 1.3174\n",
      "val Loss: 0.0073 Acc: 0.4100 CIR-1: 0.8000 RMSE 1.2402\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.0078 Acc: 0.3630 CIR-1: 0.7738 RMSE 1.3013\n",
      "val Loss: 0.0073 Acc: 0.4120 CIR-1: 0.8100 RMSE 1.2182\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.0078 Acc: 0.3614 CIR-1: 0.7741 RMSE 1.3113\n",
      "val Loss: 0.0073 Acc: 0.4240 CIR-1: 0.8100 RMSE 1.1967\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.0079 Acc: 0.3625 CIR-1: 0.7684 RMSE 1.3023\n",
      "val Loss: 0.0073 Acc: 0.4100 CIR-1: 0.8010 RMSE 1.2450\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.0079 Acc: 0.3600 CIR-1: 0.7735 RMSE 1.2916\n",
      "val Loss: 0.0073 Acc: 0.4100 CIR-1: 0.8150 RMSE 1.2239\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.0078 Acc: 0.3642 CIR-1: 0.7736 RMSE 1.3004\n",
      "val Loss: 0.0073 Acc: 0.4070 CIR-1: 0.8090 RMSE 1.2145\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "LR is set to 0.00010000000000000002\n",
      "train Loss: 0.0078 Acc: 0.3607 CIR-1: 0.7703 RMSE 1.2940\n",
      "val Loss: 0.0073 Acc: 0.4200 CIR-1: 0.8160 RMSE 1.1900\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.0078 Acc: 0.3548 CIR-1: 0.7728 RMSE 1.2914\n",
      "val Loss: 0.0073 Acc: 0.4270 CIR-1: 0.8140 RMSE 1.1916\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.0078 Acc: 0.3725 CIR-1: 0.7802 RMSE 1.2743\n",
      "val Loss: 0.0073 Acc: 0.4250 CIR-1: 0.8130 RMSE 1.1916\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.0079 Acc: 0.3667 CIR-1: 0.7766 RMSE 1.2827\n",
      "val Loss: 0.0073 Acc: 0.4070 CIR-1: 0.8100 RMSE 1.2161\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.0078 Acc: 0.3699 CIR-1: 0.7761 RMSE 1.2837\n",
      "val Loss: 0.0073 Acc: 0.4100 CIR-1: 0.8110 RMSE 1.2046\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.0078 Acc: 0.3619 CIR-1: 0.7768 RMSE 1.2906\n",
      "val Loss: 0.0073 Acc: 0.4090 CIR-1: 0.8090 RMSE 1.2145\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.0078 Acc: 0.3675 CIR-1: 0.7749 RMSE 1.2833\n",
      "val Loss: 0.0073 Acc: 0.4170 CIR-1: 0.8090 RMSE 1.2153\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.0077 Acc: 0.3668 CIR-1: 0.7798 RMSE 1.2742\n",
      "val Loss: 0.0072 Acc: 0.4200 CIR-1: 0.8130 RMSE 1.2071\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.0078 Acc: 0.3636 CIR-1: 0.7866 RMSE 1.2695\n",
      "val Loss: 0.0072 Acc: 0.4220 CIR-1: 0.8110 RMSE 1.2087\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.0079 Acc: 0.3665 CIR-1: 0.7727 RMSE 1.2864\n",
      "val Loss: 0.0072 Acc: 0.4170 CIR-1: 0.8120 RMSE 1.2095\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "LR is set to 1.0000000000000003e-05\n",
      "train Loss: 0.0078 Acc: 0.3733 CIR-1: 0.7749 RMSE 1.2807\n",
      "val Loss: 0.0072 Acc: 0.4170 CIR-1: 0.8110 RMSE 1.2108\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.0078 Acc: 0.3654 CIR-1: 0.7761 RMSE 1.2805\n",
      "val Loss: 0.0072 Acc: 0.4170 CIR-1: 0.8120 RMSE 1.2095\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.0077 Acc: 0.3690 CIR-1: 0.7803 RMSE 1.2705\n",
      "val Loss: 0.0072 Acc: 0.4170 CIR-1: 0.8120 RMSE 1.2095\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.0078 Acc: 0.3682 CIR-1: 0.7756 RMSE 1.2824\n",
      "val Loss: 0.0072 Acc: 0.4190 CIR-1: 0.8120 RMSE 1.2087\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.0078 Acc: 0.3687 CIR-1: 0.7775 RMSE 1.2845\n",
      "val Loss: 0.0072 Acc: 0.4200 CIR-1: 0.8130 RMSE 1.2071\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.0078 Acc: 0.3712 CIR-1: 0.7774 RMSE 1.2865\n",
      "val Loss: 0.0072 Acc: 0.4190 CIR-1: 0.8130 RMSE 1.2075\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.0078 Acc: 0.3591 CIR-1: 0.7732 RMSE 1.2894\n",
      "val Loss: 0.0072 Acc: 0.4200 CIR-1: 0.8130 RMSE 1.2050\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.0078 Acc: 0.3642 CIR-1: 0.7759 RMSE 1.2896\n",
      "val Loss: 0.0072 Acc: 0.4190 CIR-1: 0.8140 RMSE 1.2042\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.0078 Acc: 0.3674 CIR-1: 0.7809 RMSE 1.2814\n",
      "val Loss: 0.0072 Acc: 0.4200 CIR-1: 0.8130 RMSE 1.2050\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.0078 Acc: 0.3703 CIR-1: 0.7759 RMSE 1.2840\n",
      "val Loss: 0.0072 Acc: 0.4180 CIR-1: 0.8130 RMSE 1.2058\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "LR is set to 1.0000000000000002e-06\n",
      "train Loss: 0.0077 Acc: 0.3617 CIR-1: 0.7839 RMSE 1.2667\n",
      "val Loss: 0.0072 Acc: 0.4180 CIR-1: 0.8130 RMSE 1.2058\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.0078 Acc: 0.3712 CIR-1: 0.7887 RMSE 1.2648\n",
      "val Loss: 0.0072 Acc: 0.4180 CIR-1: 0.8130 RMSE 1.2058\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.0078 Acc: 0.3623 CIR-1: 0.7746 RMSE 1.2819\n",
      "val Loss: 0.0072 Acc: 0.4180 CIR-1: 0.8130 RMSE 1.2058\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.0077 Acc: 0.3842 CIR-1: 0.7770 RMSE 1.2704\n",
      "val Loss: 0.0072 Acc: 0.4190 CIR-1: 0.8130 RMSE 1.2054\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.0078 Acc: 0.3807 CIR-1: 0.7821 RMSE 1.2707\n",
      "val Loss: 0.0072 Acc: 0.4190 CIR-1: 0.8130 RMSE 1.2054\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.0078 Acc: 0.3700 CIR-1: 0.7779 RMSE 1.2805\n",
      "val Loss: 0.0072 Acc: 0.4190 CIR-1: 0.8130 RMSE 1.2054\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.0078 Acc: 0.3637 CIR-1: 0.7818 RMSE 1.2783\n",
      "val Loss: 0.0072 Acc: 0.4180 CIR-1: 0.8130 RMSE 1.2058\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.0078 Acc: 0.3717 CIR-1: 0.7802 RMSE 1.2756\n",
      "val Loss: 0.0072 Acc: 0.4190 CIR-1: 0.8140 RMSE 1.2042\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.0078 Acc: 0.3603 CIR-1: 0.7785 RMSE 1.2943\n",
      "val Loss: 0.0072 Acc: 0.4190 CIR-1: 0.8140 RMSE 1.2042\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.0078 Acc: 0.3614 CIR-1: 0.7766 RMSE 1.2894\n",
      "val Loss: 0.0072 Acc: 0.4190 CIR-1: 0.8140 RMSE 1.2042\n",
      "\n",
      "Training complete in 0m 47s\n",
      "Best val RMSE: 1.189958\n",
      "Single loss = 0.3999999999999999, Multi loss = 0.6000000000000001\n",
      "Epoch 0/49\n",
      "----------\n",
      "LR is set to 0.01\n",
      "train Loss: 0.0084 Acc: 0.1991 CIR-1: 0.5914 RMSE 1.6726\n",
      "val Loss: 0.0085 Acc: 0.1990 CIR-1: 0.5970 RMSE 1.4177\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 0.0084 Acc: 0.2020 CIR-1: 0.6043 RMSE 1.6104\n",
      "val Loss: 0.0084 Acc: 0.2320 CIR-1: 0.7140 RMSE 1.4920\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 0.0083 Acc: 0.2197 CIR-1: 0.6393 RMSE 1.6153\n",
      "val Loss: 0.0083 Acc: 0.2210 CIR-1: 0.6370 RMSE 1.6000\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.0081 Acc: 0.2472 CIR-1: 0.6582 RMSE 1.6024\n",
      "val Loss: 0.0076 Acc: 0.3140 CIR-1: 0.7610 RMSE 1.3740\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.0079 Acc: 0.2863 CIR-1: 0.6999 RMSE 1.4754\n",
      "val Loss: 0.0073 Acc: 0.3770 CIR-1: 0.7930 RMSE 1.2562\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.0077 Acc: 0.2956 CIR-1: 0.7221 RMSE 1.4004\n",
      "val Loss: 0.0071 Acc: 0.4000 CIR-1: 0.7830 RMSE 1.2685\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.0077 Acc: 0.2887 CIR-1: 0.7296 RMSE 1.3928\n",
      "val Loss: 0.0073 Acc: 0.3780 CIR-1: 0.8530 RMSE 1.1091\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.0076 Acc: 0.3106 CIR-1: 0.7266 RMSE 1.3984\n",
      "val Loss: 0.0070 Acc: 0.3780 CIR-1: 0.8240 RMSE 1.1615\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.0076 Acc: 0.3091 CIR-1: 0.7383 RMSE 1.3589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0070 Acc: 0.3730 CIR-1: 0.8240 RMSE 1.1718\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.0076 Acc: 0.3194 CIR-1: 0.7367 RMSE 1.3674\n",
      "val Loss: 0.0070 Acc: 0.3860 CIR-1: 0.8520 RMSE 1.0932\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "LR is set to 0.001\n",
      "train Loss: 0.0074 Acc: 0.3208 CIR-1: 0.7517 RMSE 1.3209\n",
      "val Loss: 0.0069 Acc: 0.3990 CIR-1: 0.8490 RMSE 1.0890\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.0074 Acc: 0.3301 CIR-1: 0.7603 RMSE 1.3123\n",
      "val Loss: 0.0068 Acc: 0.4110 CIR-1: 0.8440 RMSE 1.0950\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.0074 Acc: 0.3284 CIR-1: 0.7692 RMSE 1.2840\n",
      "val Loss: 0.0068 Acc: 0.4180 CIR-1: 0.8430 RMSE 1.0932\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.3484 CIR-1: 0.7700 RMSE 1.2895\n",
      "val Loss: 0.0067 Acc: 0.4320 CIR-1: 0.8440 RMSE 1.0831\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.3419 CIR-1: 0.7735 RMSE 1.2717\n",
      "val Loss: 0.0067 Acc: 0.4210 CIR-1: 0.8350 RMSE 1.1027\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.3452 CIR-1: 0.7766 RMSE 1.2831\n",
      "val Loss: 0.0067 Acc: 0.4120 CIR-1: 0.8360 RMSE 1.1212\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.3407 CIR-1: 0.7682 RMSE 1.2853\n",
      "val Loss: 0.0067 Acc: 0.4140 CIR-1: 0.8370 RMSE 1.1100\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.0072 Acc: 0.3384 CIR-1: 0.7775 RMSE 1.2601\n",
      "val Loss: 0.0067 Acc: 0.4220 CIR-1: 0.8290 RMSE 1.1194\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.3390 CIR-1: 0.7743 RMSE 1.2677\n",
      "val Loss: 0.0067 Acc: 0.4210 CIR-1: 0.8410 RMSE 1.1014\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.3448 CIR-1: 0.7759 RMSE 1.2662\n",
      "val Loss: 0.0067 Acc: 0.4250 CIR-1: 0.8380 RMSE 1.1127\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "LR is set to 0.00010000000000000002\n",
      "train Loss: 0.0073 Acc: 0.3491 CIR-1: 0.7736 RMSE 1.2718\n",
      "val Loss: 0.0067 Acc: 0.4210 CIR-1: 0.8350 RMSE 1.1095\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.3418 CIR-1: 0.7728 RMSE 1.2702\n",
      "val Loss: 0.0067 Acc: 0.4210 CIR-1: 0.8360 RMSE 1.1059\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.3458 CIR-1: 0.7777 RMSE 1.2648\n",
      "val Loss: 0.0067 Acc: 0.4240 CIR-1: 0.8340 RMSE 1.1095\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.3455 CIR-1: 0.7735 RMSE 1.2684\n",
      "val Loss: 0.0067 Acc: 0.4220 CIR-1: 0.8340 RMSE 1.1104\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.0072 Acc: 0.3388 CIR-1: 0.7799 RMSE 1.2560\n",
      "val Loss: 0.0067 Acc: 0.4240 CIR-1: 0.8360 RMSE 1.1091\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.0072 Acc: 0.3472 CIR-1: 0.7747 RMSE 1.2635\n",
      "val Loss: 0.0067 Acc: 0.4230 CIR-1: 0.8330 RMSE 1.1091\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.0072 Acc: 0.3409 CIR-1: 0.7752 RMSE 1.2634\n",
      "val Loss: 0.0067 Acc: 0.4220 CIR-1: 0.8350 RMSE 1.1045\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.3429 CIR-1: 0.7742 RMSE 1.2707\n",
      "val Loss: 0.0067 Acc: 0.4270 CIR-1: 0.8350 RMSE 1.1023\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.3400 CIR-1: 0.7761 RMSE 1.2697\n",
      "val Loss: 0.0067 Acc: 0.4240 CIR-1: 0.8320 RMSE 1.1122\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.0072 Acc: 0.3521 CIR-1: 0.7764 RMSE 1.2573\n",
      "val Loss: 0.0067 Acc: 0.4300 CIR-1: 0.8350 RMSE 1.1054\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "LR is set to 1.0000000000000003e-05\n",
      "train Loss: 0.0073 Acc: 0.3462 CIR-1: 0.7710 RMSE 1.2653\n",
      "val Loss: 0.0067 Acc: 0.4300 CIR-1: 0.8350 RMSE 1.1054\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.0072 Acc: 0.3432 CIR-1: 0.7802 RMSE 1.2608\n",
      "val Loss: 0.0067 Acc: 0.4310 CIR-1: 0.8360 RMSE 1.1036\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.3477 CIR-1: 0.7771 RMSE 1.2646\n",
      "val Loss: 0.0067 Acc: 0.4310 CIR-1: 0.8340 RMSE 1.1063\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.0072 Acc: 0.3516 CIR-1: 0.7799 RMSE 1.2505\n",
      "val Loss: 0.0067 Acc: 0.4310 CIR-1: 0.8350 RMSE 1.1050\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.0072 Acc: 0.3477 CIR-1: 0.7779 RMSE 1.2497\n",
      "val Loss: 0.0067 Acc: 0.4300 CIR-1: 0.8360 RMSE 1.1041\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.0072 Acc: 0.3612 CIR-1: 0.7842 RMSE 1.2452\n",
      "val Loss: 0.0067 Acc: 0.4300 CIR-1: 0.8350 RMSE 1.1054\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.0072 Acc: 0.3540 CIR-1: 0.7798 RMSE 1.2561\n",
      "val Loss: 0.0067 Acc: 0.4300 CIR-1: 0.8350 RMSE 1.1054\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.0072 Acc: 0.3441 CIR-1: 0.7835 RMSE 1.2428\n",
      "val Loss: 0.0067 Acc: 0.4290 CIR-1: 0.8340 RMSE 1.1072\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.0072 Acc: 0.3445 CIR-1: 0.7880 RMSE 1.2420\n",
      "val Loss: 0.0067 Acc: 0.4290 CIR-1: 0.8340 RMSE 1.1072\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.3412 CIR-1: 0.7767 RMSE 1.2678\n",
      "val Loss: 0.0067 Acc: 0.4300 CIR-1: 0.8340 RMSE 1.1068\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "LR is set to 1.0000000000000002e-06\n",
      "train Loss: 0.0072 Acc: 0.3484 CIR-1: 0.7771 RMSE 1.2559\n",
      "val Loss: 0.0067 Acc: 0.4300 CIR-1: 0.8340 RMSE 1.1068\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.3480 CIR-1: 0.7714 RMSE 1.2738\n",
      "val Loss: 0.0067 Acc: 0.4300 CIR-1: 0.8340 RMSE 1.1068\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.0072 Acc: 0.3532 CIR-1: 0.7814 RMSE 1.2499\n",
      "val Loss: 0.0067 Acc: 0.4290 CIR-1: 0.8340 RMSE 1.1072\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.0072 Acc: 0.3519 CIR-1: 0.7868 RMSE 1.2456\n",
      "val Loss: 0.0067 Acc: 0.4300 CIR-1: 0.8340 RMSE 1.1068\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.0072 Acc: 0.3444 CIR-1: 0.7750 RMSE 1.2655\n",
      "val Loss: 0.0067 Acc: 0.4300 CIR-1: 0.8340 RMSE 1.1068\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.0072 Acc: 0.3504 CIR-1: 0.7745 RMSE 1.2549\n",
      "val Loss: 0.0067 Acc: 0.4300 CIR-1: 0.8340 RMSE 1.1068\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.0072 Acc: 0.3454 CIR-1: 0.7845 RMSE 1.2479\n",
      "val Loss: 0.0067 Acc: 0.4290 CIR-1: 0.8340 RMSE 1.1072\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.0072 Acc: 0.3434 CIR-1: 0.7834 RMSE 1.2499\n",
      "val Loss: 0.0067 Acc: 0.4290 CIR-1: 0.8340 RMSE 1.1072\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.0072 Acc: 0.3476 CIR-1: 0.7811 RMSE 1.2466\n",
      "val Loss: 0.0067 Acc: 0.4290 CIR-1: 0.8340 RMSE 1.1072\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.0072 Acc: 0.3509 CIR-1: 0.7795 RMSE 1.2532\n",
      "val Loss: 0.0067 Acc: 0.4290 CIR-1: 0.8340 RMSE 1.1072\n",
      "\n",
      "Training complete in 0m 49s\n",
      "Best val RMSE: 1.083051\n",
      "Single loss = 0.29999999999999993, Multi loss = 0.7000000000000001\n",
      "Epoch 0/49\n",
      "----------\n",
      "LR is set to 0.01\n",
      "train Loss: 0.0077 Acc: 0.1988 CIR-1: 0.5957 RMSE 1.7176\n",
      "val Loss: 0.0077 Acc: 0.1990 CIR-1: 0.5980 RMSE 1.7384\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 0.0076 Acc: 0.2062 CIR-1: 0.6176 RMSE 1.6794\n",
      "val Loss: 0.0077 Acc: 0.2400 CIR-1: 0.7160 RMSE 1.4973\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 0.0076 Acc: 0.2122 CIR-1: 0.6400 RMSE 1.6418\n",
      "val Loss: 0.0075 Acc: 0.2560 CIR-1: 0.7450 RMSE 1.4314\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.0075 Acc: 0.2343 CIR-1: 0.6771 RMSE 1.5711\n",
      "val Loss: 0.0073 Acc: 0.2670 CIR-1: 0.7510 RMSE 1.4036\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2589 CIR-1: 0.7111 RMSE 1.4876\n",
      "val Loss: 0.0069 Acc: 0.3010 CIR-1: 0.8010 RMSE 1.2712\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.0071 Acc: 0.2697 CIR-1: 0.7322 RMSE 1.4230\n",
      "val Loss: 0.0064 Acc: 0.3520 CIR-1: 0.8120 RMSE 1.2272\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.0071 Acc: 0.2842 CIR-1: 0.7319 RMSE 1.4251\n",
      "val Loss: 0.0065 Acc: 0.3550 CIR-1: 0.8350 RMSE 1.1692\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.0070 Acc: 0.2999 CIR-1: 0.7487 RMSE 1.3676\n",
      "val Loss: 0.0066 Acc: 0.3280 CIR-1: 0.8200 RMSE 1.1256\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.0069 Acc: 0.3024 CIR-1: 0.7551 RMSE 1.3551\n",
      "val Loss: 0.0067 Acc: 0.3590 CIR-1: 0.7750 RMSE 1.3050\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.0069 Acc: 0.3030 CIR-1: 0.7670 RMSE 1.3273\n",
      "val Loss: 0.0064 Acc: 0.3800 CIR-1: 0.8410 RMSE 1.1221\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "LR is set to 0.001\n",
      "train Loss: 0.0067 Acc: 0.3284 CIR-1: 0.7738 RMSE 1.2887\n",
      "val Loss: 0.0062 Acc: 0.3930 CIR-1: 0.8480 RMSE 1.1068\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.0067 Acc: 0.3238 CIR-1: 0.7741 RMSE 1.2958\n",
      "val Loss: 0.0062 Acc: 0.3680 CIR-1: 0.8650 RMSE 1.0812\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.0067 Acc: 0.3279 CIR-1: 0.7830 RMSE 1.2695\n",
      "val Loss: 0.0062 Acc: 0.3840 CIR-1: 0.8640 RMSE 1.0798\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.0067 Acc: 0.3388 CIR-1: 0.7902 RMSE 1.2685\n",
      "val Loss: 0.0061 Acc: 0.3850 CIR-1: 0.8610 RMSE 1.0904\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.0067 Acc: 0.3277 CIR-1: 0.7921 RMSE 1.2613\n",
      "val Loss: 0.0062 Acc: 0.3800 CIR-1: 0.8630 RMSE 1.0831\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.0066 Acc: 0.3284 CIR-1: 0.7948 RMSE 1.2464\n",
      "val Loss: 0.0061 Acc: 0.3800 CIR-1: 0.8590 RMSE 1.0886\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.0066 Acc: 0.3327 CIR-1: 0.8001 RMSE 1.2434\n",
      "val Loss: 0.0061 Acc: 0.3740 CIR-1: 0.8580 RMSE 1.0927\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.0066 Acc: 0.3441 CIR-1: 0.7987 RMSE 1.2515\n",
      "val Loss: 0.0062 Acc: 0.3840 CIR-1: 0.8560 RMSE 1.0977\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.0066 Acc: 0.3415 CIR-1: 0.7967 RMSE 1.2413\n",
      "val Loss: 0.0061 Acc: 0.3770 CIR-1: 0.8600 RMSE 1.0863\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.0065 Acc: 0.3468 CIR-1: 0.8005 RMSE 1.2348\n",
      "val Loss: 0.0061 Acc: 0.4030 CIR-1: 0.8540 RMSE 1.0872\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "LR is set to 0.00010000000000000002\n",
      "train Loss: 0.0066 Acc: 0.3458 CIR-1: 0.8031 RMSE 1.2359\n",
      "val Loss: 0.0061 Acc: 0.4050 CIR-1: 0.8530 RMSE 1.0877\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.0066 Acc: 0.3382 CIR-1: 0.7960 RMSE 1.2449\n",
      "val Loss: 0.0061 Acc: 0.4050 CIR-1: 0.8550 RMSE 1.0895\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.0065 Acc: 0.3469 CIR-1: 0.8021 RMSE 1.2311\n",
      "val Loss: 0.0061 Acc: 0.4050 CIR-1: 0.8540 RMSE 1.0886\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.0065 Acc: 0.3498 CIR-1: 0.7957 RMSE 1.2494\n",
      "val Loss: 0.0061 Acc: 0.4070 CIR-1: 0.8580 RMSE 1.0798\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.0065 Acc: 0.3444 CIR-1: 0.7982 RMSE 1.2408\n",
      "val Loss: 0.0061 Acc: 0.4070 CIR-1: 0.8560 RMSE 1.0826\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.0065 Acc: 0.3411 CIR-1: 0.8026 RMSE 1.2328\n",
      "val Loss: 0.0061 Acc: 0.4050 CIR-1: 0.8560 RMSE 1.0835\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.0065 Acc: 0.3373 CIR-1: 0.7985 RMSE 1.2437\n",
      "val Loss: 0.0061 Acc: 0.4050 CIR-1: 0.8540 RMSE 1.0840\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.0066 Acc: 0.3408 CIR-1: 0.7955 RMSE 1.2417\n",
      "val Loss: 0.0061 Acc: 0.4060 CIR-1: 0.8560 RMSE 1.0807\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.0066 Acc: 0.3447 CIR-1: 0.7951 RMSE 1.2470\n",
      "val Loss: 0.0061 Acc: 0.4050 CIR-1: 0.8580 RMSE 1.0807\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.0066 Acc: 0.3422 CIR-1: 0.7995 RMSE 1.2423\n",
      "val Loss: 0.0061 Acc: 0.4010 CIR-1: 0.8580 RMSE 1.0826\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "LR is set to 1.0000000000000003e-05\n",
      "train Loss: 0.0066 Acc: 0.3532 CIR-1: 0.7973 RMSE 1.2330\n",
      "val Loss: 0.0061 Acc: 0.4010 CIR-1: 0.8580 RMSE 1.0826\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.0066 Acc: 0.3480 CIR-1: 0.7989 RMSE 1.2443\n",
      "val Loss: 0.0061 Acc: 0.4010 CIR-1: 0.8590 RMSE 1.0812\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.0066 Acc: 0.3401 CIR-1: 0.7988 RMSE 1.2435\n",
      "val Loss: 0.0061 Acc: 0.4010 CIR-1: 0.8580 RMSE 1.0826\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.0066 Acc: 0.3443 CIR-1: 0.8069 RMSE 1.2200\n",
      "val Loss: 0.0061 Acc: 0.4010 CIR-1: 0.8580 RMSE 1.0826\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.0066 Acc: 0.3337 CIR-1: 0.7932 RMSE 1.2518\n",
      "val Loss: 0.0061 Acc: 0.4010 CIR-1: 0.8590 RMSE 1.0812\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.0066 Acc: 0.3429 CIR-1: 0.7984 RMSE 1.2474\n",
      "val Loss: 0.0061 Acc: 0.4010 CIR-1: 0.8600 RMSE 1.0798\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.0065 Acc: 0.3507 CIR-1: 0.8037 RMSE 1.2252\n",
      "val Loss: 0.0061 Acc: 0.4000 CIR-1: 0.8600 RMSE 1.0803\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.0065 Acc: 0.3512 CIR-1: 0.8087 RMSE 1.2148\n",
      "val Loss: 0.0061 Acc: 0.4000 CIR-1: 0.8590 RMSE 1.0817\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.0065 Acc: 0.3462 CIR-1: 0.8019 RMSE 1.2361\n",
      "val Loss: 0.0061 Acc: 0.3990 CIR-1: 0.8590 RMSE 1.0821\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.0066 Acc: 0.3507 CIR-1: 0.7931 RMSE 1.2480\n",
      "val Loss: 0.0061 Acc: 0.4040 CIR-1: 0.8580 RMSE 1.0812\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "LR is set to 1.0000000000000002e-06\n",
      "train Loss: 0.0066 Acc: 0.3457 CIR-1: 0.7989 RMSE 1.2395\n",
      "val Loss: 0.0061 Acc: 0.4040 CIR-1: 0.8580 RMSE 1.0812\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.0065 Acc: 0.3476 CIR-1: 0.8056 RMSE 1.2293\n",
      "val Loss: 0.0061 Acc: 0.4040 CIR-1: 0.8580 RMSE 1.0812\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.0066 Acc: 0.3515 CIR-1: 0.8024 RMSE 1.2277\n",
      "val Loss: 0.0061 Acc: 0.4040 CIR-1: 0.8580 RMSE 1.0812\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.0066 Acc: 0.3412 CIR-1: 0.8016 RMSE 1.2333\n",
      "val Loss: 0.0061 Acc: 0.4040 CIR-1: 0.8580 RMSE 1.0812\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.0066 Acc: 0.3311 CIR-1: 0.7982 RMSE 1.2523\n",
      "val Loss: 0.0061 Acc: 0.4040 CIR-1: 0.8580 RMSE 1.0812\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.0065 Acc: 0.3423 CIR-1: 0.8027 RMSE 1.2409\n",
      "val Loss: 0.0061 Acc: 0.4040 CIR-1: 0.8580 RMSE 1.0812\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.0066 Acc: 0.3491 CIR-1: 0.8041 RMSE 1.2284\n",
      "val Loss: 0.0061 Acc: 0.4040 CIR-1: 0.8580 RMSE 1.0812\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.0065 Acc: 0.3507 CIR-1: 0.8042 RMSE 1.2269\n",
      "val Loss: 0.0061 Acc: 0.4040 CIR-1: 0.8580 RMSE 1.0812\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.0066 Acc: 0.3408 CIR-1: 0.8006 RMSE 1.2437\n",
      "val Loss: 0.0061 Acc: 0.4040 CIR-1: 0.8580 RMSE 1.0812\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.0065 Acc: 0.3458 CIR-1: 0.8010 RMSE 1.2386\n",
      "val Loss: 0.0061 Acc: 0.4040 CIR-1: 0.8580 RMSE 1.0812\n",
      "\n",
      "Training complete in 0m 48s\n",
      "Best val RMSE: 1.079815\n",
      "Single loss = 0.19999999999999996, Multi loss = 0.8\n",
      "Epoch 0/49\n",
      "----------\n",
      "LR is set to 0.01\n",
      "train Loss: 0.0069 Acc: 0.1998 CIR-1: 0.6021 RMSE 1.5582\n",
      "val Loss: 0.0070 Acc: 0.1990 CIR-1: 0.6010 RMSE 1.7315\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 0.0069 Acc: 0.1944 CIR-1: 0.5975 RMSE 1.6332\n",
      "val Loss: 0.0069 Acc: 0.2020 CIR-1: 0.6340 RMSE 1.4128\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 0.0069 Acc: 0.2141 CIR-1: 0.6257 RMSE 1.6522\n",
      "val Loss: 0.0069 Acc: 0.2520 CIR-1: 0.7090 RMSE 1.4853\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.0068 Acc: 0.2177 CIR-1: 0.6550 RMSE 1.5972\n",
      "val Loss: 0.0065 Acc: 0.2850 CIR-1: 0.7720 RMSE 1.3568\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.0066 Acc: 0.2494 CIR-1: 0.6980 RMSE 1.4885\n",
      "val Loss: 0.0063 Acc: 0.2890 CIR-1: 0.8100 RMSE 1.2751\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.0064 Acc: 0.2718 CIR-1: 0.7340 RMSE 1.3900\n",
      "val Loss: 0.0059 Acc: 0.3150 CIR-1: 0.8270 RMSE 1.2223\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.0063 Acc: 0.2835 CIR-1: 0.7379 RMSE 1.3699\n",
      "val Loss: 0.0059 Acc: 0.3130 CIR-1: 0.8250 RMSE 1.1925\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.0063 Acc: 0.2878 CIR-1: 0.7546 RMSE 1.3300\n",
      "val Loss: 0.0057 Acc: 0.3580 CIR-1: 0.8310 RMSE 1.1658\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.0062 Acc: 0.2995 CIR-1: 0.7668 RMSE 1.3062\n",
      "val Loss: 0.0055 Acc: 0.3600 CIR-1: 0.8450 RMSE 1.1411\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.0061 Acc: 0.3053 CIR-1: 0.7704 RMSE 1.2826\n",
      "val Loss: 0.0055 Acc: 0.3580 CIR-1: 0.8480 RMSE 1.1261\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "LR is set to 0.001\n",
      "train Loss: 0.0060 Acc: 0.3188 CIR-1: 0.7939 RMSE 1.2330\n",
      "val Loss: 0.0055 Acc: 0.3470 CIR-1: 0.8610 RMSE 1.1045\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.0059 Acc: 0.3205 CIR-1: 0.7956 RMSE 1.2279\n",
      "val Loss: 0.0055 Acc: 0.3470 CIR-1: 0.8680 RMSE 1.0982\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.0059 Acc: 0.3281 CIR-1: 0.7999 RMSE 1.2157\n",
      "val Loss: 0.0054 Acc: 0.3580 CIR-1: 0.8660 RMSE 1.0959\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.0059 Acc: 0.3333 CIR-1: 0.7977 RMSE 1.2229\n",
      "val Loss: 0.0054 Acc: 0.3660 CIR-1: 0.8630 RMSE 1.0886\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.0059 Acc: 0.3336 CIR-1: 0.7998 RMSE 1.2149\n",
      "val Loss: 0.0054 Acc: 0.3600 CIR-1: 0.8670 RMSE 1.0835\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.3345 CIR-1: 0.8034 RMSE 1.1963\n",
      "val Loss: 0.0054 Acc: 0.3650 CIR-1: 0.8710 RMSE 1.0881\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.3387 CIR-1: 0.8027 RMSE 1.2053\n",
      "val Loss: 0.0053 Acc: 0.3670 CIR-1: 0.8620 RMSE 1.1063\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.3297 CIR-1: 0.8069 RMSE 1.1952\n",
      "val Loss: 0.0054 Acc: 0.3560 CIR-1: 0.8690 RMSE 1.0849\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.0059 Acc: 0.3263 CIR-1: 0.8066 RMSE 1.2004\n",
      "val Loss: 0.0054 Acc: 0.3570 CIR-1: 0.8640 RMSE 1.1036\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.3269 CIR-1: 0.8076 RMSE 1.1929\n",
      "val Loss: 0.0054 Acc: 0.3600 CIR-1: 0.8630 RMSE 1.1036\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "LR is set to 0.00010000000000000002\n",
      "train Loss: 0.0058 Acc: 0.3350 CIR-1: 0.8087 RMSE 1.1863\n",
      "val Loss: 0.0054 Acc: 0.3580 CIR-1: 0.8670 RMSE 1.0900\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.3352 CIR-1: 0.8035 RMSE 1.1976\n",
      "val Loss: 0.0054 Acc: 0.3600 CIR-1: 0.8660 RMSE 1.0858\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.3252 CIR-1: 0.8134 RMSE 1.1888\n",
      "val Loss: 0.0054 Acc: 0.3640 CIR-1: 0.8670 RMSE 1.0849\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.3380 CIR-1: 0.8048 RMSE 1.1995\n",
      "val Loss: 0.0054 Acc: 0.3620 CIR-1: 0.8680 RMSE 1.0844\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.3382 CIR-1: 0.8123 RMSE 1.1788\n",
      "val Loss: 0.0053 Acc: 0.3630 CIR-1: 0.8650 RMSE 1.0904\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.3419 CIR-1: 0.8123 RMSE 1.1825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0053 Acc: 0.3650 CIR-1: 0.8670 RMSE 1.0867\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.0059 Acc: 0.3405 CIR-1: 0.8094 RMSE 1.1904\n",
      "val Loss: 0.0053 Acc: 0.3630 CIR-1: 0.8680 RMSE 1.0863\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.0059 Acc: 0.3375 CIR-1: 0.8031 RMSE 1.1964\n",
      "val Loss: 0.0054 Acc: 0.3640 CIR-1: 0.8660 RMSE 1.0886\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.3377 CIR-1: 0.8126 RMSE 1.1787\n",
      "val Loss: 0.0054 Acc: 0.3630 CIR-1: 0.8670 RMSE 1.0877\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.3452 CIR-1: 0.8167 RMSE 1.1744\n",
      "val Loss: 0.0053 Acc: 0.3620 CIR-1: 0.8660 RMSE 1.0872\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "LR is set to 1.0000000000000003e-05\n",
      "train Loss: 0.0058 Acc: 0.3404 CIR-1: 0.8063 RMSE 1.1940\n",
      "val Loss: 0.0053 Acc: 0.3620 CIR-1: 0.8660 RMSE 1.0872\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.3351 CIR-1: 0.8051 RMSE 1.1925\n",
      "val Loss: 0.0053 Acc: 0.3620 CIR-1: 0.8660 RMSE 1.0872\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.3375 CIR-1: 0.8087 RMSE 1.1905\n",
      "val Loss: 0.0053 Acc: 0.3620 CIR-1: 0.8660 RMSE 1.0872\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.3426 CIR-1: 0.8135 RMSE 1.1840\n",
      "val Loss: 0.0053 Acc: 0.3610 CIR-1: 0.8660 RMSE 1.0877\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.3415 CIR-1: 0.8102 RMSE 1.1879\n",
      "val Loss: 0.0053 Acc: 0.3610 CIR-1: 0.8660 RMSE 1.0877\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.3408 CIR-1: 0.8009 RMSE 1.1994\n",
      "val Loss: 0.0053 Acc: 0.3600 CIR-1: 0.8660 RMSE 1.0858\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.3350 CIR-1: 0.8074 RMSE 1.1867\n",
      "val Loss: 0.0053 Acc: 0.3630 CIR-1: 0.8660 RMSE 1.0867\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.3348 CIR-1: 0.8133 RMSE 1.1859\n",
      "val Loss: 0.0053 Acc: 0.3640 CIR-1: 0.8660 RMSE 1.0840\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.3386 CIR-1: 0.8076 RMSE 1.1924\n",
      "val Loss: 0.0053 Acc: 0.3640 CIR-1: 0.8660 RMSE 1.0840\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.3425 CIR-1: 0.8117 RMSE 1.1898\n",
      "val Loss: 0.0053 Acc: 0.3640 CIR-1: 0.8660 RMSE 1.0840\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "LR is set to 1.0000000000000002e-06\n",
      "train Loss: 0.0058 Acc: 0.3331 CIR-1: 0.8109 RMSE 1.1827\n",
      "val Loss: 0.0053 Acc: 0.3640 CIR-1: 0.8660 RMSE 1.0840\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.3366 CIR-1: 0.8102 RMSE 1.1876\n",
      "val Loss: 0.0053 Acc: 0.3640 CIR-1: 0.8660 RMSE 1.0840\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.3354 CIR-1: 0.8144 RMSE 1.1868\n",
      "val Loss: 0.0053 Acc: 0.3630 CIR-1: 0.8660 RMSE 1.0844\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.3393 CIR-1: 0.8110 RMSE 1.1863\n",
      "val Loss: 0.0053 Acc: 0.3640 CIR-1: 0.8660 RMSE 1.0840\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.3298 CIR-1: 0.8116 RMSE 1.1828\n",
      "val Loss: 0.0053 Acc: 0.3640 CIR-1: 0.8660 RMSE 1.0840\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.3382 CIR-1: 0.8122 RMSE 1.1902\n",
      "val Loss: 0.0053 Acc: 0.3640 CIR-1: 0.8660 RMSE 1.0840\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.3443 CIR-1: 0.8103 RMSE 1.1771\n",
      "val Loss: 0.0053 Acc: 0.3640 CIR-1: 0.8660 RMSE 1.0840\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.3432 CIR-1: 0.8158 RMSE 1.1780\n",
      "val Loss: 0.0053 Acc: 0.3640 CIR-1: 0.8660 RMSE 1.0840\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.3306 CIR-1: 0.8049 RMSE 1.1938\n",
      "val Loss: 0.0053 Acc: 0.3640 CIR-1: 0.8660 RMSE 1.0840\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.3405 CIR-1: 0.8134 RMSE 1.1814\n",
      "val Loss: 0.0053 Acc: 0.3640 CIR-1: 0.8660 RMSE 1.0840\n",
      "\n",
      "Training complete in 0m 47s\n",
      "Best val RMSE: 1.083513\n",
      "Single loss = 0.09999999999999998, Multi loss = 0.9\n",
      "Epoch 0/49\n",
      "----------\n",
      "LR is set to 0.01\n",
      "train Loss: 0.0061 Acc: 0.2024 CIR-1: 0.5936 RMSE 1.5840\n",
      "val Loss: 0.0062 Acc: 0.2000 CIR-1: 0.6230 RMSE 1.4021\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 0.0061 Acc: 0.2144 CIR-1: 0.6425 RMSE 1.6177\n",
      "val Loss: 0.0059 Acc: 0.2640 CIR-1: 0.7600 RMSE 1.3968\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 0.0059 Acc: 0.2421 CIR-1: 0.7034 RMSE 1.5100\n",
      "val Loss: 0.0057 Acc: 0.2570 CIR-1: 0.7570 RMSE 1.3953\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.0057 Acc: 0.2597 CIR-1: 0.7335 RMSE 1.4351\n",
      "val Loss: 0.0056 Acc: 0.2840 CIR-1: 0.7630 RMSE 1.3737\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.0057 Acc: 0.2759 CIR-1: 0.7506 RMSE 1.3923\n",
      "val Loss: 0.0052 Acc: 0.3110 CIR-1: 0.8290 RMSE 1.1925\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.0055 Acc: 0.2809 CIR-1: 0.7671 RMSE 1.3525\n",
      "val Loss: 0.0054 Acc: 0.2800 CIR-1: 0.7860 RMSE 1.3065\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.0056 Acc: 0.2835 CIR-1: 0.7608 RMSE 1.3783\n",
      "val Loss: 0.0054 Acc: 0.2750 CIR-1: 0.7950 RMSE 1.2942\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.0055 Acc: 0.2853 CIR-1: 0.7718 RMSE 1.3357\n",
      "val Loss: 0.0051 Acc: 0.3160 CIR-1: 0.8250 RMSE 1.2037\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.0055 Acc: 0.2916 CIR-1: 0.7807 RMSE 1.3171\n",
      "val Loss: 0.0051 Acc: 0.3220 CIR-1: 0.8360 RMSE 1.1937\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.0055 Acc: 0.2945 CIR-1: 0.7753 RMSE 1.3269\n",
      "val Loss: 0.0050 Acc: 0.3290 CIR-1: 0.8270 RMSE 1.1979\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "LR is set to 0.001\n",
      "train Loss: 0.0054 Acc: 0.2935 CIR-1: 0.7874 RMSE 1.3082\n",
      "val Loss: 0.0050 Acc: 0.3270 CIR-1: 0.8400 RMSE 1.1803\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.3017 CIR-1: 0.7934 RMSE 1.2844\n",
      "val Loss: 0.0050 Acc: 0.3230 CIR-1: 0.8450 RMSE 1.1670\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.3033 CIR-1: 0.8081 RMSE 1.2526\n",
      "val Loss: 0.0050 Acc: 0.3250 CIR-1: 0.8470 RMSE 1.1572\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.3066 CIR-1: 0.8031 RMSE 1.2646\n",
      "val Loss: 0.0050 Acc: 0.3240 CIR-1: 0.8510 RMSE 1.1502\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.3026 CIR-1: 0.8030 RMSE 1.2631\n",
      "val Loss: 0.0050 Acc: 0.3230 CIR-1: 0.8500 RMSE 1.1563\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.3052 CIR-1: 0.8008 RMSE 1.2679\n",
      "val Loss: 0.0050 Acc: 0.3180 CIR-1: 0.8400 RMSE 1.1692\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.3039 CIR-1: 0.8014 RMSE 1.2656\n",
      "val Loss: 0.0049 Acc: 0.3420 CIR-1: 0.8480 RMSE 1.1441\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.3105 CIR-1: 0.8124 RMSE 1.2463\n",
      "val Loss: 0.0049 Acc: 0.3210 CIR-1: 0.8510 RMSE 1.1493\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.3155 CIR-1: 0.8092 RMSE 1.2488\n",
      "val Loss: 0.0049 Acc: 0.3410 CIR-1: 0.8440 RMSE 1.1541\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.3051 CIR-1: 0.8096 RMSE 1.2443\n",
      "val Loss: 0.0049 Acc: 0.3370 CIR-1: 0.8490 RMSE 1.1384\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "LR is set to 0.00010000000000000002\n",
      "train Loss: 0.0053 Acc: 0.3096 CIR-1: 0.8120 RMSE 1.2404\n",
      "val Loss: 0.0049 Acc: 0.3330 CIR-1: 0.8470 RMSE 1.1428\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.3060 CIR-1: 0.8087 RMSE 1.2488\n",
      "val Loss: 0.0049 Acc: 0.3330 CIR-1: 0.8470 RMSE 1.1450\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.3071 CIR-1: 0.8080 RMSE 1.2469\n",
      "val Loss: 0.0049 Acc: 0.3360 CIR-1: 0.8470 RMSE 1.1437\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.3098 CIR-1: 0.8065 RMSE 1.2547\n",
      "val Loss: 0.0049 Acc: 0.3310 CIR-1: 0.8470 RMSE 1.1480\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.3039 CIR-1: 0.8076 RMSE 1.2550\n",
      "val Loss: 0.0049 Acc: 0.3310 CIR-1: 0.8480 RMSE 1.1489\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.3085 CIR-1: 0.8080 RMSE 1.2502\n",
      "val Loss: 0.0049 Acc: 0.3310 CIR-1: 0.8480 RMSE 1.1489\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.3051 CIR-1: 0.8080 RMSE 1.2460\n",
      "val Loss: 0.0049 Acc: 0.3320 CIR-1: 0.8490 RMSE 1.1472\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.3131 CIR-1: 0.8124 RMSE 1.2420\n",
      "val Loss: 0.0049 Acc: 0.3330 CIR-1: 0.8480 RMSE 1.1480\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.3112 CIR-1: 0.8060 RMSE 1.2515\n",
      "val Loss: 0.0049 Acc: 0.3360 CIR-1: 0.8500 RMSE 1.1441\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.3095 CIR-1: 0.8103 RMSE 1.2426\n",
      "val Loss: 0.0049 Acc: 0.3340 CIR-1: 0.8510 RMSE 1.1437\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "LR is set to 1.0000000000000003e-05\n",
      "train Loss: 0.0052 Acc: 0.3102 CIR-1: 0.8119 RMSE 1.2371\n",
      "val Loss: 0.0049 Acc: 0.3340 CIR-1: 0.8510 RMSE 1.1437\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.3094 CIR-1: 0.8110 RMSE 1.2395\n",
      "val Loss: 0.0049 Acc: 0.3340 CIR-1: 0.8510 RMSE 1.1437\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.3103 CIR-1: 0.8090 RMSE 1.2441\n",
      "val Loss: 0.0049 Acc: 0.3340 CIR-1: 0.8510 RMSE 1.1437\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.3163 CIR-1: 0.8109 RMSE 1.2473\n",
      "val Loss: 0.0049 Acc: 0.3350 CIR-1: 0.8510 RMSE 1.1432\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0052 Acc: 0.3145 CIR-1: 0.8120 RMSE 1.2418\n",
      "val Loss: 0.0049 Acc: 0.3350 CIR-1: 0.8510 RMSE 1.1432\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.3120 CIR-1: 0.8159 RMSE 1.2274\n",
      "val Loss: 0.0049 Acc: 0.3350 CIR-1: 0.8510 RMSE 1.1432\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.3122 CIR-1: 0.8155 RMSE 1.2317\n",
      "val Loss: 0.0049 Acc: 0.3350 CIR-1: 0.8510 RMSE 1.1432\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.3102 CIR-1: 0.8110 RMSE 1.2436\n",
      "val Loss: 0.0049 Acc: 0.3350 CIR-1: 0.8510 RMSE 1.1432\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.3084 CIR-1: 0.8102 RMSE 1.2441\n",
      "val Loss: 0.0049 Acc: 0.3350 CIR-1: 0.8510 RMSE 1.1432\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.3070 CIR-1: 0.8084 RMSE 1.2475\n",
      "val Loss: 0.0049 Acc: 0.3360 CIR-1: 0.8510 RMSE 1.1428\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "LR is set to 1.0000000000000002e-06\n",
      "train Loss: 0.0052 Acc: 0.3087 CIR-1: 0.8098 RMSE 1.2453\n",
      "val Loss: 0.0049 Acc: 0.3360 CIR-1: 0.8510 RMSE 1.1428\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.3133 CIR-1: 0.8109 RMSE 1.2438\n",
      "val Loss: 0.0049 Acc: 0.3360 CIR-1: 0.8510 RMSE 1.1428\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.3159 CIR-1: 0.8160 RMSE 1.2271\n",
      "val Loss: 0.0049 Acc: 0.3360 CIR-1: 0.8510 RMSE 1.1428\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.3087 CIR-1: 0.8120 RMSE 1.2405\n",
      "val Loss: 0.0049 Acc: 0.3360 CIR-1: 0.8510 RMSE 1.1428\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.3069 CIR-1: 0.8133 RMSE 1.2420\n",
      "val Loss: 0.0049 Acc: 0.3360 CIR-1: 0.8510 RMSE 1.1428\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.3102 CIR-1: 0.8105 RMSE 1.2406\n",
      "val Loss: 0.0049 Acc: 0.3360 CIR-1: 0.8510 RMSE 1.1428\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.3115 CIR-1: 0.8138 RMSE 1.2299\n",
      "val Loss: 0.0049 Acc: 0.3360 CIR-1: 0.8510 RMSE 1.1428\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.3091 CIR-1: 0.8128 RMSE 1.2349\n",
      "val Loss: 0.0049 Acc: 0.3360 CIR-1: 0.8510 RMSE 1.1428\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.3108 CIR-1: 0.8152 RMSE 1.2330\n",
      "val Loss: 0.0049 Acc: 0.3360 CIR-1: 0.8510 RMSE 1.1428\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.3112 CIR-1: 0.8106 RMSE 1.2405\n",
      "val Loss: 0.0049 Acc: 0.3360 CIR-1: 0.8510 RMSE 1.1428\n",
      "\n",
      "Training complete in 0m 44s\n",
      "Best val RMSE: 1.138420\n",
      "Single loss = 0.0, Multi loss = 1.0\n",
      "Epoch 0/49\n",
      "----------\n",
      "LR is set to 0.01\n",
      "train Loss: 0.0053 Acc: 0.2034 CIR-1: 0.6114 RMSE 1.7112\n",
      "val Loss: 0.0053 Acc: 0.2340 CIR-1: 0.7190 RMSE 1.4812\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.2223 CIR-1: 0.6669 RMSE 1.5952\n",
      "val Loss: 0.0051 Acc: 0.2710 CIR-1: 0.7770 RMSE 1.3704\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 0.0050 Acc: 0.2436 CIR-1: 0.7308 RMSE 1.4546\n",
      "val Loss: 0.0048 Acc: 0.2710 CIR-1: 0.8060 RMSE 1.2791\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.0050 Acc: 0.2501 CIR-1: 0.7593 RMSE 1.3777\n",
      "val Loss: 0.0047 Acc: 0.2830 CIR-1: 0.8400 RMSE 1.1798\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.0049 Acc: 0.2588 CIR-1: 0.7729 RMSE 1.3419\n",
      "val Loss: 0.0045 Acc: 0.2770 CIR-1: 0.8110 RMSE 1.2490\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.0049 Acc: 0.2599 CIR-1: 0.7745 RMSE 1.3460\n",
      "val Loss: 0.0045 Acc: 0.3080 CIR-1: 0.8460 RMSE 1.1441\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.0049 Acc: 0.2543 CIR-1: 0.7799 RMSE 1.3216\n",
      "val Loss: 0.0044 Acc: 0.3130 CIR-1: 0.8660 RMSE 1.1018\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.0048 Acc: 0.2717 CIR-1: 0.7964 RMSE 1.2722\n",
      "val Loss: 0.0044 Acc: 0.2890 CIR-1: 0.8560 RMSE 1.1459\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.0047 Acc: 0.2732 CIR-1: 0.8052 RMSE 1.2689\n",
      "val Loss: 0.0043 Acc: 0.2990 CIR-1: 0.8680 RMSE 1.1077\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.0048 Acc: 0.2695 CIR-1: 0.8049 RMSE 1.2581\n",
      "val Loss: 0.0043 Acc: 0.3010 CIR-1: 0.8680 RMSE 1.1203\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "LR is set to 0.001\n",
      "train Loss: 0.0047 Acc: 0.2731 CIR-1: 0.8142 RMSE 1.2387\n",
      "val Loss: 0.0043 Acc: 0.2960 CIR-1: 0.8730 RMSE 1.0932\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.0046 Acc: 0.2742 CIR-1: 0.8212 RMSE 1.2210\n",
      "val Loss: 0.0042 Acc: 0.2960 CIR-1: 0.8730 RMSE 1.0863\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.2843 CIR-1: 0.8366 RMSE 1.1898\n",
      "val Loss: 0.0042 Acc: 0.2940 CIR-1: 0.8750 RMSE 1.0890\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.0046 Acc: 0.2800 CIR-1: 0.8338 RMSE 1.2004\n",
      "val Loss: 0.0042 Acc: 0.2990 CIR-1: 0.8760 RMSE 1.0854\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.2881 CIR-1: 0.8380 RMSE 1.1874\n",
      "val Loss: 0.0041 Acc: 0.2980 CIR-1: 0.8810 RMSE 1.0812\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.2820 CIR-1: 0.8397 RMSE 1.1887\n",
      "val Loss: 0.0041 Acc: 0.3000 CIR-1: 0.8740 RMSE 1.0900\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.2887 CIR-1: 0.8402 RMSE 1.1764\n",
      "val Loss: 0.0041 Acc: 0.2930 CIR-1: 0.8790 RMSE 1.0863\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.2891 CIR-1: 0.8482 RMSE 1.1646\n",
      "val Loss: 0.0041 Acc: 0.3040 CIR-1: 0.8800 RMSE 1.0821\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.2850 CIR-1: 0.8395 RMSE 1.1809\n",
      "val Loss: 0.0041 Acc: 0.2980 CIR-1: 0.8810 RMSE 1.0812\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.2845 CIR-1: 0.8387 RMSE 1.1910\n",
      "val Loss: 0.0041 Acc: 0.3030 CIR-1: 0.8770 RMSE 1.0821\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "LR is set to 0.00010000000000000002\n",
      "train Loss: 0.0045 Acc: 0.2813 CIR-1: 0.8354 RMSE 1.1910\n",
      "val Loss: 0.0041 Acc: 0.3060 CIR-1: 0.8760 RMSE 1.0798\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.2845 CIR-1: 0.8419 RMSE 1.1811\n",
      "val Loss: 0.0041 Acc: 0.3050 CIR-1: 0.8760 RMSE 1.0826\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.0044 Acc: 0.2884 CIR-1: 0.8454 RMSE 1.1687\n",
      "val Loss: 0.0041 Acc: 0.3060 CIR-1: 0.8770 RMSE 1.0831\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.2888 CIR-1: 0.8458 RMSE 1.1665\n",
      "val Loss: 0.0041 Acc: 0.3010 CIR-1: 0.8770 RMSE 1.0854\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.2884 CIR-1: 0.8441 RMSE 1.1698\n",
      "val Loss: 0.0041 Acc: 0.3060 CIR-1: 0.8780 RMSE 1.0817\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.2862 CIR-1: 0.8430 RMSE 1.1757\n",
      "val Loss: 0.0041 Acc: 0.3050 CIR-1: 0.8770 RMSE 1.0858\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.2862 CIR-1: 0.8414 RMSE 1.1793\n",
      "val Loss: 0.0041 Acc: 0.3020 CIR-1: 0.8770 RMSE 1.0872\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.2898 CIR-1: 0.8420 RMSE 1.1760\n",
      "val Loss: 0.0041 Acc: 0.3000 CIR-1: 0.8780 RMSE 1.0844\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.2902 CIR-1: 0.8445 RMSE 1.1693\n",
      "val Loss: 0.0041 Acc: 0.3020 CIR-1: 0.8780 RMSE 1.0858\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.2895 CIR-1: 0.8458 RMSE 1.1662\n",
      "val Loss: 0.0041 Acc: 0.3010 CIR-1: 0.8760 RMSE 1.0890\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "LR is set to 1.0000000000000003e-05\n",
      "train Loss: 0.0045 Acc: 0.2909 CIR-1: 0.8443 RMSE 1.1670\n",
      "val Loss: 0.0041 Acc: 0.3010 CIR-1: 0.8760 RMSE 1.0890\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.2881 CIR-1: 0.8468 RMSE 1.1701\n",
      "val Loss: 0.0041 Acc: 0.3020 CIR-1: 0.8760 RMSE 1.0886\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.2913 CIR-1: 0.8440 RMSE 1.1743\n",
      "val Loss: 0.0041 Acc: 0.3010 CIR-1: 0.8770 RMSE 1.0877\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.2885 CIR-1: 0.8391 RMSE 1.1776\n",
      "val Loss: 0.0041 Acc: 0.3010 CIR-1: 0.8780 RMSE 1.0840\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.2887 CIR-1: 0.8486 RMSE 1.1648\n",
      "val Loss: 0.0041 Acc: 0.3010 CIR-1: 0.8780 RMSE 1.0840\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.0044 Acc: 0.2914 CIR-1: 0.8480 RMSE 1.1613\n",
      "val Loss: 0.0041 Acc: 0.3010 CIR-1: 0.8780 RMSE 1.0840\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.2903 CIR-1: 0.8418 RMSE 1.1699\n",
      "val Loss: 0.0041 Acc: 0.3010 CIR-1: 0.8780 RMSE 1.0840\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.2882 CIR-1: 0.8455 RMSE 1.1716\n",
      "val Loss: 0.0041 Acc: 0.3010 CIR-1: 0.8780 RMSE 1.0840\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.0044 Acc: 0.2836 CIR-1: 0.8458 RMSE 1.1667\n",
      "val Loss: 0.0041 Acc: 0.3010 CIR-1: 0.8780 RMSE 1.0863\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.2917 CIR-1: 0.8484 RMSE 1.1595\n",
      "val Loss: 0.0041 Acc: 0.3010 CIR-1: 0.8780 RMSE 1.0863\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "LR is set to 1.0000000000000002e-06\n",
      "train Loss: 0.0044 Acc: 0.2905 CIR-1: 0.8487 RMSE 1.1594\n",
      "val Loss: 0.0041 Acc: 0.3010 CIR-1: 0.8780 RMSE 1.0863\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.0044 Acc: 0.2900 CIR-1: 0.8514 RMSE 1.1537\n",
      "val Loss: 0.0041 Acc: 0.3010 CIR-1: 0.8780 RMSE 1.0863\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.0044 Acc: 0.2885 CIR-1: 0.8426 RMSE 1.1734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0041 Acc: 0.3010 CIR-1: 0.8780 RMSE 1.0863\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.0044 Acc: 0.2900 CIR-1: 0.8483 RMSE 1.1676\n",
      "val Loss: 0.0041 Acc: 0.3010 CIR-1: 0.8780 RMSE 1.0863\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.2896 CIR-1: 0.8394 RMSE 1.1850\n",
      "val Loss: 0.0041 Acc: 0.3010 CIR-1: 0.8780 RMSE 1.0863\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.0044 Acc: 0.2931 CIR-1: 0.8464 RMSE 1.1673\n",
      "val Loss: 0.0041 Acc: 0.3010 CIR-1: 0.8780 RMSE 1.0863\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.2892 CIR-1: 0.8443 RMSE 1.1686\n",
      "val Loss: 0.0041 Acc: 0.3010 CIR-1: 0.8780 RMSE 1.0863\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.2830 CIR-1: 0.8401 RMSE 1.1799\n",
      "val Loss: 0.0041 Acc: 0.3010 CIR-1: 0.8780 RMSE 1.0863\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.2875 CIR-1: 0.8501 RMSE 1.1582\n",
      "val Loss: 0.0041 Acc: 0.3010 CIR-1: 0.8780 RMSE 1.0863\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.2878 CIR-1: 0.8436 RMSE 1.1707\n",
      "val Loss: 0.0041 Acc: 0.3010 CIR-1: 0.8780 RMSE 1.0863\n",
      "\n",
      "Training complete in 0m 47s\n",
      "Best val RMSE: 1.079815\n"
     ]
    }
   ],
   "source": [
    "mse_loss=False\n",
    "for lmbda in [.1*k for k in range(4,11)]:\n",
    "    single_loss=1.-lmbda\n",
    "    multi_loss = lmbda\n",
    "    print('Single loss = '+str(single_loss)+', Multi loss = '+str(multi_loss))\n",
    "    run_network()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
