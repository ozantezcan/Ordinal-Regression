{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchsample\n",
    "from torchsample import transforms as ts_transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from PIL import Image\n",
    "from tensorboardX import SummaryWriter\n",
    "from datetime import datetime\n",
    "import importlib\n",
    "\n",
    "\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "num_classes=5\n",
    "\n",
    "\n",
    "#from torchsample.transforms import RangeNorm\n",
    "\n",
    "import functions.fine_tune as ft\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bank32nh.data\n",
      "bank8FM.data\n",
      "bostonhousing\n",
      "cal_housing.data\n",
      "cpu_act.data\n",
      "cpu_small.data\n",
      "house_16H.data\n",
      "house_8L.data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"./dataset/regression\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8192, 33)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8192, 33)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"./dataset/regression/bank32nh.data\", sep=' ', header=None)\n",
    "train_df=train_df.drop(train_df.columns[-1],axis=1)\n",
    "print(train_df.shape)\n",
    "\n",
    "columns=[\"feat\"+str(k) for k in range(train_df.shape[1])]\n",
    "columns[-1]=\"label\"\n",
    "train_df.columns=columns\n",
    "\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat0</th>\n",
       "      <th>feat1</th>\n",
       "      <th>feat2</th>\n",
       "      <th>feat3</th>\n",
       "      <th>feat4</th>\n",
       "      <th>feat5</th>\n",
       "      <th>feat6</th>\n",
       "      <th>feat7</th>\n",
       "      <th>feat8</th>\n",
       "      <th>feat9</th>\n",
       "      <th>feat10</th>\n",
       "      <th>feat11</th>\n",
       "      <th>feat12</th>\n",
       "      <th>feat13</th>\n",
       "      <th>feat14</th>\n",
       "      <th>feat15</th>\n",
       "      <th>feat16</th>\n",
       "      <th>feat17</th>\n",
       "      <th>feat18</th>\n",
       "      <th>feat19</th>\n",
       "      <th>feat20</th>\n",
       "      <th>feat21</th>\n",
       "      <th>feat22</th>\n",
       "      <th>feat23</th>\n",
       "      <th>feat24</th>\n",
       "      <th>feat25</th>\n",
       "      <th>feat26</th>\n",
       "      <th>feat27</th>\n",
       "      <th>feat28</th>\n",
       "      <th>feat29</th>\n",
       "      <th>feat30</th>\n",
       "      <th>feat31</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.413010</td>\n",
       "      <td>0.607442</td>\n",
       "      <td>0.332608</td>\n",
       "      <td>0.406812</td>\n",
       "      <td>-0.151224</td>\n",
       "      <td>1.525222</td>\n",
       "      <td>-0.144368</td>\n",
       "      <td>0.852368</td>\n",
       "      <td>0.412397</td>\n",
       "      <td>1.728169</td>\n",
       "      <td>-0.449231</td>\n",
       "      <td>4.078482</td>\n",
       "      <td>0.232042</td>\n",
       "      <td>-0.323190</td>\n",
       "      <td>0.792235</td>\n",
       "      <td>0.421474</td>\n",
       "      <td>-0.307503</td>\n",
       "      <td>3.086689</td>\n",
       "      <td>0.363949</td>\n",
       "      <td>0.441308</td>\n",
       "      <td>-0.276851</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.974706</td>\n",
       "      <td>-0.776759</td>\n",
       "      <td>-0.783770</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.603486</td>\n",
       "      <td>-0.997118</td>\n",
       "      <td>-0.502138</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.169388</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.049118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.602384</td>\n",
       "      <td>0.350618</td>\n",
       "      <td>0.429196</td>\n",
       "      <td>0.414476</td>\n",
       "      <td>-0.124489</td>\n",
       "      <td>4.597991</td>\n",
       "      <td>0.579458</td>\n",
       "      <td>0.651134</td>\n",
       "      <td>0.104394</td>\n",
       "      <td>0.636356</td>\n",
       "      <td>-0.283787</td>\n",
       "      <td>3.546643</td>\n",
       "      <td>0.115860</td>\n",
       "      <td>0.409074</td>\n",
       "      <td>2.152997</td>\n",
       "      <td>0.758680</td>\n",
       "      <td>0.341127</td>\n",
       "      <td>1.478951</td>\n",
       "      <td>0.662488</td>\n",
       "      <td>0.462398</td>\n",
       "      <td>0.339673</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.798979</td>\n",
       "      <td>-0.002820</td>\n",
       "      <td>-0.080542</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.125542</td>\n",
       "      <td>-0.983397</td>\n",
       "      <td>-0.107632</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.186039</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.242579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.322881</td>\n",
       "      <td>-0.538491</td>\n",
       "      <td>1.602260</td>\n",
       "      <td>0.039605</td>\n",
       "      <td>0.196023</td>\n",
       "      <td>1.909005</td>\n",
       "      <td>-0.675672</td>\n",
       "      <td>0.963618</td>\n",
       "      <td>0.147458</td>\n",
       "      <td>1.414008</td>\n",
       "      <td>0.495453</td>\n",
       "      <td>0.056459</td>\n",
       "      <td>-0.163151</td>\n",
       "      <td>0.350221</td>\n",
       "      <td>1.124090</td>\n",
       "      <td>1.398160</td>\n",
       "      <td>-0.456921</td>\n",
       "      <td>1.600723</td>\n",
       "      <td>0.650252</td>\n",
       "      <td>-0.247380</td>\n",
       "      <td>0.318002</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.577355</td>\n",
       "      <td>-0.952645</td>\n",
       "      <td>-0.571600</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.280392</td>\n",
       "      <td>0.771129</td>\n",
       "      <td>-0.665756</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.024203</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.233570</td>\n",
       "      <td>-0.936451</td>\n",
       "      <td>1.710192</td>\n",
       "      <td>2.179527</td>\n",
       "      <td>0.438461</td>\n",
       "      <td>4.742055</td>\n",
       "      <td>-0.163625</td>\n",
       "      <td>-0.923273</td>\n",
       "      <td>0.597622</td>\n",
       "      <td>0.118409</td>\n",
       "      <td>0.229981</td>\n",
       "      <td>3.209085</td>\n",
       "      <td>-0.165046</td>\n",
       "      <td>0.012872</td>\n",
       "      <td>0.398148</td>\n",
       "      <td>1.335824</td>\n",
       "      <td>0.119910</td>\n",
       "      <td>13.070052</td>\n",
       "      <td>0.308221</td>\n",
       "      <td>-0.743841</td>\n",
       "      <td>0.258362</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.760084</td>\n",
       "      <td>-0.198235</td>\n",
       "      <td>-0.205276</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.509727</td>\n",
       "      <td>-0.579544</td>\n",
       "      <td>0.480094</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.568492</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.469045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.403126</td>\n",
       "      <td>0.313367</td>\n",
       "      <td>0.822382</td>\n",
       "      <td>1.393975</td>\n",
       "      <td>0.253435</td>\n",
       "      <td>9.398630</td>\n",
       "      <td>0.312528</td>\n",
       "      <td>0.288321</td>\n",
       "      <td>0.431867</td>\n",
       "      <td>0.110369</td>\n",
       "      <td>0.294665</td>\n",
       "      <td>1.274100</td>\n",
       "      <td>0.328350</td>\n",
       "      <td>-0.288962</td>\n",
       "      <td>0.067075</td>\n",
       "      <td>0.632938</td>\n",
       "      <td>0.148618</td>\n",
       "      <td>3.633846</td>\n",
       "      <td>0.233204</td>\n",
       "      <td>-0.685285</td>\n",
       "      <td>-0.758206</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.170067</td>\n",
       "      <td>0.573352</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.622033</td>\n",
       "      <td>-0.134747</td>\n",
       "      <td>0.669948</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.295913</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feat0     feat1     feat2     feat3     feat4     feat5     feat6  \\\n",
       "0  0.413010  0.607442  0.332608  0.406812 -0.151224  1.525222 -0.144368   \n",
       "1 -0.602384  0.350618  0.429196  0.414476 -0.124489  4.597991  0.579458   \n",
       "2 -0.322881 -0.538491  1.602260  0.039605  0.196023  1.909005 -0.675672   \n",
       "3 -0.233570 -0.936451  1.710192  2.179527  0.438461  4.742055 -0.163625   \n",
       "4  0.403126  0.313367  0.822382  1.393975  0.253435  9.398630  0.312528   \n",
       "\n",
       "      feat7     feat8     feat9    feat10    feat11    feat12    feat13  \\\n",
       "0  0.852368  0.412397  1.728169 -0.449231  4.078482  0.232042 -0.323190   \n",
       "1  0.651134  0.104394  0.636356 -0.283787  3.546643  0.115860  0.409074   \n",
       "2  0.963618  0.147458  1.414008  0.495453  0.056459 -0.163151  0.350221   \n",
       "3 -0.923273  0.597622  0.118409  0.229981  3.209085 -0.165046  0.012872   \n",
       "4  0.288321  0.431867  0.110369  0.294665  1.274100  0.328350 -0.288962   \n",
       "\n",
       "     feat14    feat15    feat16     feat17    feat18    feat19    feat20  \\\n",
       "0  0.792235  0.421474 -0.307503   3.086689  0.363949  0.441308 -0.276851   \n",
       "1  2.152997  0.758680  0.341127   1.478951  0.662488  0.462398  0.339673   \n",
       "2  1.124090  1.398160 -0.456921   1.600723  0.650252 -0.247380  0.318002   \n",
       "3  0.398148  1.335824  0.119910  13.070052  0.308221 -0.743841  0.258362   \n",
       "4  0.067075  0.632938  0.148618   3.633846  0.233204 -0.685285 -0.758206   \n",
       "\n",
       "   feat21    feat22    feat23    feat24  feat25    feat26    feat27    feat28  \\\n",
       "0     2.0  1.974706 -0.776759 -0.783770     8.0  0.603486 -0.997118 -0.502138   \n",
       "1     6.0  0.798979 -0.002820 -0.080542     2.0  1.125542 -0.983397 -0.107632   \n",
       "2     3.0  0.577355 -0.952645 -0.571600     5.0  1.280392  0.771129 -0.665756   \n",
       "3     4.0  0.760084 -0.198235 -0.205276     2.0  0.509727 -0.579544  0.480094   \n",
       "4     6.0  1.170067  0.573352  0.315217     2.0  0.622033 -0.134747  0.669948   \n",
       "\n",
       "   feat29    feat30  feat31     label  \n",
       "0     5.0  1.169388     9.0  0.049118  \n",
       "1     5.0  1.186039     7.0  0.242579  \n",
       "2     5.0  1.024203     6.0  0.000000  \n",
       "3     6.0  1.568492     7.0  0.469045  \n",
       "4     3.0  1.295913     9.0  0.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1638.4\n",
      "[0.0, 0.0001, 0.012395000000000002, 0.051264999999999998, 0.15377600000000002, 1.8201649999999998]\n",
      "[False False  True ..., False False False]\n",
      "[False False False ...,  True False False]\n",
      "[ True False False ..., False  True False]\n",
      "[False False False ..., False False  True]\n",
      "[False  True False ..., False False False]\n",
      "[ 2.  4.  0. ...,  1.  2.  3.]\n",
      "      feat0     feat1     feat2     feat3     feat4     feat5     feat6  \\\n",
      "0  0.413010  0.607442  0.332608  0.406812 -0.151224  1.525222 -0.144368   \n",
      "1 -0.602384  0.350618  0.429196  0.414476 -0.124489  4.597991  0.579458   \n",
      "2 -0.322881 -0.538491  1.602260  0.039605  0.196023  1.909005 -0.675672   \n",
      "3 -0.233570 -0.936451  1.710192  2.179527  0.438461  4.742055 -0.163625   \n",
      "4  0.403126  0.313367  0.822382  1.393975  0.253435  9.398630  0.312528   \n",
      "\n",
      "      feat7     feat8     feat9    feat10    feat11    feat12    feat13  \\\n",
      "0  0.852368  0.412397  1.728169 -0.449231  4.078482  0.232042 -0.323190   \n",
      "1  0.651134  0.104394  0.636356 -0.283787  3.546643  0.115860  0.409074   \n",
      "2  0.963618  0.147458  1.414008  0.495453  0.056459 -0.163151  0.350221   \n",
      "3 -0.923273  0.597622  0.118409  0.229981  3.209085 -0.165046  0.012872   \n",
      "4  0.288321  0.431867  0.110369  0.294665  1.274100  0.328350 -0.288962   \n",
      "\n",
      "     feat14    feat15    feat16     feat17    feat18    feat19    feat20  \\\n",
      "0  0.792235  0.421474 -0.307503   3.086689  0.363949  0.441308 -0.276851   \n",
      "1  2.152997  0.758680  0.341127   1.478951  0.662488  0.462398  0.339673   \n",
      "2  1.124090  1.398160 -0.456921   1.600723  0.650252 -0.247380  0.318002   \n",
      "3  0.398148  1.335824  0.119910  13.070052  0.308221 -0.743841  0.258362   \n",
      "4  0.067075  0.632938  0.148618   3.633846  0.233204 -0.685285 -0.758206   \n",
      "\n",
      "   feat21    feat22    feat23    feat24  feat25    feat26    feat27    feat28  \\\n",
      "0     2.0  1.974706 -0.776759 -0.783770     8.0  0.603486 -0.997118 -0.502138   \n",
      "1     6.0  0.798979 -0.002820 -0.080542     2.0  1.125542 -0.983397 -0.107632   \n",
      "2     3.0  0.577355 -0.952645 -0.571600     5.0  1.280392  0.771129 -0.665756   \n",
      "3     4.0  0.760084 -0.198235 -0.205276     2.0  0.509727 -0.579544  0.480094   \n",
      "4     6.0  1.170067  0.573352  0.315217     2.0  0.622033 -0.134747  0.669948   \n",
      "\n",
      "   feat29    feat30  feat31     label  label_ord  \n",
      "0     5.0  1.169388     9.0  0.049118        2.0  \n",
      "1     5.0  1.186039     7.0  0.242579        4.0  \n",
      "2     5.0  1.024203     6.0  0.000000        0.0  \n",
      "3     6.0  1.568492     7.0  0.469045        4.0  \n",
      "4     3.0  1.295913     9.0  0.000000        0.0  \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFJCAYAAACsBZWNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nh6QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1822e8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train_df['label_ord']=train_df['label']\n",
    "label=train_df.label.values\n",
    "sorted_idx=np.argsort(train_df.label.values)\n",
    "num_samples_per_class=train_df.shape[0]/num_classes\n",
    "print(num_samples_per_class)\n",
    "\n",
    "bins=[(k*1e-4+label[sorted_idx[np.round(k*num_samples_per_class-1).astype(np.int)]]) for k in range(1,num_classes+1)]\n",
    "bins.insert(0,0.0)\n",
    "bins[-1]=bins[-1]+1\n",
    "print(bins)\n",
    "\n",
    "label_ord=label.copy()\n",
    "for k in range(num_classes):\n",
    "    print(np.all([label>=bins[k], label<bins[k+1]],0))\n",
    "    label_ord[np.all([label>=bins[k], label<bins[k+1]],0)]=k\n",
    "    \n",
    "print(label_ord)\n",
    "\n",
    "\n",
    "train_df['label_ord']=label_ord\n",
    "print(train_df.head())\n",
    "\n",
    "plt.scatter(label,label_ord)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1a18801c88>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFyCAYAAADccVJQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHIbyBgDAYShvAAAchvIGAMBh/h/XNubAQlugZgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1822b438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(range(train_df.shape[0]), label[sorted_idx],s=3,c=np.sort(label_ord[sorted_idx]))\n",
    "plt.xlabel('index', fontsize=12)\n",
    "plt.ylabel('label', fontsize=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1a18530cf8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAskAAAHfCAYAAACiUkX2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAICCSAYAgML/Bw8q2TPXlnrmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1855f518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''ulimit = np.percentile(train_df.label.values, 98)\n",
    "llimit = np.percentile(train_df.label.values, 2)\n",
    "train_df['label'].ix[train_df['label']>ulimit] = ulimit\n",
    "train_df['label'].ix[train_df['label']<llimit] = llimit'''\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.distplot(train_df.label.values, bins=50, kde=False)\n",
    "plt.xlabel('label', fontsize=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training - Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -3.46686086e-03  -9.20904637e-04  -8.03606985e-01  -7.73078363e-01\n",
      "  -3.56635604e-03  -7.72843486e-01  -1.18393606e-04  -9.97569510e-04\n",
      "  -7.48644283e-01  -8.52985102e-01  -2.84872124e-03  -7.72901335e-01\n",
      "  -6.64816878e-03  -1.01483779e-03  -7.10571434e-01  -7.82965571e-01\n",
      "  -5.51937200e-03  -8.13909000e-01   6.02760301e-03   6.02673678e-03\n",
      "   1.52910570e-03  -3.44319661e-01  -2.20519716e-03  -1.63086888e-02\n",
      "   1.64501047e-03  -3.35123698e-01  -3.73533559e-03   8.63693798e-04\n",
      "   1.63181312e-03  -3.29711914e-01  -1.04553928e-02   1.46484375e-03]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "(8192, 32)\n",
      "[2 4 0 ..., 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "feat=train_df.values[:,:-2]\n",
    "\n",
    "#Normalize the features\n",
    "\n",
    "feat_max = np.amax(feat,axis=0)\n",
    "feat_min = np.amin(feat,axis=0)\n",
    "\n",
    "feat=(feat-feat_min)/(feat_max-feat_min)\n",
    "feat=feat*2-1\n",
    "\n",
    "'''feat_mean = np.mean(feat,axis=0)\n",
    "feat_std = np.std(feat,axis=0)\n",
    "\n",
    "feat=(feat-feat_mean)/feat_std\n",
    "'''\n",
    "label_ord=train_df.values[:,-1].astype(np.int)\n",
    "\n",
    "print(np.mean(feat,axis=0))\n",
    "print(np.min(feat,axis=0))\n",
    "print(feat.shape)\n",
    "print(label_ord)\n",
    "\n",
    "fvec=feat.copy()\n",
    "#label=np.eye(num_classes)[label_ord]\n",
    "#print(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(7192,)\n",
      "(1000, 32)\n",
      "(7192, 32)\n"
     ]
    }
   ],
   "source": [
    "test_size = 1000\n",
    "\n",
    "test_idx=sorted_idx[np.floor(np.linspace(0,len(label_ord)-1,test_size)).astype(np.int)]\n",
    "train_idx = np.setdiff1d(np.arange(0,len(label_ord)),test_idx)\n",
    "\n",
    "\n",
    "\n",
    "label_ord_test=label_ord[test_idx]\n",
    "label_ord_train=label_ord[train_idx]\n",
    "label_test=np.eye(num_classes)[label_ord_test]\n",
    "label_train=np.eye(num_classes)[label_ord_train]\n",
    "fvec_test=fvec[test_idx,:]\n",
    "fvec_train=fvec[train_idx,:]\n",
    "\n",
    "print(label_ord_test.shape)\n",
    "print(label_ord_train.shape)\n",
    "print(fvec_test.shape)\n",
    "print(fvec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Load Data\n",
    "\n",
    "Before running the code, the data should be downloaded and foldered in the way that is usable for imagefolder function of the PyTorch. The following code assumes that the main directory for the dataset is 'data_dir' and it includes subdirectories for all of the separate classes.\n",
    "\n",
    "For details on how to create those folders, pleaserefer to 'dataset/Folder_images.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 7192, 'val': 1000}\n",
      "!!!!! NO CUDA GPUS DETECTED\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Macros\n",
    "'''\n",
    "#uniform_sampler=False\n",
    "batch_size=128\n",
    "\n",
    "dsets={'train': torch.utils.data.TensorDataset(torch.from_numpy(fvec_train).type(torch.FloatTensor),\n",
    "                                               torch.from_numpy(label_ord_train).type(torch.LongTensor)),\n",
    "       'val': torch.utils.data.TensorDataset(torch.from_numpy(fvec_test).type(torch.FloatTensor),\n",
    "                                             torch.from_numpy(label_ord_test).type(torch.LongTensor))}\n",
    "\n",
    "'''Define dataset loaders'''\n",
    "dset_loaders = {'train':torch.utils.data.DataLoader(dsets['train'], batch_size=batch_size,shuffle=True,\n",
    "                                                    num_workers=12),\n",
    "                'val':torch.utils.data.DataLoader(dsets['val'], batch_size=batch_size,shuffle=False,\n",
    "                                                    num_workers=12)}\n",
    "\n",
    "\n",
    "dset_sizes={'train':len(dsets['train']),'val':len(dsets['val'])}\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "print(dset_sizes)\n",
    "\n",
    "if use_gpu:\n",
    "    print('GPU is available')\n",
    "else:\n",
    "    print('!!!!! NO CUDA GPUS DETECTED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 32])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "inputs, classes = next(iter(dset_loaders['val']))\n",
    "print(inputs.shape)\n",
    "print(classes.shape)\n",
    "#print(dsets['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Log Keeping\n",
    "\n",
    "This section includes the functions defined for the log keeping. Since CNNs require lots of trials, I found it easy to record the properties of the each trial with their performances in an excel file. I also added tnesorboard summaries for every trial and individual text files for showing the details of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code declares the required parameters for the network which will be also used inside log keeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network='resnet34' #Initial network archtiecture.'loaded'forusing a saved network\n",
    "networkName='resnet18_real_sgd_multisoft_August29  19:06:27' #Directory for the saved network\n",
    "optimizer='sgd' #Optimizer function\n",
    "iter_loc=14 #Number of the first column in the excel file for writing the results.\n",
    "end_to_end=True #Booolean to decide whether to train the network end-to-end or not.\n",
    "lr=0.01 #Initial learning rate\n",
    "momentum=0.9\n",
    "weight_decay=0.0005\n",
    "lr_scheduler=ft.exp_lr_scheduler #Learning rate scheduler\n",
    "lr_decay_epoch=10 #Number of epoch for learning rate decay\n",
    "pretrained=True \n",
    "mse_loss=False #Scalar MSE loss\n",
    "nclasses=5 #Number of output classes\n",
    "split=1000\n",
    "random_seed=1\n",
    "shuffle=True\n",
    "\n",
    "'''Multipliers for loss functions'''\n",
    "single_loss=1.\n",
    "multi_loss=0.\n",
    "\n",
    "comment=' ' #Additional comments if any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the functions for creating a text file and adding networkproperties to an excel file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeLog(logname):\n",
    "    '''\n",
    "    Creates a text file named Network_properties.txt inside runs/'logname'\n",
    "    '''\n",
    "    f=open('runs/'+logname+'/Network_properties.txt','w')\n",
    "    f.write('Batch size: '+str(batch_size)+'\\n')\n",
    "    f.write('Validation size: '+str(split)+'\\n')\n",
    "    f.write('Random seed: '+str(random_seed)+'\\n')\n",
    "    f.write('Shuffle: '+str(shuffle)+'\\n')\n",
    "    f.write('Validation size: '+str(split)+'\\n')\n",
    "    if mse_loss:\n",
    "        crt='MSE'\n",
    "    else:\n",
    "        crt=str(single_loss)+'xsingle + '+str(multi_loss)+'Xmulti'\n",
    "    f.write('Criterion: '+crt+'\\n')\n",
    "    f.write('Learning rate: '+str(lr)+'\\n')\n",
    "    f.write('Momentum: '+str(momentum)+'\\n')\n",
    "    f.write('Leraning Rate Scheduler: '+str(lr_scheduler)+'\\n')\n",
    "    f.write('Leraning Rate Decay Period: '+str(lr_decay_epoch)+'\\n')\n",
    "    f.write('MSE loss function: '+str(mse_loss)+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import time\n",
    "\n",
    "def writeLog_xlsx(logname='logs.xlsx',iter_loc=14):\n",
    "    '''\n",
    "    Adds a line to logs.xlsx with the network properties and outcomes.\n",
    "    :param iter_loc: First column to record the outcomes.\n",
    "    '''\n",
    "    book = openpyxl.load_workbook(logname)\n",
    "    sheet = book.active\n",
    "    if mse_loss:\n",
    "        crt='MSE'\n",
    "    else:\n",
    "        crt=str(single_loss)+'xsingle + '+str(multi_loss)+'Xmulti'\n",
    "    if network=='loaded':\n",
    "        specs=(datetime.now().strftime('%B%d  %H:%M:%S'),networkName,str(split),str(random_seed),str(shuffle),\n",
    "               optimizer, crt,str(lr),str(momentum),str(lr_scheduler),str(lr_decay_epoch),str(pretrained),\n",
    "               str(batch_size))\n",
    "    else:\n",
    "        specs=(datetime.now().strftime('%B%d  %H:%M:%S'),network,str(split),str(random_seed),str(shuffle),\n",
    "               optimizer, crt,str(lr),str(momentum),str(lr_scheduler),str(lr_decay_epoch),str(pretrained),\n",
    "               str(batch_size))\n",
    "    sheet.append(specs)\n",
    "    current_row = sheet.max_row\n",
    "    sheet.cell(row=current_row, column=iter_loc+5).value = comment\n",
    "    book.save(logname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Training and Validation\n",
    "\n",
    "In this part we will define the functions for training a CNN with different properties and loss functions.\n",
    "\n",
    "The following function takes bunch of properties defined in the beginning of Section-2 as input and creates network using those properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': Linear (10 -> 2), 'b': Linear (2 -> 1)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "layer={'a':nn.Linear(10,2)}\n",
    "layer['b']=nn.Linear(2,1)\n",
    "print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n",
      "  (fc1): Linear (32 -> 16)\n",
      "  (relu): ReLU ()\n",
      "  (fc2): Linear (16 -> 5)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model=Net(32, 16, 5)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n",
      "  (fc0): Linear (32 -> 16)\n",
      "  (relu0): ReLU ()\n",
      "  (drop0): Dropout (p = 0.3)\n",
      "  (fc1): Linear (16 -> 5)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, dropouts, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.numHidden=len(hidden_sizes)\n",
    "        setattr(self, 'fc0', nn.Linear(input_size, hidden_sizes[0]))\n",
    "        setattr(self, 'relu0', nn.ReLU())\n",
    "        setattr(self, 'drop0', nn.Dropout(p=dropouts[0]))\n",
    "        for k in range(len(hidden_sizes)-1):\n",
    "            setattr(self, 'fc'+str(k+1), nn.Linear(hidden_sizes[k], hidden_sizes[k+1]))\n",
    "            setattr(self, 'relu'+str(k+1), nn.ReLU())\n",
    "            setattr(self, 'drop'+str(k+1), nn.Dropout(p=dropouts[k+1]))\n",
    "        setattr(self, 'fc'+str(len(hidden_sizes)), nn.Linear(hidden_sizes[-1], num_classes))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out=self.fc0(x)\n",
    "        out = self.relu0(out)\n",
    "        out = self.drop0(out)\n",
    "        for k in range(self.numHidden-1):\n",
    "            fc = getattr(self,'fc'+str(k+1))\n",
    "            relu = getattr(self,'relu'+str(k+1))\n",
    "            drop = getattr(self,'drop'+str(k+1))\n",
    "            out = fc(out)\n",
    "            out = relu(out)\n",
    "            out = drop(out)\n",
    "        fc = getattr(self,'fc'+str(self.numHidden))\n",
    "        out = fc(out)\n",
    "        return out\n",
    "    \n",
    "model=Net(32, [16],[.3], 5)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-115-9ab35b138d66>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-115-9ab35b138d66>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    dropouts = [.5, .5, .5, .5, .5, .5, .5]):\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def network_loader(comment=comment,\n",
    "                    network=network,\n",
    "                    networkName=networkName,\n",
    "                    optimizer=optimizer,\n",
    "                    iter_loc=iter_loc,\n",
    "                    end_to_end=end_to_end,\n",
    "                    lr=lr,\n",
    "                    momentum=momentum,\n",
    "                    weight_decay=weight_decay,\n",
    "                    lr_scheduler=lr_scheduler,\n",
    "                    lr_decay_epoch=lr_decay_epoch,\n",
    "                    pretrained=pretrained,\n",
    "                    mse_loss=mse_loss,\n",
    "                    nclasses=nclasses,\n",
    "                    hidden_size = [32, 16, 16, 12, 12, 8, 8]\n",
    "                    dropouts = [.5, .5, .5, .5, .5, .5, .5]):\n",
    "    \n",
    "    '''Load the network from pytorch'''\n",
    "    \n",
    "    model_ft = Net(32, hidden_sizes,dropouts, 5)\n",
    "\n",
    "    if use_gpu:\n",
    "        model_ft = model_ft.cuda()\n",
    "\n",
    "    '''Define the optimizer function'''\n",
    "    if(optimizer=='adam'):\n",
    "        optimizer_ft = optim.Adam(model_ft.parameters(),lr=lr,weight_decay=weight_decay)\n",
    "    elif(optimizer=='sgd'):\n",
    "        if(end_to_end):\n",
    "            optimizer_ft = optim.SGD(model_ft.parameters(), lr=lr, momentum=momentum)\n",
    "        else:\n",
    "            optimizer_ft = optim.SGD(model_ft.fc.parameters(), lr=lr, momentum=momentum,weight_decay=weight_decay)\n",
    "    return model_ft, optimizer_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define a simple function to be able to run our training in a single line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "importlib.reload(ft)\n",
    "    \n",
    "def run_network():\n",
    "    '''\n",
    "    Cretaes the log files and starts the training\n",
    "    '''\n",
    "    model_ft, optimizer_ft = network_loader(comment=comment, #'Tested for three rooms'\n",
    "                                            network=network,\n",
    "                                            networkName=networkName,\n",
    "                                            optimizer=optimizer,\n",
    "                                            iter_loc=iter_loc,\n",
    "                                            end_to_end=end_to_end,\n",
    "                                            lr=lr,\n",
    "                                            momentum=momentum,\n",
    "                                            weight_decay=weight_decay,\n",
    "                                            lr_scheduler=lr_scheduler,\n",
    "                                            lr_decay_epoch=lr_decay_epoch,\n",
    "                                            pretrained=pretrained,\n",
    "                                            mse_loss=mse_loss,\n",
    "                                            nclasses=nclasses)\n",
    "    \n",
    "    #net.cuda()\n",
    "    \n",
    "    '''Name of the trial'''\n",
    "    if mse_loss:\n",
    "        crt='MSE'\n",
    "    else:\n",
    "        crt=str(single_loss)+'xsingle + '+str(multi_loss)+'Xmulti'\n",
    "    logname=network+'_'+'_'+optimizer+'_'+crt+'_'+datetime.now().strftime('%B%d  %H:%M:%S')\n",
    "    writer = SummaryWriter('runs/'+logname) #For tensorboard\n",
    "    writeLog(logname)\n",
    "    writeLog_xlsx()\n",
    "    \n",
    "    '''Start trianing'''\n",
    "    best_model, last_model = ft.train_model(model_ft,optimizer_ft, lr_scheduler,dset_loaders,\n",
    "                            dset_sizes,writer,use_gpu=use_gpu,num_epochs=100,batch_size=batch_size,num_log=250,\n",
    "                            multi_prob=False,lr_decay_epoch=lr_decay_epoch,init_lr=lr,mse_loss=mse_loss,\n",
    "                            iter_loc=iter_loc,cross_loss=single_loss,multi_loss=multi_loss,numOut=nclasses)\n",
    "    \n",
    "    '''Save the models'''\n",
    "    torch.save(best_model,'./saved_models/'+logname+'_best')\n",
    "    torch.save(last_model,'./saved_models/'+logname+'_last')\n",
    "    \n",
    "    '''Free up the memory'''\n",
    "    del model_ft\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function we used in the experiments is the following,\n",
    "\n",
    "$$loss(\\mathbf{y},\\hat{y})=(1-\\lambda )loss_{single}(\\mathbf{y},\\hat{y})+\\lambda loss_{multi}(\\mathbf{y},\\hat{y})$$\n",
    "where $\\mathbf{y}$ is the ground truth and $\\hat{y}$ is the prediction.\n",
    "\n",
    "Now lets test our function for different values of $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "LR is set to 0.01\n",
      "train Loss: 0.0129 Acc: 0.1922 CIR-1: 0.5659 RMSE 1.7240\n",
      "val Loss: 0.0129 Acc: 0.2240 CIR-1: 0.5410 RMSE 1.7897\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 0.0128 Acc: 0.2016 CIR-1: 0.5606 RMSE 1.8424\n",
      "val Loss: 0.0128 Acc: 0.2290 CIR-1: 0.5550 RMSE 1.8764\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.0127 Acc: 0.2293 CIR-1: 0.5528 RMSE 1.9238\n",
      "val Loss: 0.0128 Acc: 0.2500 CIR-1: 0.5570 RMSE 1.9272\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.0126 Acc: 0.2471 CIR-1: 0.5761 RMSE 1.9062\n",
      "val Loss: 0.0127 Acc: 0.2740 CIR-1: 0.5950 RMSE 1.8598\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.0126 Acc: 0.2503 CIR-1: 0.5784 RMSE 1.8918\n",
      "val Loss: 0.0126 Acc: 0.2920 CIR-1: 0.5850 RMSE 1.8628\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.0124 Acc: 0.2685 CIR-1: 0.5882 RMSE 1.8528\n",
      "val Loss: 0.0124 Acc: 0.2990 CIR-1: 0.6010 RMSE 1.8590\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.0123 Acc: 0.2910 CIR-1: 0.5816 RMSE 1.8918\n",
      "val Loss: 0.0122 Acc: 0.3290 CIR-1: 0.6480 RMSE 1.7441\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.0121 Acc: 0.3028 CIR-1: 0.6078 RMSE 1.8293\n",
      "val Loss: 0.0120 Acc: 0.3570 CIR-1: 0.6760 RMSE 1.6589\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.0120 Acc: 0.3030 CIR-1: 0.6242 RMSE 1.7673\n",
      "val Loss: 0.0118 Acc: 0.3620 CIR-1: 0.6490 RMSE 1.7135\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.0117 Acc: 0.3355 CIR-1: 0.6410 RMSE 1.7178\n",
      "val Loss: 0.0116 Acc: 0.3850 CIR-1: 0.7120 RMSE 1.5017\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "LR is set to 0.001\n",
      "train Loss: 0.0117 Acc: 0.3357 CIR-1: 0.6603 RMSE 1.6633\n",
      "val Loss: 0.0115 Acc: 0.4050 CIR-1: 0.7280 RMSE 1.4717\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.0116 Acc: 0.3372 CIR-1: 0.6687 RMSE 1.6431\n",
      "val Loss: 0.0115 Acc: 0.4040 CIR-1: 0.7260 RMSE 1.4799\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.0115 Acc: 0.3464 CIR-1: 0.6699 RMSE 1.6249\n",
      "val Loss: 0.0115 Acc: 0.4030 CIR-1: 0.7220 RMSE 1.4859\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.0115 Acc: 0.3457 CIR-1: 0.6632 RMSE 1.6456\n",
      "val Loss: 0.0115 Acc: 0.4000 CIR-1: 0.7250 RMSE 1.4856\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.0116 Acc: 0.3491 CIR-1: 0.6660 RMSE 1.6359\n",
      "val Loss: 0.0114 Acc: 0.4010 CIR-1: 0.7230 RMSE 1.4873\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.0115 Acc: 0.3491 CIR-1: 0.6673 RMSE 1.6240\n",
      "val Loss: 0.0114 Acc: 0.4020 CIR-1: 0.7270 RMSE 1.4795\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.0115 Acc: 0.3521 CIR-1: 0.6723 RMSE 1.6125\n",
      "val Loss: 0.0114 Acc: 0.4010 CIR-1: 0.7260 RMSE 1.4734\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.0115 Acc: 0.3515 CIR-1: 0.6748 RMSE 1.6126\n",
      "val Loss: 0.0114 Acc: 0.4000 CIR-1: 0.7250 RMSE 1.4690\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3548 CIR-1: 0.6760 RMSE 1.5966\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7270 RMSE 1.4622\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3557 CIR-1: 0.6784 RMSE 1.6043\n",
      "val Loss: 0.0113 Acc: 0.4030 CIR-1: 0.7300 RMSE 1.4522\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "LR is set to 0.00010000000000000002\n",
      "train Loss: 0.0114 Acc: 0.3522 CIR-1: 0.6749 RMSE 1.6157\n",
      "val Loss: 0.0113 Acc: 0.4010 CIR-1: 0.7300 RMSE 1.4529\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3562 CIR-1: 0.6788 RMSE 1.6002\n",
      "val Loss: 0.0113 Acc: 0.4030 CIR-1: 0.7310 RMSE 1.4512\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3561 CIR-1: 0.6714 RMSE 1.6165\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7300 RMSE 1.4550\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3578 CIR-1: 0.6803 RMSE 1.6096\n",
      "val Loss: 0.0113 Acc: 0.4010 CIR-1: 0.7310 RMSE 1.4519\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3550 CIR-1: 0.6795 RMSE 1.5936\n",
      "val Loss: 0.0113 Acc: 0.3990 CIR-1: 0.7300 RMSE 1.4536\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3515 CIR-1: 0.6727 RMSE 1.6070\n",
      "val Loss: 0.0113 Acc: 0.3990 CIR-1: 0.7300 RMSE 1.4536\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3585 CIR-1: 0.6771 RMSE 1.5964\n",
      "val Loss: 0.0113 Acc: 0.4010 CIR-1: 0.7320 RMSE 1.4509\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3539 CIR-1: 0.6806 RMSE 1.5899\n",
      "val Loss: 0.0113 Acc: 0.4020 CIR-1: 0.7320 RMSE 1.4505\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3618 CIR-1: 0.6813 RMSE 1.5933\n",
      "val Loss: 0.0113 Acc: 0.3970 CIR-1: 0.7290 RMSE 1.4553\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3579 CIR-1: 0.6767 RMSE 1.5998\n",
      "val Loss: 0.0113 Acc: 0.4010 CIR-1: 0.7320 RMSE 1.4474\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "LR is set to 1.0000000000000003e-05\n",
      "train Loss: 0.0114 Acc: 0.3487 CIR-1: 0.6712 RMSE 1.6069\n",
      "val Loss: 0.0113 Acc: 0.4010 CIR-1: 0.7320 RMSE 1.4474\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3476 CIR-1: 0.6673 RMSE 1.6178\n",
      "val Loss: 0.0113 Acc: 0.4010 CIR-1: 0.7320 RMSE 1.4474\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3608 CIR-1: 0.6803 RMSE 1.5989\n",
      "val Loss: 0.0113 Acc: 0.4010 CIR-1: 0.7320 RMSE 1.4474\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3610 CIR-1: 0.6826 RMSE 1.5830\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3593 CIR-1: 0.6774 RMSE 1.5972\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3489 CIR-1: 0.6766 RMSE 1.5904\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3533 CIR-1: 0.6742 RMSE 1.6161\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3471 CIR-1: 0.6739 RMSE 1.6114\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4522\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3575 CIR-1: 0.6866 RMSE 1.5903\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3571 CIR-1: 0.6794 RMSE 1.6000\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "LR is set to 1.0000000000000002e-06\n",
      "train Loss: 0.0114 Acc: 0.3621 CIR-1: 0.6780 RMSE 1.6057\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3547 CIR-1: 0.6835 RMSE 1.5792\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3558 CIR-1: 0.6751 RMSE 1.6022\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3534 CIR-1: 0.6802 RMSE 1.5961\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3528 CIR-1: 0.6749 RMSE 1.6108\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3554 CIR-1: 0.6803 RMSE 1.5917\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3553 CIR-1: 0.6763 RMSE 1.6031\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3536 CIR-1: 0.6766 RMSE 1.6057\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3561 CIR-1: 0.6823 RMSE 1.5932\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3540 CIR-1: 0.6766 RMSE 1.6034\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "LR is set to 1.0000000000000002e-07\n",
      "train Loss: 0.0114 Acc: 0.3546 CIR-1: 0.6688 RMSE 1.6152\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3548 CIR-1: 0.6806 RMSE 1.5996\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3455 CIR-1: 0.6738 RMSE 1.6138\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3586 CIR-1: 0.6810 RMSE 1.5926\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3505 CIR-1: 0.6691 RMSE 1.6145\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3621 CIR-1: 0.6778 RMSE 1.5998\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3541 CIR-1: 0.6787 RMSE 1.5999\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3596 CIR-1: 0.6820 RMSE 1.5939\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3521 CIR-1: 0.6806 RMSE 1.6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3536 CIR-1: 0.6798 RMSE 1.6003\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "LR is set to 1.0000000000000004e-08\n",
      "train Loss: 0.0114 Acc: 0.3576 CIR-1: 0.6805 RMSE 1.5910\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3534 CIR-1: 0.6717 RMSE 1.6132\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3548 CIR-1: 0.6819 RMSE 1.5944\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3536 CIR-1: 0.6707 RMSE 1.6177\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3564 CIR-1: 0.6759 RMSE 1.6105\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3601 CIR-1: 0.6844 RMSE 1.5948\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3600 CIR-1: 0.6783 RMSE 1.5923\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3608 CIR-1: 0.6831 RMSE 1.5851\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3585 CIR-1: 0.6841 RMSE 1.6031\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3465 CIR-1: 0.6664 RMSE 1.6223\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "LR is set to 1.0000000000000005e-09\n",
      "train Loss: 0.0114 Acc: 0.3479 CIR-1: 0.6780 RMSE 1.5996\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3454 CIR-1: 0.6760 RMSE 1.6053\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3525 CIR-1: 0.6803 RMSE 1.5872\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3589 CIR-1: 0.6805 RMSE 1.5992\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3547 CIR-1: 0.6802 RMSE 1.5835\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3462 CIR-1: 0.6732 RMSE 1.6147\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3508 CIR-1: 0.6738 RMSE 1.6064\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3593 CIR-1: 0.6784 RMSE 1.6014\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3658 CIR-1: 0.6883 RMSE 1.5867\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3661 CIR-1: 0.6927 RMSE 1.5710\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "LR is set to 1.0000000000000006e-10\n",
      "train Loss: 0.0115 Acc: 0.3532 CIR-1: 0.6789 RMSE 1.6126\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3498 CIR-1: 0.6682 RMSE 1.6159\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3583 CIR-1: 0.6763 RMSE 1.6106\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3558 CIR-1: 0.6781 RMSE 1.6027\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3546 CIR-1: 0.6744 RMSE 1.6093\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3541 CIR-1: 0.6713 RMSE 1.6051\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3636 CIR-1: 0.6821 RMSE 1.5887\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3533 CIR-1: 0.6780 RMSE 1.6126\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3536 CIR-1: 0.6748 RMSE 1.6118\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3585 CIR-1: 0.6792 RMSE 1.5885\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "LR is set to 1.0000000000000004e-11\n",
      "train Loss: 0.0114 Acc: 0.3604 CIR-1: 0.6869 RMSE 1.5826\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3521 CIR-1: 0.6784 RMSE 1.5900\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3572 CIR-1: 0.6734 RMSE 1.6028\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3626 CIR-1: 0.6866 RMSE 1.5838\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 0.0115 Acc: 0.3508 CIR-1: 0.6732 RMSE 1.6160\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3582 CIR-1: 0.6738 RMSE 1.6026\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3516 CIR-1: 0.6767 RMSE 1.6140\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3562 CIR-1: 0.6803 RMSE 1.6106\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3572 CIR-1: 0.6801 RMSE 1.5914\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.3484 CIR-1: 0.6752 RMSE 1.5988\n",
      "val Loss: 0.0113 Acc: 0.4000 CIR-1: 0.7310 RMSE 1.4505\n",
      "\n",
      "Training complete in 0m 46s\n",
      "Best val RMSE: 1.447411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozan-macbook-air/anaconda/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "single_loss=1.\n",
    "multi_loss=0.\n",
    "\n",
    "run_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "LR is set to 0.01\n",
      "train Loss: 0.0055 Acc: 0.2012 CIR-1: 0.5831 RMSE 1.5599\n",
      "val Loss: 0.0055 Acc: 0.1830 CIR-1: 0.5940 RMSE 1.5116\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 0.0054 Acc: 0.1974 CIR-1: 0.5965 RMSE 1.5666\n",
      "val Loss: 0.0054 Acc: 0.1860 CIR-1: 0.6080 RMSE 1.5588\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.0054 Acc: 0.2009 CIR-1: 0.6064 RMSE 1.5874\n",
      "val Loss: 0.0054 Acc: 0.1950 CIR-1: 0.6040 RMSE 1.5837\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.2101 CIR-1: 0.6201 RMSE 1.5731\n",
      "val Loss: 0.0054 Acc: 0.2050 CIR-1: 0.6160 RMSE 1.5595\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.2097 CIR-1: 0.6218 RMSE 1.5690\n",
      "val Loss: 0.0054 Acc: 0.2120 CIR-1: 0.6280 RMSE 1.5473\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.2170 CIR-1: 0.6338 RMSE 1.5450\n",
      "val Loss: 0.0054 Acc: 0.2190 CIR-1: 0.6550 RMSE 1.4920\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.2159 CIR-1: 0.6546 RMSE 1.5053\n",
      "val Loss: 0.0053 Acc: 0.2290 CIR-1: 0.6770 RMSE 1.4612\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.2343 CIR-1: 0.6705 RMSE 1.4940\n",
      "val Loss: 0.0053 Acc: 0.2320 CIR-1: 0.6940 RMSE 1.4546\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.2325 CIR-1: 0.6795 RMSE 1.4960\n",
      "val Loss: 0.0053 Acc: 0.2430 CIR-1: 0.7130 RMSE 1.4311\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.2307 CIR-1: 0.6897 RMSE 1.4821\n",
      "val Loss: 0.0052 Acc: 0.2470 CIR-1: 0.7290 RMSE 1.4093\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "LR is set to 0.001\n",
      "train Loss: 0.0052 Acc: 0.2362 CIR-1: 0.7018 RMSE 1.4783\n",
      "val Loss: 0.0052 Acc: 0.2490 CIR-1: 0.7320 RMSE 1.4036\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.2372 CIR-1: 0.7065 RMSE 1.4718\n",
      "val Loss: 0.0052 Acc: 0.2520 CIR-1: 0.7340 RMSE 1.4004\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.2372 CIR-1: 0.7059 RMSE 1.4688\n",
      "val Loss: 0.0052 Acc: 0.2540 CIR-1: 0.7360 RMSE 1.3993\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.2365 CIR-1: 0.7109 RMSE 1.4661\n",
      "val Loss: 0.0052 Acc: 0.2540 CIR-1: 0.7390 RMSE 1.3943\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.2442 CIR-1: 0.7137 RMSE 1.4608\n",
      "val Loss: 0.0052 Acc: 0.2560 CIR-1: 0.7400 RMSE 1.3925\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.2415 CIR-1: 0.7133 RMSE 1.4622\n",
      "val Loss: 0.0052 Acc: 0.2560 CIR-1: 0.7410 RMSE 1.3896\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.2330 CIR-1: 0.7061 RMSE 1.4781\n",
      "val Loss: 0.0052 Acc: 0.2580 CIR-1: 0.7420 RMSE 1.3878\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.2425 CIR-1: 0.7134 RMSE 1.4560\n",
      "val Loss: 0.0052 Acc: 0.2590 CIR-1: 0.7420 RMSE 1.3892\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.2397 CIR-1: 0.7126 RMSE 1.4649\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7450 RMSE 1.3849\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2380 CIR-1: 0.7179 RMSE 1.4594\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3864\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "LR is set to 0.00010000000000000002\n",
      "train Loss: 0.0051 Acc: 0.2440 CIR-1: 0.7227 RMSE 1.4497\n",
      "val Loss: 0.0052 Acc: 0.2580 CIR-1: 0.7470 RMSE 1.3860\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2451 CIR-1: 0.7173 RMSE 1.4568\n",
      "val Loss: 0.0052 Acc: 0.2580 CIR-1: 0.7470 RMSE 1.3860\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2373 CIR-1: 0.7080 RMSE 1.4692\n",
      "val Loss: 0.0052 Acc: 0.2580 CIR-1: 0.7470 RMSE 1.3860\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2430 CIR-1: 0.7200 RMSE 1.4533\n",
      "val Loss: 0.0052 Acc: 0.2580 CIR-1: 0.7480 RMSE 1.3831\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2410 CIR-1: 0.7204 RMSE 1.4524\n",
      "val Loss: 0.0052 Acc: 0.2580 CIR-1: 0.7480 RMSE 1.3831\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2469 CIR-1: 0.7157 RMSE 1.4586\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7480 RMSE 1.3835\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.2382 CIR-1: 0.7073 RMSE 1.4737\n",
      "val Loss: 0.0052 Acc: 0.2580 CIR-1: 0.7480 RMSE 1.3831\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2440 CIR-1: 0.7143 RMSE 1.4603\n",
      "val Loss: 0.0052 Acc: 0.2580 CIR-1: 0.7480 RMSE 1.3831\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.2397 CIR-1: 0.7141 RMSE 1.4619\n",
      "val Loss: 0.0052 Acc: 0.2580 CIR-1: 0.7480 RMSE 1.3831\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2375 CIR-1: 0.7172 RMSE 1.4591\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "LR is set to 1.0000000000000003e-05\n",
      "train Loss: 0.0051 Acc: 0.2449 CIR-1: 0.7202 RMSE 1.4510\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2378 CIR-1: 0.7108 RMSE 1.4693\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2422 CIR-1: 0.7239 RMSE 1.4424\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2379 CIR-1: 0.7177 RMSE 1.4600\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2378 CIR-1: 0.7272 RMSE 1.4412\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2440 CIR-1: 0.7161 RMSE 1.4630\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2401 CIR-1: 0.7166 RMSE 1.4580\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2440 CIR-1: 0.7165 RMSE 1.4635\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2418 CIR-1: 0.7136 RMSE 1.4653\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2437 CIR-1: 0.7223 RMSE 1.4533\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "LR is set to 1.0000000000000002e-06\n",
      "train Loss: 0.0051 Acc: 0.2392 CIR-1: 0.7166 RMSE 1.4529\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2439 CIR-1: 0.7216 RMSE 1.4516\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.2325 CIR-1: 0.7105 RMSE 1.4721\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2457 CIR-1: 0.7154 RMSE 1.4557\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2418 CIR-1: 0.7191 RMSE 1.4564\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2378 CIR-1: 0.7201 RMSE 1.4569\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2419 CIR-1: 0.7133 RMSE 1.4639\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2471 CIR-1: 0.7236 RMSE 1.4482\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2415 CIR-1: 0.7147 RMSE 1.4612\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2373 CIR-1: 0.7175 RMSE 1.4598\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "LR is set to 1.0000000000000002e-07\n",
      "train Loss: 0.0052 Acc: 0.2387 CIR-1: 0.7190 RMSE 1.4496\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2389 CIR-1: 0.7222 RMSE 1.4525\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2449 CIR-1: 0.7168 RMSE 1.4605\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.2371 CIR-1: 0.7101 RMSE 1.4689\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2383 CIR-1: 0.7207 RMSE 1.4485\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2396 CIR-1: 0.7159 RMSE 1.4585\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2433 CIR-1: 0.7182 RMSE 1.4577\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2369 CIR-1: 0.7144 RMSE 1.4597\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2425 CIR-1: 0.7151 RMSE 1.4647\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0051 Acc: 0.2454 CIR-1: 0.7158 RMSE 1.4606\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "LR is set to 1.0000000000000004e-08\n",
      "train Loss: 0.0051 Acc: 0.2430 CIR-1: 0.7198 RMSE 1.4573\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2419 CIR-1: 0.7222 RMSE 1.4471\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2442 CIR-1: 0.7140 RMSE 1.4596\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.2387 CIR-1: 0.7168 RMSE 1.4650\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2414 CIR-1: 0.7179 RMSE 1.4585\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2465 CIR-1: 0.7180 RMSE 1.4554\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2483 CIR-1: 0.7209 RMSE 1.4498\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2339 CIR-1: 0.7162 RMSE 1.4544\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2436 CIR-1: 0.7225 RMSE 1.4503\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.2344 CIR-1: 0.7101 RMSE 1.4690\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "LR is set to 1.0000000000000005e-09\n",
      "train Loss: 0.0051 Acc: 0.2398 CIR-1: 0.7154 RMSE 1.4594\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2412 CIR-1: 0.7207 RMSE 1.4530\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2497 CIR-1: 0.7226 RMSE 1.4490\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2465 CIR-1: 0.7273 RMSE 1.4472\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2460 CIR-1: 0.7157 RMSE 1.4585\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2437 CIR-1: 0.7122 RMSE 1.4708\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2421 CIR-1: 0.7148 RMSE 1.4640\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2401 CIR-1: 0.7173 RMSE 1.4599\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2422 CIR-1: 0.7248 RMSE 1.4476\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2400 CIR-1: 0.7186 RMSE 1.4604\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "LR is set to 1.0000000000000006e-10\n",
      "train Loss: 0.0051 Acc: 0.2478 CIR-1: 0.7152 RMSE 1.4559\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2383 CIR-1: 0.7109 RMSE 1.4652\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2451 CIR-1: 0.7189 RMSE 1.4566\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2411 CIR-1: 0.7176 RMSE 1.4574\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2449 CIR-1: 0.7183 RMSE 1.4511\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2365 CIR-1: 0.7122 RMSE 1.4648\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2350 CIR-1: 0.7107 RMSE 1.4730\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2425 CIR-1: 0.7216 RMSE 1.4523\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2376 CIR-1: 0.7208 RMSE 1.4524\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2403 CIR-1: 0.7164 RMSE 1.4559\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "LR is set to 1.0000000000000004e-11\n",
      "train Loss: 0.0051 Acc: 0.2432 CIR-1: 0.7148 RMSE 1.4603\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2371 CIR-1: 0.7170 RMSE 1.4563\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2411 CIR-1: 0.7193 RMSE 1.4540\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2464 CIR-1: 0.7175 RMSE 1.4600\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2415 CIR-1: 0.7155 RMSE 1.4613\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2443 CIR-1: 0.7251 RMSE 1.4449\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.2297 CIR-1: 0.7165 RMSE 1.4543\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.2412 CIR-1: 0.7154 RMSE 1.4608\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2439 CIR-1: 0.7126 RMSE 1.4663\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 0.0051 Acc: 0.2375 CIR-1: 0.7158 RMSE 1.4555\n",
      "val Loss: 0.0052 Acc: 0.2570 CIR-1: 0.7470 RMSE 1.3846\n",
      "\n",
      "Training complete in 0m 52s\n",
      "Best val RMSE: 1.383112\n",
      "Epoch 0/99\n",
      "----------\n",
      "LR is set to 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozan-macbook-air/anaconda/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0077 Acc: 0.1972 CIR-1: 0.5335 RMSE 1.9866\n",
      "val Loss: 0.0077 Acc: 0.2100 CIR-1: 0.5950 RMSE 1.7473\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 0.0076 Acc: 0.2109 CIR-1: 0.6021 RMSE 1.7099\n",
      "val Loss: 0.0077 Acc: 0.2140 CIR-1: 0.6260 RMSE 1.6340\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.0076 Acc: 0.2176 CIR-1: 0.6275 RMSE 1.6349\n",
      "val Loss: 0.0076 Acc: 0.2120 CIR-1: 0.6520 RMSE 1.5611\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.0076 Acc: 0.2120 CIR-1: 0.6322 RMSE 1.6293\n",
      "val Loss: 0.0076 Acc: 0.2240 CIR-1: 0.6620 RMSE 1.5547\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.0075 Acc: 0.2266 CIR-1: 0.6456 RMSE 1.6105\n",
      "val Loss: 0.0076 Acc: 0.2390 CIR-1: 0.6810 RMSE 1.5281\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.0075 Acc: 0.2261 CIR-1: 0.6482 RMSE 1.5985\n",
      "val Loss: 0.0076 Acc: 0.2470 CIR-1: 0.6920 RMSE 1.4957\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.0075 Acc: 0.2293 CIR-1: 0.6613 RMSE 1.5897\n",
      "val Loss: 0.0075 Acc: 0.2440 CIR-1: 0.7060 RMSE 1.4792\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.0075 Acc: 0.2330 CIR-1: 0.6727 RMSE 1.5578\n",
      "val Loss: 0.0075 Acc: 0.2620 CIR-1: 0.7270 RMSE 1.4394\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.0074 Acc: 0.2325 CIR-1: 0.6887 RMSE 1.5286\n",
      "val Loss: 0.0074 Acc: 0.2690 CIR-1: 0.7370 RMSE 1.4318\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.0074 Acc: 0.2551 CIR-1: 0.6934 RMSE 1.5466\n",
      "val Loss: 0.0074 Acc: 0.2590 CIR-1: 0.7510 RMSE 1.4082\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "LR is set to 0.001\n",
      "train Loss: 0.0074 Acc: 0.2429 CIR-1: 0.6926 RMSE 1.5302\n",
      "val Loss: 0.0074 Acc: 0.2650 CIR-1: 0.7510 RMSE 1.4061\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2532 CIR-1: 0.6976 RMSE 1.5201\n",
      "val Loss: 0.0073 Acc: 0.2670 CIR-1: 0.7530 RMSE 1.4068\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2575 CIR-1: 0.6990 RMSE 1.5201\n",
      "val Loss: 0.0073 Acc: 0.2670 CIR-1: 0.7530 RMSE 1.4085\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2563 CIR-1: 0.7066 RMSE 1.5117\n",
      "val Loss: 0.0073 Acc: 0.2680 CIR-1: 0.7550 RMSE 1.4061\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2536 CIR-1: 0.7038 RMSE 1.5165\n",
      "val Loss: 0.0073 Acc: 0.2690 CIR-1: 0.7600 RMSE 1.3986\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2572 CIR-1: 0.6997 RMSE 1.5256\n",
      "val Loss: 0.0073 Acc: 0.2740 CIR-1: 0.7590 RMSE 1.3996\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2565 CIR-1: 0.6992 RMSE 1.5246\n",
      "val Loss: 0.0073 Acc: 0.2750 CIR-1: 0.7600 RMSE 1.3982\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2581 CIR-1: 0.7022 RMSE 1.5234\n",
      "val Loss: 0.0073 Acc: 0.2750 CIR-1: 0.7600 RMSE 1.3964\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2664 CIR-1: 0.7126 RMSE 1.5030\n",
      "val Loss: 0.0073 Acc: 0.2740 CIR-1: 0.7610 RMSE 1.3957\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2643 CIR-1: 0.7059 RMSE 1.5116\n",
      "val Loss: 0.0073 Acc: 0.2750 CIR-1: 0.7630 RMSE 1.3932\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "LR is set to 0.00010000000000000002\n",
      "train Loss: 0.0073 Acc: 0.2677 CIR-1: 0.7079 RMSE 1.5099\n",
      "val Loss: 0.0073 Acc: 0.2750 CIR-1: 0.7630 RMSE 1.3932\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2589 CIR-1: 0.7063 RMSE 1.5215\n",
      "val Loss: 0.0073 Acc: 0.2750 CIR-1: 0.7630 RMSE 1.3932\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2633 CIR-1: 0.7093 RMSE 1.5169\n",
      "val Loss: 0.0073 Acc: 0.2760 CIR-1: 0.7650 RMSE 1.3907\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2640 CIR-1: 0.7076 RMSE 1.5097\n",
      "val Loss: 0.0073 Acc: 0.2760 CIR-1: 0.7650 RMSE 1.3907\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2643 CIR-1: 0.7080 RMSE 1.4994\n",
      "val Loss: 0.0073 Acc: 0.2760 CIR-1: 0.7640 RMSE 1.3918\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2528 CIR-1: 0.6991 RMSE 1.5299\n",
      "val Loss: 0.0073 Acc: 0.2760 CIR-1: 0.7650 RMSE 1.3907\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2653 CIR-1: 0.7034 RMSE 1.5224\n",
      "val Loss: 0.0073 Acc: 0.2760 CIR-1: 0.7650 RMSE 1.3907\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2636 CIR-1: 0.7072 RMSE 1.5047\n",
      "val Loss: 0.0073 Acc: 0.2760 CIR-1: 0.7650 RMSE 1.3907\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2579 CIR-1: 0.7033 RMSE 1.5198\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3921\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2639 CIR-1: 0.7073 RMSE 1.5117\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7660 RMSE 1.3874\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "LR is set to 1.0000000000000003e-05\n",
      "train Loss: 0.0073 Acc: 0.2665 CIR-1: 0.7102 RMSE 1.5076\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7660 RMSE 1.3874\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2625 CIR-1: 0.7095 RMSE 1.4997\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7660 RMSE 1.3874\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2690 CIR-1: 0.7133 RMSE 1.5050\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7660 RMSE 1.3874\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2565 CIR-1: 0.7095 RMSE 1.5071\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7660 RMSE 1.3874\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2593 CIR-1: 0.7051 RMSE 1.5097\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2671 CIR-1: 0.7072 RMSE 1.5031\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2496 CIR-1: 0.7008 RMSE 1.5294\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2656 CIR-1: 0.7061 RMSE 1.4952\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2583 CIR-1: 0.7087 RMSE 1.5023\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2603 CIR-1: 0.7047 RMSE 1.5150\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "LR is set to 1.0000000000000002e-06\n",
      "train Loss: 0.0073 Acc: 0.2657 CIR-1: 0.7076 RMSE 1.5125\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2672 CIR-1: 0.7095 RMSE 1.5020\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2590 CIR-1: 0.7002 RMSE 1.5200\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2643 CIR-1: 0.7087 RMSE 1.5071\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2624 CIR-1: 0.7118 RMSE 1.5008\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2684 CIR-1: 0.7168 RMSE 1.4871\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2717 CIR-1: 0.7198 RMSE 1.4803\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2661 CIR-1: 0.7034 RMSE 1.5199\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2621 CIR-1: 0.7059 RMSE 1.5151\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2586 CIR-1: 0.7077 RMSE 1.5143\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "LR is set to 1.0000000000000002e-07\n",
      "train Loss: 0.0073 Acc: 0.2590 CIR-1: 0.7068 RMSE 1.5099\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2622 CIR-1: 0.7112 RMSE 1.4920\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2643 CIR-1: 0.7055 RMSE 1.5142\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2632 CIR-1: 0.7115 RMSE 1.5030\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2639 CIR-1: 0.7073 RMSE 1.4992\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2600 CIR-1: 0.7095 RMSE 1.5138\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2603 CIR-1: 0.7138 RMSE 1.4999\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2622 CIR-1: 0.7100 RMSE 1.5028\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2699 CIR-1: 0.7169 RMSE 1.4965\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2652 CIR-1: 0.7186 RMSE 1.4903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "LR is set to 1.0000000000000004e-08\n",
      "train Loss: 0.0073 Acc: 0.2574 CIR-1: 0.7076 RMSE 1.5115\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2581 CIR-1: 0.7002 RMSE 1.5174\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2533 CIR-1: 0.7150 RMSE 1.5030\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2558 CIR-1: 0.6981 RMSE 1.5284\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2570 CIR-1: 0.7055 RMSE 1.5170\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2674 CIR-1: 0.7120 RMSE 1.4997\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2635 CIR-1: 0.7002 RMSE 1.5237\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2739 CIR-1: 0.7123 RMSE 1.5016\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2574 CIR-1: 0.7116 RMSE 1.5024\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2572 CIR-1: 0.7072 RMSE 1.5058\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "LR is set to 1.0000000000000005e-09\n",
      "train Loss: 0.0073 Acc: 0.2660 CIR-1: 0.7164 RMSE 1.4935\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2650 CIR-1: 0.7109 RMSE 1.5002\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2602 CIR-1: 0.7125 RMSE 1.4988\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2632 CIR-1: 0.7072 RMSE 1.5103\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2617 CIR-1: 0.7112 RMSE 1.5081\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2620 CIR-1: 0.7052 RMSE 1.5110\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2645 CIR-1: 0.7087 RMSE 1.5075\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2674 CIR-1: 0.7036 RMSE 1.5125\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2714 CIR-1: 0.7100 RMSE 1.5034\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2607 CIR-1: 0.7093 RMSE 1.5084\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "LR is set to 1.0000000000000006e-10\n",
      "train Loss: 0.0073 Acc: 0.2703 CIR-1: 0.7108 RMSE 1.5044\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2686 CIR-1: 0.7157 RMSE 1.4873\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2625 CIR-1: 0.7147 RMSE 1.4940\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2638 CIR-1: 0.7123 RMSE 1.5017\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2670 CIR-1: 0.7161 RMSE 1.4877\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2597 CIR-1: 0.7083 RMSE 1.5078\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2606 CIR-1: 0.7068 RMSE 1.5067\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2653 CIR-1: 0.7102 RMSE 1.5006\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2652 CIR-1: 0.7108 RMSE 1.5009\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2633 CIR-1: 0.7080 RMSE 1.4974\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "LR is set to 1.0000000000000004e-11\n",
      "train Loss: 0.0073 Acc: 0.2670 CIR-1: 0.7102 RMSE 1.5002\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2667 CIR-1: 0.7009 RMSE 1.5269\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2624 CIR-1: 0.7043 RMSE 1.5145\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2632 CIR-1: 0.7140 RMSE 1.4935\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2665 CIR-1: 0.7086 RMSE 1.5024\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2703 CIR-1: 0.7125 RMSE 1.4980\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2604 CIR-1: 0.7049 RMSE 1.5156\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2668 CIR-1: 0.7129 RMSE 1.4977\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2627 CIR-1: 0.7019 RMSE 1.5232\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.2650 CIR-1: 0.7127 RMSE 1.5028\n",
      "val Loss: 0.0073 Acc: 0.2770 CIR-1: 0.7650 RMSE 1.3885\n",
      "\n",
      "Training complete in 0m 57s\n",
      "Best val RMSE: 1.387444\n"
     ]
    }
   ],
   "source": [
    "single_loss=0.\n",
    "multi_loss=1.\n",
    "\n",
    "run_network()\n",
    "single_loss=0.3\n",
    "multi_loss=0.7\n",
    "\n",
    "run_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mse_loss=False\n",
    "for lmbda in [.7, .5]:\n",
    "    single_loss=1.-lmbda\n",
    "    multi_loss = lmbda\n",
    "    print('Single loss = '+str(single_loss)+', Multi loss = '+str(multi_loss))\n",
    "    run_network()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
